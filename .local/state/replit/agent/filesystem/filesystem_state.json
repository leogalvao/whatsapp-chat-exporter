{"file_contents":{"data/analyze_chat.py":{"content":"#!/usr/bin/env python3\n\"\"\"Analyze WhatsApp chat export: display message table and plot message timeline.\"\"\"\n\nimport json\nimport sys\nfrom datetime import datetime\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nJSON_FILE = \"whatsapp_Israel_Sidewalks_2026-02-08.json\"\n\n\ndef parse_timestamp(ts_str):\n    \"\"\"Parse a timestamp like '2:29 PM' into a datetime object (date fixed to export day).\"\"\"\n    if not ts_str:\n        return None\n    try:\n        t = datetime.strptime(ts_str.strip(), \"%I:%M %p\")\n        # Attach the export date so matplotlib can plot it on a time axis\n        return t.replace(year=2026, month=2, day=8)\n    except ValueError:\n        return None\n\n\ndef load_messages(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return data\n\n\ndef print_table(messages):\n    \"\"\"Print a formatted table of messages.\"\"\"\n    header = f\"{'#':<4} {'Time':<10} {'Sender':<16} {'Type':<7} {'Content (truncated)'}\"\n    sep = \"-\" * 90\n    print(sep)\n    print(header)\n    print(sep)\n    idx = 0\n    for msg in messages:\n        ts = msg.get(\"timestamp\", \"\")\n        sender = msg.get(\"sender\", \"\") or \"(system)\"\n        mtype = msg.get(\"type\", \"\")\n        content = msg.get(\"content\", \"\") or \"\"\n        # Truncate long content for display\n        content_short = content[:50].replace(\"\\n\", \" \")\n        if len(content) > 50:\n            content_short += \"...\"\n        if not ts:\n            continue  # skip metadata rows with no timestamp\n        idx += 1\n        print(f\"{idx:<4} {ts:<10} {sender:<16} {mtype:<7} {content_short}\")\n    print(sep)\n    print(f\"Total displayed: {idx}\")\n\n\ndef plot_timeline(messages):\n    \"\"\"Plot message time vs cumulative messages per sender.\"\"\"\n    # Collect timestamped messages grouped by sender\n    sender_times = defaultdict(list)\n    for msg in messages:\n        ts = parse_timestamp(msg.get(\"timestamp\", \"\"))\n        sender = msg.get(\"sender\", \"\")\n        if ts is None or not sender:\n            continue\n        sender_times[sender].append(ts)\n\n    # Sort each sender's timestamps\n    for sender in sender_times:\n        sender_times[sender].sort()\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    colors = plt.cm.tab10.colors\n    for i, (sender, times) in enumerate(sorted(sender_times.items())):\n        cumulative = list(range(1, len(times) + 1))\n        color = colors[i % len(colors)]\n        ax.step(times, cumulative, where=\"post\", label=sender, color=color,\n                linewidth=2, marker=\"o\", markersize=5)\n\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%I:%M %p\"))\n    ax.xaxis.set_major_locator(mdates.HourLocator(interval=2))\n    fig.autofmt_xdate(rotation=45)\n\n    ax.set_xlabel(\"Time of Day\")\n    ax.set_ylabel(\"Cumulative Messages Sent\")\n    ax.set_title(\"WhatsApp Messages Timeline â€” Israel Sidewalks\")\n    ax.legend(title=\"Sender\")\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(\"chat_timeline.png\", dpi=150)\n    print(\"\\nGraph saved to chat_timeline.png\")\n    plt.show()\n\n\ndef main():\n    path = sys.argv[1] if len(sys.argv) > 1 else JSON_FILE\n    data = load_messages(path)\n    messages = data.get(\"messages\", [])\n\n    print(f\"\\nChat: {data['exportInfo']['chatName']}\")\n    print(f\"Export date: {data['exportInfo']['exportDate']}\")\n    print(f\"Total messages: {data['exportInfo']['totalMessages']}\")\n    print(f\"Total media: {data['exportInfo']['totalMedia']}\\n\")\n\n    print_table(messages)\n    plot_timeline(messages)\n\n\nif __name__ == \"__main__\":\n    main()\n","path":null,"size_bytes":3548,"size_tokens":null},"replit.md":{"content":"# Snow Removal Deployment Tracker\n\n## Overview\nThis project is a Python Dash web application designed to analyze exported WhatsApp chat data. Its primary purpose is to provide interactive charts and filters for tracking the productivity of snow removal field crews. The application aims to offer insights into operational efficiency, financial performance, and crew activity, leveraging chat data to monitor deployments and identify areas for improvement.\n\n## User Preferences\nThe user prefers clear and concise information. The agent should focus on delivering functional code and avoid overly verbose explanations. Iterative development with regular check-ins for major architectural decisions is preferred. The user values maintainable and well-structured code.\n\n## System Architecture\n\n### Data Analysis Core\nThe application is built on Python 3.11, utilizing Plotly Dash for the web interface. Core data analysis is handled by modules responsible for chat analysis, timeline plotting, and generating interactive dashboards. A standalone domain model module (`data/domain_model.py`) encapsulates snow removal specific logic, including job log building, crew summaries, and deployment burndown. Configuration for deployment types, service times, and non-trackable senders is managed via `data/config/snow_removal.json`, while contract pricing is stored in `data/config/pricing.json`.\n\n### Dashboard Structure and Features\nThe web dashboard is organized into 9 main tabs:\n- **Overview**: Message volume, activity heatmaps, daily timelines.\n- **Productivity**: Daily scores, crew leaderboard, idle time detection.\n- **Crew Analysis**: Per-sender metrics, message gaps, route timelines.\n- **Deployments**: Summary, timeline, cross-deployment comparison, downloadable HTML reports, breakdown with crew-location reassignment.\n- **Operations**: Routing Gantt chart, deployment burn-down, traffic analysis, recall detection.\n- **Finances**: Revenue forecasting, cost analysis, profit metrics, invoice reconciliation.\n- **Map**: DC Service Map with OpenStreetMap tiles, location color-coding by crew, deployment filtering.\n- **Data Quality**: Noise filtering, raw message data.\n- **Settings**: Crew assignment configuration, sender management, expected hours, service times, location registry upload, invoice upload.\n\nKey dashboard features include:\n- A persistent KPI summary bar.\n- Collapsible sidebar filters for granular data selection.\n- A weighted productivity scoring system.\n- Billable route model: DeploymentID + LocationID + ServiceArea combinations (not just unique addresses).\n- Detection of idle time and recall events.\n- Automatic location type inference and trackable sender filtering.\n- Deployment burn-down charts scaled by crew size.\n- Crew merging capabilities for consistent tracking.\n- Drag-and-drop file upload for various data formats.\n- Distance-based travel efficiency analysis.\n- Location registry with fuzzy matching and map integration.\n- Invoice reconciliation with SW/PL split validation.\n\n### Supported Data Formats\nThe application supports multiple JSON data formats for chat exports, including unified multi-chat exports, legacy per-chat exports, pre-computed metrics (`metrics_report.json`), and automatically skips OCR dispatch data (`_dispatches.json`). It also includes robust handling for a new `v2 CrewChatData` format, supporting message-level deduplication and detailed message attributes.\n\n## External Dependencies\n\n- **Python Libraries**: `dash`, `dash-bootstrap-components`, `plotly`, `pandas`, `numpy`.\n- **Mapping Services**: OpenStreetMap for map tiles, ArcGIS API for DC boundary polygon data.\n- **Data Storage**: Configuration and pricing data are stored in JSON files (`data/config/snow_removal.json`, `data/config/pricing.json`).\n\n### Invoice Parser\n- `data/invoice_parser.py` - Supports both Excel (.xlsx) and CSV (.csv) invoice files\n- Auto-detects file format from extension\n- Three invoice formats: simple billing, pre-treatment report, completion report\n- Auto-detects deployment type, snow tier, and date from filename/headers\n- Skips summary rows (TOTAL, SUBTOTAL, GRAND TOTAL, T&M)\n- CSV parsing converts cells to float where possible for price column detection","path":null,"size_bytes":4215,"size_tokens":null},"data/dashboard.py":{"content":"#!/usr/bin/env python3\n\"\"\"Interactive WhatsApp chat dashboard using Seaborn + matplotlib widgets.\nReads all whatsapp_*.json exports and provides rich filtering and chart types.\"\"\"\n\nimport glob\nimport json\nimport os\nfrom datetime import datetime, date\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.widgets import CheckButtons, RadioButtons\n\nDATA_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# â”€â”€ Seaborn theme â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=0.9,\n              rc={\"axes.facecolor\": \"#fafafa\", \"figure.facecolor\": \"#f0f0f0\"})\n\nPALETTE_NAME = \"tab10\"\n\n\n# â”€â”€ Data loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nEXCLUDE_DIRS = {\".claude\", \"__pycache__\", \"node_modules\"}\n\n\ndef parse_export_date(iso_str):\n    \"\"\"Extract date from ISO 8601 exportDate string.\"\"\"\n    try:\n        return datetime.fromisoformat(iso_str.replace(\"Z\", \"+00:00\")).date()\n    except (ValueError, AttributeError):\n        return date(2026, 2, 8)\n\n\ndef parse_msg_datetime(msg, export_date=None):\n    \"\"\"Resolve a message's datetime using the best available field.\n\n    Priority:\n      1. ``dateTime`` (ISO 8601, e.g. \"2026-02-10T14:40:00\")\n      2. ``date`` + ``timestamp`` combined\n      3. ``timestamp`` + *export_date* fallback\n    \"\"\"\n    dt_str = msg.get(\"dateTime\")\n    if dt_str:\n        try:\n            return datetime.fromisoformat(dt_str)\n        except (ValueError, TypeError):\n            pass\n\n    ts_str = (msg.get(\"timestamp\") or \"\").strip()\n    date_str = msg.get(\"date\")\n\n    if date_str and ts_str:\n        try:\n            d = date.fromisoformat(date_str)\n            t = datetime.strptime(ts_str, \"%I:%M %p\")\n            return t.replace(year=d.year, month=d.month, day=d.day)\n        except (ValueError, TypeError):\n            pass\n\n    if ts_str:\n        try:\n            t = datetime.strptime(ts_str, \"%I:%M %p\")\n            if export_date:\n                return t.replace(year=export_date.year, month=export_date.month,\n                                 day=export_date.day)\n            return t.replace(year=2026, month=2, day=8)\n        except ValueError:\n            pass\n\n    return None\n\n\ndef _discover_json_files(data_dir):\n    \"\"\"Find all WhatsApp JSON exports under *data_dir*.\"\"\"\n    paths = []\n    for p in glob.glob(os.path.join(data_dir, \"archive\", \"whatsapp_*.json\")):\n        paths.append(p)\n    for root, dirs, files in os.walk(data_dir):\n        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS and d != \"archive\"]\n        for fn in files:\n            if fn.endswith(\".json\"):\n                paths.append(os.path.join(root, fn))\n    return sorted(set(paths))\n\n\ndef load_all_chats(data_dir):\n    chats = {}\n    for path in _discover_json_files(data_dir):\n        try:\n            with open(path, encoding=\"utf-8\") as f:\n                data = json.load(f)\n            if \"exportInfo\" not in data or \"messages\" not in data:\n                continue\n        except (json.JSONDecodeError, KeyError):\n            continue\n        name = data[\"exportInfo\"][\"chatName\"]\n        msgs = data[\"messages\"]\n        export_date = parse_export_date(data[\"exportInfo\"].get(\"exportDate\", \"\"))\n        if name not in chats or len(msgs) > len(chats[name][0]):\n            chats[name] = (msgs, export_date)\n    return chats\n\n\ndef build_dataframe(chats):\n    \"\"\"Build a single DataFrame from all chats.\"\"\"\n    rows = []\n    for chat_name, (msgs, export_date) in chats.items():\n        for m in msgs:\n            sender = m.get(\"sender\", \"\")\n            ts = parse_msg_datetime(m, export_date)\n            if ts is None or not sender:\n                continue\n            content = m.get(\"content\", \"\") or \"\"\n            msg_date = ts.date() if ts else None\n            rows.append({\n                \"chat\":     chat_name,\n                \"sender\":   sender,\n                \"time\":     ts,\n                \"hour\":     ts.hour + ts.minute / 60.0,\n                \"hour_int\": ts.hour,\n                \"msg_date\": msg_date,\n                \"type\":     m.get(\"type\", \"text\"),\n                \"content_len\": len(content),\n            })\n    return pd.DataFrame(rows)\n\n\n# â”€â”€ Filter constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTIME_RANGES = {\n    \"All day\":    (0, 24),\n    \"AM (0-12)\":  (0, 12),\n    \"PM (12-24)\": (12, 24),\n    \"Morning\":    (6, 12),\n    \"Afternoon\":  (12, 18),\n    \"Evening\":    (18, 24),\n    \"Night\":      (0, 6),\n}\n\nCONTENT_LEN = {\n    \"Any length\":  (0, 999999),\n    \"Short (<20)\": (0, 20),\n    \"Medium\":      (20, 100),\n    \"Long (>100)\": (100, 999999),\n}\n\n\n# â”€â”€ Dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass Dashboard:\n    def __init__(self, df_all):\n        self.df_all = df_all\n\n        self.chat_names = sorted(df_all[\"chat\"].unique())\n        self.all_senders = sorted(df_all[\"sender\"].unique())\n        self.all_types = sorted(df_all[\"type\"].unique())\n\n        # Active filters\n        self.active_chats = set(self.chat_names)\n        self.active_senders = set(self.all_senders)\n        self.active_types = set(self.all_types)\n        self.time_range = \"All day\"\n        self.content_len = \"Any length\"\n\n        # Chart settings\n        self.chart_mode = \"Cumulative\"\n        self.y_scale = \"Linear\"\n        self.group_by = \"Sender\"\n\n        self._build_ui()\n        self._redraw()\n\n    # â”€â”€ Filtered DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _filtered_df(self):\n        df = self.df_all\n        df = df[df[\"chat\"].isin(self.active_chats)]\n        df = df[df[\"sender\"].isin(self.active_senders)]\n        df = df[df[\"type\"].isin(self.active_types)]\n\n        lo_h, hi_h = TIME_RANGES[self.time_range]\n        df = df[(df[\"hour\"] >= lo_h) & (df[\"hour\"] < hi_h)]\n\n        lo_cl, hi_cl = CONTENT_LEN[self.content_len]\n        df = df[(df[\"content_len\"] >= lo_cl) & (df[\"content_len\"] < hi_cl)]\n\n        return df.copy()\n\n    @property\n    def _hue_col(self):\n        return {\"Sender\": \"sender\", \"Chat\": \"chat\", \"Type\": \"type\"}[self.group_by]\n\n    # â”€â”€ UI layout â€” 3 control columns + plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _build_ui(self):\n        self.fig = plt.figure(figsize=(20, 11))\n        self.fig.canvas.manager.set_window_title(\n            \"WhatsApp Chat Dashboard (Seaborn)\")\n\n        # Main plot area\n        self.ax = self.fig.add_axes([0.32, 0.08, 0.66, 0.84])\n\n        # Geometry\n        ITEM_H  = 0.028\n        TITLE_H = 0.022\n        GAP     = 0.015\n        TOP     = 0.96\n        COL_W   = 0.095\n        COL_X   = [0.005, 0.105, 0.205]\n\n        def _h(n):\n            return TITLE_H + ITEM_H * n\n\n        def _place(col, cur, title, n):\n            h = _h(n)\n            cur -= h\n            ax = self.fig.add_axes([COL_X[col], cur, COL_W, h])\n            ax.set_title(title, fontsize=8, fontweight=\"bold\", loc=\"left\",\n                         pad=2)\n            return ax, cur - GAP\n\n        # â”€â”€ COL 0 â€” data filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        c0 = TOP\n        ax, c0 = _place(0, c0, \"Chats\", len(self.chat_names))\n        self.chk_chats = CheckButtons(\n            ax, self.chat_names, [True] * len(self.chat_names))\n        self.chk_chats.on_clicked(self._on_chat_toggle)\n\n        ax, c0 = _place(0, c0, \"Senders\", len(self.all_senders))\n        self.chk_senders = CheckButtons(\n            ax, self.all_senders, [True] * len(self.all_senders))\n        self.chk_senders.on_clicked(self._on_sender_toggle)\n\n        ax, c0 = _place(0, c0, \"Msg Type\", len(self.all_types))\n        self.chk_types = CheckButtons(\n            ax, self.all_types, [True] * len(self.all_types))\n        self.chk_types.on_clicked(self._on_type_toggle)\n\n        # â”€â”€ COL 1 â€” more filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        c1 = TOP\n        time_labels = list(TIME_RANGES.keys())\n        ax, c1 = _place(1, c1, \"Time of Day\", len(time_labels))\n        self.radio_time = RadioButtons(ax, time_labels)\n        self.radio_time.on_clicked(self._on_time_change)\n\n        clen_labels = list(CONTENT_LEN.keys())\n        ax, c1 = _place(1, c1, \"Content Length\", len(clen_labels))\n        self.radio_clen = RadioButtons(ax, clen_labels)\n        self.radio_clen.on_clicked(self._on_clen_change)\n\n        # â”€â”€ COL 2 â€” chart options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        c2 = TOP\n        modes = [\n            \"Cumulative\", \"Scatter\", \"Bar\", \"Stacked Bar\",\n            \"Pie\", \"Histogram\", \"Heatmap\", \"Area\",\n            \"Violin\", \"Box\", \"Swarm\", \"KDE\",\n        ]\n        ax, c2 = _place(2, c2, \"Chart Type\", len(modes))\n        self.radio_mode = RadioButtons(ax, modes)\n        self.radio_mode.on_clicked(self._on_mode_change)\n\n        scales = [\"Linear\", \"Log (Y)\", \"SymLog (Y)\"]\n        ax, c2 = _place(2, c2, \"Y Scale\", len(scales))\n        self.radio_scale = RadioButtons(ax, scales)\n        self.radio_scale.on_clicked(self._on_scale_change)\n\n        groups = [\"Sender\", \"Chat\", \"Type\"]\n        ax, c2 = _place(2, c2, \"Group By\", len(groups))\n        self.radio_grp = RadioButtons(ax, groups)\n        self.radio_grp.on_clicked(self._on_group_change)\n\n    # â”€â”€ Callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _toggle(self, s, label):\n        s.discard(label) if label in s else s.add(label)\n\n    def _on_chat_toggle(self, l):   self._toggle(self.active_chats, l);   self._redraw()\n    def _on_sender_toggle(self, l): self._toggle(self.active_senders, l); self._redraw()\n    def _on_type_toggle(self, l):   self._toggle(self.active_types, l);   self._redraw()\n    def _on_time_change(self, l):   self.time_range = l;   self._redraw()\n    def _on_clen_change(self, l):   self.content_len = l;  self._redraw()\n    def _on_mode_change(self, l):   self.chart_mode = l;   self._redraw()\n    def _on_scale_change(self, l):  self.y_scale = l;      self._redraw()\n    def _on_group_change(self, l):  self.group_by = l;     self._redraw()\n\n    # â”€â”€ Drawing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _redraw(self):\n        self.ax.clear()\n        # Remove leftover colorbars from heatmap\n        for cb in getattr(self, \"_colorbars\", []):\n            cb.remove()\n        self._colorbars = []\n\n        df = self._filtered_df()\n        hue = self._hue_col\n\n        dispatch = {\n            \"Cumulative\":  self._draw_cumulative,\n            \"Scatter\":     self._draw_scatter,\n            \"Bar\":         self._draw_bar,\n            \"Stacked Bar\": self._draw_stacked_bar,\n            \"Pie\":         self._draw_pie,\n            \"Histogram\":   self._draw_histogram,\n            \"Heatmap\":     self._draw_heatmap,\n            \"Area\":        self._draw_area,\n            \"Violin\":      self._draw_violin,\n            \"Box\":         self._draw_box,\n            \"Swarm\":       self._draw_swarm,\n            \"KDE\":         self._draw_kde,\n        }\n        draw_fn = dispatch.get(self.chart_mode, self._draw_cumulative)\n        draw_fn(df, hue)\n\n        # Apply scale (skip where inapplicable)\n        if self.chart_mode not in (\"Pie\", \"Heatmap\", \"Scatter\", \"Swarm\"):\n            self._apply_scale()\n\n        chats_str = \", \".join(sorted(self.active_chats)) or \"None\"\n        if len(chats_str) > 80:\n            chats_str = chats_str[:77] + \"...\"\n        scale_tag = f\" [{self.y_scale}]\" if self.y_scale != \"Linear\" else \"\"\n        self.ax.set_title(f\"WhatsApp Dashboard â€” {chats_str}{scale_tag}\",\n                          fontsize=10)\n        self.fig.canvas.draw_idle()\n\n    def _apply_scale(self):\n        if self.y_scale == \"Log (Y)\":\n            self.ax.set_yscale(\"log\")\n            self.ax.yaxis.set_major_formatter(plt.ScalarFormatter())\n        elif self.y_scale == \"SymLog (Y)\":\n            self.ax.set_yscale(\"symlog\", linthresh=1)\n            self.ax.yaxis.set_major_formatter(plt.ScalarFormatter())\n\n    def _time_axis(self):\n        self.ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%I:%M %p\"))\n        self.ax.xaxis.set_major_locator(mdates.HourLocator(interval=2))\n        self.fig.autofmt_xdate(rotation=45)\n\n    def _no_data(self):\n        self.ax.text(0.5, 0.5, \"No data for current filters\",\n                     transform=self.ax.transAxes, ha=\"center\", va=\"center\",\n                     fontsize=14, color=\"gray\")\n\n    def _palette(self, df, hue):\n        keys = sorted(df[hue].unique())\n        return dict(zip(keys, sns.color_palette(PALETTE_NAME, len(keys))))\n\n    # â”€â”€ 1. Cumulative step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_cumulative(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        pal = self._palette(df, hue)\n        for key in sorted(df[hue].unique()):\n            sub = df[df[hue] == key].sort_values(\"time\")\n            cumul = range(1, len(sub) + 1)\n            self.ax.step(sub[\"time\"].values, list(cumul), where=\"post\",\n                         label=key, color=pal[key], linewidth=2,\n                         marker=\"o\", markersize=4)\n        self.ax.set_ylabel(\"Cumulative Messages\")\n        self.ax.set_xlabel(\"Time of Day\")\n        self.ax.legend(title=self.group_by, loc=\"upper left\", fontsize=7)\n        self._time_axis()\n\n    # â”€â”€ 2. Scatter (seaborn scatterplot) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_scatter(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        pal = self._palette(df, hue)\n        sns.scatterplot(data=df, x=\"time\", y=hue, hue=hue, size=\"content_len\",\n                        sizes=(40, 300), palette=pal, alpha=0.8,\n                        edgecolor=\"white\", ax=self.ax, legend=\"brief\")\n        self.ax.set_xlabel(\"Time of Day\")\n        self.ax.set_ylabel(self.group_by)\n        self.ax.legend(title=self.group_by, loc=\"upper left\", fontsize=7,\n                       ncol=2)\n        self._time_axis()\n\n    # â”€â”€ 3. Bar (seaborn countplot) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_bar(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        pal = self._palette(df, hue)\n        order = df[hue].value_counts().index.tolist()\n        sns.countplot(data=df, x=hue, hue=hue, palette=pal, order=order,\n                      ax=self.ax, legend=False)\n        # Annotate counts\n        for p in self.ax.patches:\n            h = p.get_height()\n            if h > 0:\n                self.ax.text(p.get_x() + p.get_width() / 2, h + 0.3,\n                             str(int(h)), ha=\"center\", fontsize=9,\n                             fontweight=\"bold\")\n        self.ax.set_ylabel(\"Message Count\")\n        self.ax.set_xlabel(self.group_by)\n        self.ax.tick_params(axis=\"x\", rotation=35)\n\n    # â”€â”€ 4. Stacked bar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_stacked_bar(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        if hue == \"sender\":\n            primary, stack = \"sender\", \"chat\"\n        elif hue == \"chat\":\n            primary, stack = \"chat\", \"sender\"\n        else:\n            primary, stack = \"type\", \"sender\"\n\n        ct = pd.crosstab(df[primary], df[stack])\n        pal = sns.color_palette(PALETTE_NAME, ct.shape[1])\n        ct.plot.bar(stacked=True, color=pal, ax=self.ax, width=0.7)\n        self.ax.set_ylabel(\"Message Count\")\n        self.ax.set_xlabel(self.group_by)\n        self.ax.legend(title=stack.title(), fontsize=7, loc=\"upper right\")\n        self.ax.tick_params(axis=\"x\", rotation=35)\n\n    # â”€â”€ 5. Pie â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_pie(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        counts = df[hue].value_counts().sort_index()\n        pal = sns.color_palette(PALETTE_NAME, len(counts))\n        wedges, texts, autotexts = self.ax.pie(\n            counts.values, labels=counts.index, colors=pal,\n            autopct=\"%1.1f%%\", startangle=140, pctdistance=0.8)\n        for t in autotexts:\n            t.set_fontsize(9); t.set_fontweight(\"bold\")\n        self.ax.set_ylabel(\"\")\n\n    # â”€â”€ 6. Histogram (seaborn histplot) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_histogram(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        pal = self._palette(df, hue)\n        sns.histplot(data=df, x=\"hour\", hue=hue, palette=pal,\n                     bins=np.arange(0, 25, 1), multiple=\"stack\",\n                     edgecolor=\"white\", ax=self.ax)\n        self.ax.set_xlabel(\"Hour of Day\")\n        self.ax.set_ylabel(\"Message Count\")\n        self.ax.set_xticks(range(0, 25, 2))\n        self.ax.set_xticklabels(\n            [f\"{h % 12 or 12} {'AM' if h < 12 else 'PM'}\"\n             for h in range(0, 25, 2)], rotation=45, ha=\"right\")\n        self.ax.legend(title=self.group_by, fontsize=7, loc=\"upper right\")\n\n    # â”€â”€ 7. Heatmap (seaborn heatmap) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_heatmap(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        ct = pd.crosstab(df[hue], df[\"hour_int\"])\n        # Ensure all 24 hours present\n        for h in range(24):\n            if h not in ct.columns:\n                ct[h] = 0\n        ct = ct[sorted(ct.columns)]\n        ct.columns = [f\"{h % 12 or 12}{'a' if h < 12 else 'p'}\"\n                      for h in ct.columns]\n\n        if self.y_scale == \"Log (Y)\":\n            ct_display = np.log1p(ct)\n            fmt = \".1f\"\n        elif self.y_scale == \"SymLog (Y)\":\n            ct_display = np.sign(ct) * np.log1p(np.abs(ct))\n            fmt = \".1f\"\n        else:\n            ct_display = ct\n            fmt = \"d\"\n\n        sns.heatmap(ct_display, annot=ct, fmt=fmt, cmap=\"YlOrRd\",\n                    linewidths=0.5, ax=self.ax, cbar_kws={\"shrink\": 0.7})\n        self.ax.set_xlabel(\"Hour of Day\")\n        self.ax.set_ylabel(self.group_by)\n\n    # â”€â”€ 8. Stacked area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_area(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        keys = sorted(df[hue].unique())\n        pal = sns.color_palette(PALETTE_NAME, len(keys))\n        all_times = sorted(df[\"time\"].unique())\n\n        stacks = []\n        for key in keys:\n            sub = df[df[hue] == key].sort_values(\"time\")\n            times_list = sub[\"time\"].tolist()\n            c, idx, series = 0, 0, []\n            for t in all_times:\n                while idx < len(times_list) and times_list[idx] <= t:\n                    c += 1; idx += 1\n                series.append(c)\n            stacks.append(series)\n\n        self.ax.stackplot(all_times, stacks, labels=keys, colors=pal,\n                          alpha=0.85)\n        self.ax.set_ylabel(\"Cumulative Messages (stacked)\")\n        self.ax.set_xlabel(\"Time of Day\")\n        self.ax.legend(title=self.group_by, loc=\"upper left\", fontsize=7)\n        self._time_axis()\n\n    # â”€â”€ 9. Violin (seaborn) â€” content length distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_violin(self, df, hue):\n        if df.empty or df[hue].nunique() < 1:\n            self._no_data(); return\n        pal = self._palette(df, hue)\n        sns.violinplot(data=df, x=hue, y=\"hour\", hue=hue,\n                       palette=pal, inner=\"quart\", density_norm=\"width\",\n                       ax=self.ax, legend=False)\n        self.ax.set_ylabel(\"Hour of Day\")\n        self.ax.set_xlabel(self.group_by)\n        self.ax.tick_params(axis=\"x\", rotation=35)\n\n    # â”€â”€ 10. Box plot (seaborn) â€” message timing distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_box(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        pal = self._palette(df, hue)\n        sns.boxplot(data=df, x=hue, y=\"hour\", hue=hue, palette=pal,\n                    ax=self.ax, legend=False, width=0.5)\n        self.ax.set_ylabel(\"Hour of Day\")\n        self.ax.set_xlabel(self.group_by)\n        self.ax.tick_params(axis=\"x\", rotation=35)\n\n    # â”€â”€ 11. Swarm plot (seaborn) â€” every message as a dot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_swarm(self, df, hue):\n        if df.empty:\n            self._no_data(); return\n        # Limit to 300 points for performance\n        plot_df = df if len(df) <= 300 else df.sample(300, random_state=42)\n        pal = self._palette(plot_df, hue)\n        sns.swarmplot(data=plot_df, x=hue, y=\"hour\", hue=hue, palette=pal,\n                      size=5, ax=self.ax, legend=False)\n        self.ax.set_ylabel(\"Hour of Day\")\n        self.ax.set_xlabel(self.group_by)\n        self.ax.tick_params(axis=\"x\", rotation=35)\n\n    # â”€â”€ 12. KDE (seaborn) â€” message density over time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _draw_kde(self, df, hue):\n        if df.empty or len(df) < 2:\n            self._no_data(); return\n        pal = self._palette(df, hue)\n        for key in sorted(df[hue].unique()):\n            sub = df[df[hue] == key]\n            if len(sub) >= 2:\n                sns.kdeplot(data=sub, x=\"hour\", color=pal[key], label=key,\n                            fill=True, alpha=0.3, linewidth=2, ax=self.ax,\n                            bw_adjust=0.8, clip=(0, 24))\n        self.ax.set_xlabel(\"Hour of Day\")\n        self.ax.set_ylabel(\"Density\")\n        self.ax.set_xlim(0, 24)\n        self.ax.set_xticks(range(0, 25, 2))\n        self.ax.set_xticklabels(\n            [f\"{h % 12 or 12} {'AM' if h < 12 else 'PM'}\"\n             for h in range(0, 25, 2)], rotation=45, ha=\"right\")\n        self.ax.legend(title=self.group_by, fontsize=7, loc=\"upper right\")\n\n    # â”€â”€ Show â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def show(self):\n        plt.show()\n\n\n# â”€â”€ Entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef main():\n    chats = load_all_chats(DATA_DIR)\n    if not chats:\n        print(\"No JSON chat files found in\", DATA_DIR)\n        return\n\n    df = build_dataframe(chats)\n    print(f\"Loaded {len(chats)} chat(s), {len(df)} messages total:\")\n    for name in sorted(chats.keys()):\n        sub = df[df[\"chat\"] == name]\n        senders = \", \".join(sorted(sub[\"sender\"].unique()))\n        dates = sorted(sub[\"msg_date\"].dropna().unique()) if \"msg_date\" in sub.columns else []\n        date_range = f\", dates: {dates[0]}..{dates[-1]}\" if dates else \"\"\n        print(f\"  - {name}: {len(sub)} msgs, senders: {senders}{date_range}\")\n\n    print(\"\\nOpening interactive dashboard (Seaborn)...\")\n    dashboard = Dashboard(df)\n    dashboard.show()\n\n\nif __name__ == \"__main__\":\n    main()\n","path":null,"size_bytes":24627,"size_tokens":null},"README.md":{"content":"# WhatsApp Chat Exporter - Chrome Extension\n\nExport your WhatsApp Web chats to JSON with organized media folders.\n\n## Features\n\n- ðŸ“± **Chat Selection** - Dropdown menu to select any chat from your WhatsApp\n- ðŸ’¾ **JSON Export** - Clean, structured JSON format with all message data\n- ðŸ–¼ï¸ **Media Support** - Downloads images and organizes them in folders\n- â±ï¸ **Timestamps** - Include or exclude message timestamps\n- ðŸ—‘ï¸ **Deleted Messages** - Option to include deleted message markers\n- ðŸŽ¨ **Beautiful UI** - Modern, WhatsApp-inspired dark theme\n\n## Installation\n\n### Developer Mode (Unpacked Extension)\n\n1. Download or clone this repository\n2. Open Chrome and navigate to `chrome://extensions/`\n3. Enable **Developer mode** (toggle in top-right corner)\n4. Click **Load unpacked**\n5. Select the `whatsapp-exporter` folder\n6. The extension icon should appear in your toolbar\n\n## Usage\n\n1. Open [WhatsApp Web](https://web.whatsapp.com) in Chrome\n2. Log in and wait for your chats to load\n3. Click the extension icon in your toolbar\n4. Select a chat from the dropdown menu\n5. Configure export options:\n   - **Include images & videos** - Download media files\n   - **Include timestamps** - Add time to each message\n   - **Include deleted messages** - Show \"[Message was deleted]\" markers\n6. Click **Export Chat**\n7. Your files will be downloaded automatically\n\n## Export Format\n\n### JSON Structure\n\n```json\n{\n  \"exportInfo\": {\n    \"chatName\": \"Contact Name\",\n    \"exportDate\": \"2024-01-15T10:30:00.000Z\",\n    \"totalMessages\": 150,\n    \"totalMedia\": 12,\n    \"exportedBy\": \"WhatsApp Chat Exporter Extension\"\n  },\n  \"messages\": [\n    {\n      \"id\": \"msg_123456789_abc123\",\n      \"type\": \"text\",\n      \"content\": \"Hello!\",\n      \"timestamp\": \"10:30 AM\",\n      \"sender\": \"You\",\n      \"isOutgoing\": true,\n      \"isDeleted\": false,\n      \"media\": null\n    },\n    {\n      \"id\": \"msg_123456790_def456\",\n      \"type\": \"image\",\n      \"content\": \"\",\n      \"timestamp\": \"10:31 AM\",\n      \"sender\": \"Contact Name\",\n      \"isOutgoing\": false,\n      \"isDeleted\": false,\n      \"media\": {\n        \"type\": \"image\",\n        \"exportedFilename\": \"image_1.jpg\"\n      }\n    }\n  ],\n  \"mediaFiles\": [\n    {\n      \"messageId\": \"msg_123456790_def456\",\n      \"type\": \"image\",\n      \"filename\": \"image_1.jpg\"\n    }\n  ]\n}\n```\n\n### File Structure\n\n```\nDownloads/\nâ”œâ”€â”€ whatsapp_Contact_Name_2024-01-15.json\nâ””â”€â”€ whatsapp_Contact_Name_2024-01-15_media/\n    â”œâ”€â”€ image_1.jpg\n    â”œâ”€â”€ image_2.jpg\n    â””â”€â”€ video_1.mp4\n```\n\n## Message Types\n\n- `text` - Regular text messages\n- `image` - Photo messages\n- `video` - Video messages\n- `audio` - Voice messages\n- `document` - Document/file attachments\n- `deleted` - Deleted message markers\n\n## Troubleshooting\n\n### \"No chats found\"\n- Make sure you're on [web.whatsapp.com](https://web.whatsapp.com)\n- Ensure you're logged in and chats are visible\n- Click \"Refresh Chat List\" to reload\n\n### \"Failed to load chats\"\n- Refresh the WhatsApp Web page\n- Check if WhatsApp Web is fully loaded\n- Try reopening the extension popup\n\n### Media not downloading\n- Some media may not be available if not loaded in chat\n- Scroll through the chat to load media before exporting\n- Check your Chrome download settings\n\n## Privacy & Security\n\n- **Local Only** - All processing happens in your browser\n- **No Server** - No data is sent to external servers\n- **No Storage** - Extension doesn't store your chat data\n- **Open Source** - Full source code available for review\n\n## Limitations\n\n- Only works with WhatsApp Web (not the desktop app)\n- Media must be loaded in the chat to be exported\n- Very long chats may take time to process\n- Some media types may not be fully supported\n\n## Technical Details\n\n- **Manifest Version**: 3\n- **Permissions**: \n  - `activeTab` - Access current tab\n  - `scripting` - Inject content scripts\n  - `downloads` - Save exported files\n- **Host Permissions**: `https://web.whatsapp.com/*`\n\n## Version History\n\n### 1.0.0\n- Initial release\n- Chat selection dropdown\n- JSON export with messages\n- Media download support\n- Export options (timestamps, deleted messages)\n\n## License\n\nMIT License - Feel free to modify and distribute.\n\n## Disclaimer\n\nThis extension is not affiliated with WhatsApp or Meta. Use responsibly and respect others' privacy. Only export chats you have permission to export.\n","path":null,"size_bytes":4348,"size_tokens":null},"data/dashboard_web.py":{"content":"#!/usr/bin/env python3\n\"\"\"Browser-based interactive WhatsApp chat dashboard using Plotly Dash.\n\nReads all whatsapp_*.json exports, applies noise filtering, and provides\na rich interactive dashboard at http://0.0.0.0:5000.\n\nUsage:\n    pip install dash dash-bootstrap-components\n    python3 dashboard_web.py\n\"\"\"\n\nimport base64\nimport glob\nimport json\nimport os\nimport re\nimport unicodedata\nfrom datetime import datetime, date, timedelta\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom flask import request as flask_request, jsonify as flask_jsonify\nfrom dash import ALL, Dash, Input, Output, State, callback_context, dash_table, dcc, html, no_update\nfrom dash.exceptions import PreventUpdate\nimport dash_bootstrap_components as dbc\n\nfrom invoice_parser import (\n    parse_invoice_bytes,\n    match_invoice_to_deployment,\n    reconcile_invoice_with_chat,\n)\n\nfrom domain_model import (\n    load_config, build_job_logs, build_crew_summary, build_route_segments,\n    build_deployment_burndown, build_location_type_stats, build_traffic_analysis,\n    build_delay_report, infer_location_type,\n    build_location_coords, haversine_km,\n    count_billable_routes, get_billable_routes_df, billable_route_key,\n)\n\nDATA_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# â”€â”€ Timestamp parsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef parse_msg_datetime(msg, export_date=None):\n    \"\"\"Resolve a message's datetime using the best available field.\n\n    Priority:\n      1. ``dateTime`` (ISO 8601, e.g. \"2026-02-10T14:40:00\")\n      2. ``date`` + ``timestamp`` combined\n      3. ``timestamp`` + *export_date* fallback\n    \"\"\"\n    # 1. Try dateTime field (most accurate)\n    dt_str = msg.get(\"dateTime\")\n    if dt_str:\n        try:\n            return datetime.fromisoformat(dt_str)\n        except (ValueError, TypeError):\n            pass\n\n    ts_str = (msg.get(\"timestamp\") or \"\").strip()\n    date_str = msg.get(\"date\")  # e.g. \"2026-02-10\"\n\n    # 2. Try date + timestamp combined\n    if date_str and ts_str:\n        try:\n            d = date.fromisoformat(date_str)\n            t = datetime.strptime(ts_str, \"%I:%M %p\")\n            return t.replace(year=d.year, month=d.month, day=d.day)\n        except (ValueError, TypeError):\n            pass\n\n    # 3. Fallback: timestamp + export_date\n    if ts_str:\n        try:\n            t = datetime.strptime(ts_str, \"%I:%M %p\")\n            if export_date:\n                return t.replace(year=export_date.year, month=export_date.month,\n                                 day=export_date.day)\n            return t.replace(year=2026, month=2, day=8)\n        except ValueError:\n            pass\n\n    return None\n\n\n# â”€â”€ Data loading & cleaning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n# Patterns for content cleaning\nRE_PHONE = re.compile(r\"\\+1\\s*\\(\\d{3}\\)\\s*\\d{3}[- ]?\\d{4}\")\nUI_TOKENS = [\"wds-ic-hd-filled\", \"tail-in\", \"forward-refreshed\",\n             \"default-group-refreshed\", \"camera-v2\"]\nRE_TRAILING_TIME = re.compile(r\"\\d{1,2}:\\d{2}\\s*[AP]M\\s*$\")\n\n# Patterns for detecting fake senders (locations / captions the exporter\n# mistakenly placed in the sender field).\n_RE_ADDR_SUFFIX = re.compile(\n    r\"\\b(st|ave|rd|street|drive|dr|road)\\s+(nw|ne|sw|se)\\b\", re.I)\n_RE_INTERSECTION = re.compile(\n    r\"\\b(st|ave|av|rd|street)\\b.*&|\\&.*\\b(st|ave|av|rd|street)\\b\", re.I)\n_RE_LOCATION_WORD = re.compile(\n    r\"\\b(school|elementary|housing|annex|camp|education|church|park)\\b\", re.I)\n_RE_PHONE_SENDER = re.compile(r\"^[@+]\")\n_RE_PHOTO_COUNT = re.compile(r\"^\\d+\\s+photos?$\", re.I)\n\n\ndef _is_fake_sender(name):\n    \"\"\"Return True if *name* looks like a location, address, or UI artifact\n    rather than a real person's name.\"\"\"\n    if not name:\n        return False\n    if _RE_ADDR_SUFFIX.search(name):\n        return True\n    if _RE_INTERSECTION.search(name):\n        return True\n    if _RE_LOCATION_WORD.search(name):\n        return True\n    if _RE_PHONE_SENDER.match(name):\n        return True\n    if _RE_PHOTO_COUNT.match(name):\n        return True\n    return False\n\n\ndef clean_content(text):\n    \"\"\"Strip CSS blobs, phone numbers, UI tokens, trailing timestamps.\"\"\"\n    if not text:\n        return \"\"\n    # Remove CSS blocks\n    text = re.sub(r\"\\.cdf40d50ba[\\s\\S]*\", \"\", text)\n    # Remove phone numbers\n    text = RE_PHONE.sub(\"\", text)\n    # Remove UI tokens\n    for tok in UI_TOKENS:\n        text = text.replace(tok, \"\")\n    # Remove trailing duplicate timestamps\n    text = RE_TRAILING_TIME.sub(\"\", text)\n    # Collapse whitespace\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\n# â”€â”€ Location extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n# Street address pattern: optional number, street name, suffix + direction\n_RE_STREET_ADDR = re.compile(\n    r\"\\d{1,5}[-â€“]?\\d{0,5}\\s+[\\w\\s]{2,40}\\b\"\n    r\"(st|ave|avenue|rd|road|drive|dr|street|blvd|boulevard|pl|place|ct|court\"\n    r\"|way|ln|lane)\\s+(nw|ne|sw|se)\\b\",\n    re.I,\n)\n\n# Named-place pattern: content contains a known place keyword\n_RE_NAMED_PLACE = re.compile(\n    r\"\\b(school|elementary|recreation\\s+center|community\\s+center\"\n    r\"|church|housing|annex|camp|park|center|library|plaza)\\b\",\n    re.I,\n)\n\n# Things that look like locations but aren't\n_RE_SUPPLY_COUNT = re.compile(r\"^\\d+\\s+(bags?|boxes?|loads?|pallets?|tons?)$\", re.I)\n_RE_IMAGE_TAG = re.compile(r\"^\\[Image\\]$\", re.I)\n\n\ndef _normalize_location(loc):\n    \"\"\"Lowercase, collapse whitespace, normalize street abbreviations.\"\"\"\n    loc = loc.lower().strip()\n    loc = re.sub(r\"\\s+\", \" \", loc)\n    # Normalize common abbreviations\n    loc = re.sub(r\"\\bavenue\\b\", \"ave\", loc)\n    loc = re.sub(r\"\\bstreet\\b\", \"st\", loc)\n    loc = re.sub(r\"\\broad\\b\", \"rd\", loc)\n    loc = re.sub(r\"\\bdrive\\b\", \"dr\", loc)\n    loc = re.sub(r\"\\bboulevard\\b\", \"blvd\", loc)\n    loc = re.sub(r\"\\bplace\\b\", \"pl\", loc)\n    loc = re.sub(r\"\\bcourt\\b\", \"ct\", loc)\n    loc = re.sub(r\"\\blane\\b\", \"ln\", loc)\n    return loc\n\n\n_ABBREV_MAP = {\n    \"street\": \"st\", \"st\": \"street\",\n    \"avenue\": \"ave\", \"ave\": \"avenue\",\n    \"drive\": \"dr\", \"dr\": \"drive\",\n    \"road\": \"rd\", \"rd\": \"road\",\n    \"boulevard\": \"blvd\", \"blvd\": \"boulevard\",\n    \"place\": \"pl\", \"pl\": \"place\",\n    \"court\": \"ct\", \"ct\": \"court\",\n    \"lane\": \"ln\", \"ln\": \"lane\",\n}\n\n\ndef _fuzzy_match_location(name, known_locations):\n    if not name or not known_locations:\n        return (\"\", 0)\n    norm = _normalize_location(name)\n    for loc in known_locations:\n        if norm == _normalize_location(loc):\n            return (loc, 100)\n    for loc in known_locations:\n        nloc = _normalize_location(loc)\n        if norm in nloc or nloc in norm:\n            return (loc, 80)\n    norm_words = norm.split()\n    expanded_variants = set()\n    expanded_variants.add(norm)\n    for i, w in enumerate(norm_words):\n        if w in _ABBREV_MAP:\n            variant = norm_words[:i] + [_ABBREV_MAP[w]] + norm_words[i+1:]\n            expanded_variants.add(\" \".join(variant))\n    for loc in known_locations:\n        nloc = _normalize_location(loc)\n        for variant in expanded_variants:\n            if variant in nloc or nloc in variant:\n                return (loc, 60)\n    return (\"\", 0)\n\n\ndef _extract_crew_from_chat(chat_name):\n    \"\"\"Extract crew name and type (sidewalk/parking) from chat group name.\"\"\"\n    name = chat_name.strip()\n    is_sidewalk = bool(re.search(r'sidewalk', name, re.IGNORECASE))\n    is_parking = bool(re.search(r'parking', name, re.IGNORECASE))\n    crew = re.sub(r'\\s*(sidewalk|parking\\s*lot|parking)s?\\s*$', '', name, flags=re.IGNORECASE).strip()\n    crew = re.sub(r'\\s*QAQC\\s*$', '', crew, flags=re.IGNORECASE).strip()\n    return crew, is_sidewalk, is_parking\n\n\ndef _build_location_crew_map(df):\n    \"\"\"Build mapping: normalized_location -> {sidewalk_crews: set, parking_crews: set}.\"\"\"\n    loc_crew = {}\n    if df.empty:\n        return loc_crew\n    loc_df = df[(df[\"location\"].str.len() > 0) & (df[\"noise_type\"] == \"clean\")]\n    for _, row in loc_df.iterrows():\n        loc = row[\"location\"]\n        chat = row[\"chat\"]\n        crew, is_sw, is_pl = _extract_crew_from_chat(chat)\n        if not crew:\n            continue\n        norm_loc = _normalize_location(loc)\n        if norm_loc not in loc_crew:\n            loc_crew[norm_loc] = {\"sidewalk\": set(), \"parking\": set()}\n        if is_sw:\n            loc_crew[norm_loc][\"sidewalk\"].add(crew)\n        elif is_pl:\n            loc_crew[norm_loc][\"parking\"].add(crew)\n    return loc_crew\n\n\ndef _find_crews_for_location(search_terms, loc_crew_map):\n    \"\"\"Try multiple search terms to find crew assignments from chat data.\"\"\"\n    for term in search_terms:\n        if not term:\n            continue\n        norm = _normalize_location(term)\n        crews = loc_crew_map.get(norm)\n        if crews and (crews[\"sidewalk\"] or crews[\"parking\"]):\n            return crews\n        for nloc, c in loc_crew_map.items():\n            if norm in nloc or nloc in norm:\n                if c[\"sidewalk\"] or c[\"parking\"]:\n                    return c\n    return None\n\n\ndef _auto_assign_crews(registry, df):\n    \"\"\"Auto-assign crews to registry locations based on chat location reports.\"\"\"\n    loc_crew_map = _build_location_crew_map(df)\n    if not loc_crew_map:\n        return registry, 0\n\n    assigned = 0\n    for entry in registry:\n        matched_raw = entry.get(\"_matched_raw\", \"\")\n        if not matched_raw:\n            mcl = entry.get(\"matched_chat_location\", \"\")\n            if mcl and \"(\" in mcl:\n                matched_raw = mcl.split(\"(\")[0].strip()\n\n        search_terms = [matched_raw, entry.get(\"address\", \"\"), entry.get(\"location_name\", \"\")]\n        crews = _find_crews_for_location(search_terms, loc_crew_map)\n        if not crews:\n            continue\n\n        changed = False\n        if crews[\"sidewalk\"] and not entry.get(\"crew_sidewalk\", \"\"):\n            entry[\"crew_sidewalk\"] = \", \".join(sorted(crews[\"sidewalk\"]))\n            changed = True\n        if crews[\"parking\"] and not entry.get(\"crew_parking_lot\", \"\"):\n            entry[\"crew_parking_lot\"] = \", \".join(sorted(crews[\"parking\"]))\n            changed = True\n        if changed:\n            assigned += 1\n\n    return registry, assigned\n\n\ndef extract_location(content):\n    \"\"\"Extract a location from message content, or return empty string.\n\n    Two-tier approach:\n      1. Street addresses like \"310-0324 Kennedy St NW\"\n      2. Named places containing keywords like \"school\", \"recreation center\"\n\n    Excludes [Image] tags, supply counts (\"5 bags\"), and very short/long strings.\n    \"\"\"\n    if not content:\n        return \"\"\n    # Skip non-location content\n    if _RE_IMAGE_TAG.match(content):\n        return \"\"\n    if _RE_SUPPLY_COUNT.match(content):\n        return \"\"\n    # Too short or too long to be a useful location\n    if len(content) < 5 or len(content) > 200:\n        return \"\"\n\n    # Tier 1: street address\n    m = _RE_STREET_ADDR.search(content)\n    if m:\n        return _normalize_location(m.group(0))\n\n    # Tier 2: named place â€” use the full content as the location\n    if _RE_NAMED_PLACE.search(content):\n        # Trim to first line / sentence for cleanliness\n        loc = content.split(\"\\n\")[0].strip()\n        if len(loc) > 100:\n            loc = loc[:100]\n        return _normalize_location(loc)\n\n    return \"\"\n\n\ndef classify_noise(msg):\n    \"\"\"Classify a message into a noise type.\n\n    Accepts the full message dict so v1.5+ fields (dateTime, date,\n    isDeleted, isForwarded) can be used for filtering.\n    \"\"\"\n    if msg.get(\"isDeleted\"):\n        return \"deleted\"\n\n    sender = msg.get(\"sender\", \"\") or \"\"\n    ts_str = msg.get(\"timestamp\", \"\") or \"\"\n    content = msg.get(\"content\", \"\") or \"\"\n\n    if not sender and not ts_str:\n        return \"system_metadata\"\n    if len(content) > 500 and \".cdf40d50ba\" in content:\n        return \"css_html\"\n    if \"This message couldn\\u0027t load\" in content or \\\n       \"This message couldn\\u2019t load\" in content:\n        return \"load_error\"\n    if ts_str and not sender:\n        if msg.get(\"dateTime\") or msg.get(\"date\"):\n            return \"clean\"\n        return \"empty_sender_caption\"\n    return \"clean\"\n\n\ndef parse_export_date(iso_str):\n    \"\"\"Extract date from ISO 8601 exportDate string.\"\"\"\n    try:\n        return datetime.fromisoformat(iso_str.replace(\"Z\", \"+00:00\")).date()\n    except (ValueError, AttributeError):\n        return date(2026, 2, 8)\n\n\nEXCLUDE_DIRS = {\".claude\", \"__pycache__\", \"node_modules\", \"config\"}\n\n\ndef _normalize_chat_name(name):\n    \"\"\"Normalize a chat name for de-duplication.\n\n    Folds accents (Ã±â†’n, Ã©â†’e) and lowercases so that variants like\n    \"Julian OrdoÃ±ez Sidewalks\" / \"Julian Ordonez sidewalks\" merge.\n    \"\"\"\n    nfkd = unicodedata.normalize(\"NFKD\", name)\n    stripped = \"\".join(ch for ch in nfkd if unicodedata.category(ch) != \"Mn\")\n    return stripped.lower().strip()\n\n\ndef _classify_json_file(path):\n    \"\"\"Classify a JSON file by its schema.\n\n    Returns one of:\n      - 'combined'  : Combined_Messages.json (unified multi-chat export)\n      - 'legacy'    : Per-chat WhatsApp export (exportInfo + messages)\n      - 'metrics'   : metrics_report.json (pre-computed metrics)\n      - 'dispatches': OCR dispatch data\n      - 'unknown'   : Unrecognised format\n    \"\"\"\n    basename = os.path.basename(path)\n    if basename.endswith(\"_dispatches.json\"):\n        return \"dispatches\"\n    try:\n        with open(path, encoding=\"utf-8\") as f:\n            data = json.load(f)\n    except (json.JSONDecodeError, OSError):\n        return \"unknown\"\n\n    if not isinstance(data, dict):\n        return \"unknown\"\n\n    if \"summary\" in data and \"crew_metrics\" in data:\n        return \"metrics\"\n\n    if \"exportInfo\" not in data or \"messages\" not in data:\n        return \"unknown\"\n\n    msgs = data.get(\"messages\", [])\n    if msgs and isinstance(msgs[0], dict) and \"chatName\" in msgs[0]:\n        return \"combined\"\n\n    return \"legacy\"\n\n\ndef _discover_json_files(data_dir):\n    \"\"\"Find all JSON files under *data_dir* and classify them.\n\n    Returns (chat_paths, metrics_path) where chat_paths is a list of\n    legacy or combined chat export files, and metrics_path is the path\n    to a metrics_report.json if found (else None).\n    \"\"\"\n    chat_paths = []\n    metrics_path = None\n\n    all_json = []\n    for p in glob.glob(os.path.join(data_dir, \"archive\", \"*.json\")):\n        all_json.append(p)\n    for root, dirs, files in os.walk(data_dir):\n        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS and d != \"archive\"]\n        for fn in files:\n            if fn.endswith(\".json\"):\n                all_json.append(os.path.join(root, fn))\n\n    for p in sorted(set(all_json)):\n        kind = _classify_json_file(p)\n        if kind in (\"legacy\", \"combined\"):\n            chat_paths.append(p)\n        elif kind == \"metrics\":\n            metrics_path = p\n\n    return chat_paths, metrics_path\n\n\ndef _process_message(m, chat_name, export_date, prev_sender_ref):\n    \"\"\"Process a single message dict into a row dict for the DataFrame.\"\"\"\n    msg_id = m.get(\"id\", \"\")\n    sender = m.get(\"sender\", \"\") or \"\"\n    ts_str = m.get(\"timestamp\", \"\") or \"\"\n    content_raw = m.get(\"content\", \"\") or \"\"\n    msg_type = m.get(\"type\", \"text\") or \"text\"\n    is_outgoing = bool(m.get(\"isOutgoing\", False))\n    is_forwarded = bool(m.get(\"isForwarded\", False))\n    is_deleted = bool(m.get(\"isDeleted\", False))\n    ts = parse_msg_datetime(m, export_date)\n\n    media_obj = m.get(\"media\")\n    media_type = media_obj.get(\"type\", \"\") if isinstance(media_obj, dict) else \"\"\n    media_src = media_obj.get(\"exportedFilename\", \"\") if isinstance(media_obj, dict) else \"\"\n    has_media = media_obj is not None\n\n    if _is_fake_sender(sender):\n        if not content_raw or content_raw == \"[Image]\":\n            content_raw = sender\n        sender = \"\"\n\n    noise = classify_noise(m)\n    content_clean = clean_content(content_raw)\n\n    sender_resolved = sender if sender else prev_sender_ref[0]\n    if sender:\n        prev_sender_ref[0] = sender\n\n    msg_date = ts.date() if ts else (export_date if export_date else None)\n\n    return {\n        \"chat\": chat_name,\n        \"sender\": sender,\n        \"sender_resolved\": sender_resolved,\n        \"timestamp\": ts_str,\n        \"time\": ts,\n        \"hour\": ts.hour + ts.minute / 60.0 if ts else None,\n        \"hour_int\": ts.hour if ts else None,\n        \"msg_date\": msg_date,\n        \"type\": msg_type,\n        \"content_raw\": content_raw,\n        \"content\": content_clean,\n        \"content_len\": len(content_clean),\n        \"location\": extract_location(content_clean),\n        \"noise_type\": noise,\n        \"export_date\": export_date,\n        \"msg_id\": msg_id,\n        \"is_outgoing\": is_outgoing,\n        \"is_forwarded\": is_forwarded,\n        \"is_deleted\": is_deleted,\n        \"has_media\": has_media,\n        \"media_type\": media_type,\n        \"media_file\": media_src,\n    }\n\n\ndef _load_combined_file(path):\n    \"\"\"Load a Combined_Messages.json (unified multi-chat export).\n\n    Each message has a ``chatName`` field identifying which crew/chat it\n    belongs to. Messages are de-duplicated by ``id``.\n    \"\"\"\n    with open(path, encoding=\"utf-8\") as f:\n        data = json.load(f)\n    export_date = parse_export_date(data.get(\"exportInfo\", {}).get(\"exportDate\", \"\"))\n    msgs = data.get(\"messages\", [])\n\n    rows = []\n    seen_ids = set()\n    prev_by_chat = {}\n    for m in msgs:\n        msg_id = m.get(\"id\", \"\")\n        if msg_id:\n            if msg_id in seen_ids:\n                continue\n            seen_ids.add(msg_id)\n        chat_name = m.get(\"chatName\", data.get(\"exportInfo\", {}).get(\"chatName\", \"Unknown\"))\n        if chat_name not in prev_by_chat:\n            prev_by_chat[chat_name] = [\"\"]\n        row = _process_message(m, chat_name, export_date, prev_by_chat[chat_name])\n        rows.append(row)\n    return rows\n\n\ndef _load_legacy_files(paths):\n    \"\"\"Load legacy per-chat WhatsApp JSON exports.\n\n    De-duplicates by normalized chatName (accent-folded, lowercased),\n    keeping the file with the most messages.\n    \"\"\"\n    chat_files = {}\n    for path in paths:\n        try:\n            with open(path, encoding=\"utf-8\") as f:\n                data = json.load(f)\n            if \"exportInfo\" not in data or \"messages\" not in data:\n                continue\n        except (json.JSONDecodeError, KeyError):\n            continue\n        name = data[\"exportInfo\"][\"chatName\"]\n        key = _normalize_chat_name(name)\n        msgs = data[\"messages\"]\n        export_date = parse_export_date(data[\"exportInfo\"].get(\"exportDate\", \"\"))\n        if key not in chat_files or len(msgs) > len(chat_files[key][1]):\n            chat_files[key] = (path, msgs, export_date, name)\n\n    rows = []\n    for _key, (path, msgs, export_date, chat_name) in chat_files.items():\n        prev_sender_ref = [\"\"]\n        seen_ids = set()\n        for m in msgs:\n            msg_id = m.get(\"id\", \"\")\n            if msg_id:\n                if msg_id in seen_ids:\n                    continue\n                seen_ids.add(msg_id)\n            row = _process_message(m, chat_name, export_date, prev_sender_ref)\n            rows.append(row)\n    return rows\n\n\ndef load_all_data(data_dir):\n    \"\"\"Load all JSON chat files, build DataFrame.\n\n    Supports three formats:\n      1. Combined_Messages.json â€” unified multi-chat export with per-message chatName\n      2. Legacy per-chat exports â€” individual files with exportInfo.chatName\n      3. v2 CrewChatData format â€” legacy with id, isOutgoing, isForwarded, isDeleted fields\n\n    Chat names are normalized (case-insensitive, accent-folded) for de-duplication.\n    Messages are de-duplicated by their ``id`` field to prevent inflation from re-exports.\n    \"\"\"\n    chat_paths, metrics_path = _discover_json_files(data_dir)\n\n    combined_paths = []\n    legacy_paths = []\n    for p in chat_paths:\n        kind = _classify_json_file(p)\n        if kind == \"combined\":\n            combined_paths.append(p)\n        else:\n            legacy_paths.append(p)\n\n    rows = []\n    for cp in combined_paths:\n        try:\n            rows.extend(_load_combined_file(cp))\n        except Exception as e:\n            print(f\"  Error loading combined file {cp}: {e}\")\n    if legacy_paths:\n        rows.extend(_load_legacy_files(legacy_paths))\n\n    df = pd.DataFrame(rows)\n    if df.empty:\n        df = pd.DataFrame(columns=[\n            \"chat\", \"sender\", \"sender_resolved\", \"timestamp\", \"time\",\n            \"hour\", \"hour_int\", \"msg_date\", \"type\", \"content_raw\",\n            \"content\", \"content_len\", \"location\", \"noise_type\", \"export_date\",\n            \"msg_id\", \"is_outgoing\", \"is_forwarded\", \"is_deleted\",\n            \"has_media\", \"media_type\", \"media_file\",\n        ])\n    df[\"export_date\"] = pd.to_datetime(df[\"export_date\"])\n    df[\"msg_date\"] = pd.to_datetime(df[\"msg_date\"])\n\n    config_path = os.path.join(data_dir, \"config\", \"snow_removal.json\")\n    try:\n        with open(config_path, encoding=\"utf-8\") as f:\n            cfg = json.load(f)\n        crew_merges = cfg.get(\"crew_merges\", {})\n        if crew_merges and not df.empty:\n            df[\"chat\"] = df[\"chat\"].replace(crew_merges)\n    except (FileNotFoundError, json.JSONDecodeError):\n        pass\n\n    return df\n\n\ndef load_metrics_report(data_dir):\n    \"\"\"Load metrics_report.json if present.\n\n    Returns dict with keys: summary, crew_metrics, daily_breakdown,\n    productivity_scores, site_visits.  Returns None if not found.\n    \"\"\"\n    _, metrics_path = _discover_json_files(data_dir)\n    if not metrics_path:\n        return None\n    try:\n        with open(metrics_path, encoding=\"utf-8\") as f:\n            data = json.load(f)\n        required = {\"summary\", \"crew_metrics\"}\n        if not required.issubset(data.keys()):\n            return None\n        return data\n    except (json.JSONDecodeError, OSError) as e:\n        print(f\"  Error loading metrics report: {e}\")\n        return None\n\n\n# â”€â”€ Build global DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nDF_ALL = load_all_data(DATA_DIR)\nMETRICS_REPORT = load_metrics_report(DATA_DIR)\nSNOW_CONFIG = load_config(os.path.join(DATA_DIR, \"config\", \"snow_removal.json\"))\n\n\ndef load_pricing(path):\n    try:\n        with open(path, encoding=\"utf-8\") as f:\n            return json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError):\n        return []\n\n\nPRICING_DATA = load_pricing(os.path.join(DATA_DIR, \"config\", \"pricing.json\"))\nPRICING_INDEX = {}\nfor p in PRICING_DATA:\n    addr_norm = _normalize_location(p.get(\"address\", \"\"))\n    if addr_norm:\n        PRICING_INDEX[addr_norm] = p\n    name_norm = _normalize_location(p.get(\"building_name\", \"\"))\n    if name_norm:\n        PRICING_INDEX[name_norm] = p\n\n\ndef _match_location_to_pricing(location, pricing_index):\n    if not location:\n        return None\n    norm = _normalize_location(location)\n    if not norm or len(norm) < 4:\n        return None\n    if norm in pricing_index:\n        return pricing_index[norm]\n    for key, entry in pricing_index.items():\n        if len(key) < 6 or len(norm) < 6:\n            continue\n        if norm in key or key in norm:\n            return entry\n    return None\n\n\ndef _compute_financials(df, deployments_list, snow_config, pricing_index):\n    finance_cfg = snow_config.get(\"finance_config\", {})\n    labor_rate_sw = finance_cfg.get(\"labor_rate_sidewalk\", 25.0)\n    labor_rate_pl = finance_cfg.get(\"labor_rate_parking\", 30.0)\n    machine_rate = finance_cfg.get(\"machine_hourly_rate\", 75.0)\n    salt_cost_lb = finance_cfg.get(\"salt_cost_per_lb\", 0.15)\n    salt_lbs_sw = finance_cfg.get(\"salt_lbs_per_site_sidewalk\", 50.0)\n    salt_lbs_pl = finance_cfg.get(\"salt_lbs_per_site_parking\", 200.0)\n    workers_sw = finance_cfg.get(\"workers_sidewalk\", 3)\n    workers_pl = finance_cfg.get(\"workers_parking\", 2)\n    overhead_pct = finance_cfg.get(\"overhead_pct\", 10.0)\n    default_tier = finance_cfg.get(\"default_snow_tier\", \"price_under_6in\")\n    deployment_tiers = finance_cfg.get(\"deployment_snow_tiers\", {})\n\n    empty_result = {\n        \"deployment_financials\": [], \"crew_financials\": [],\n        \"total_revenue\": 0, \"total_costs\": 0, \"total_profit\": 0,\n        \"profit_margin\": 0, \"sites_serviced\": 0, \"sites_contracted\": len(PRICING_DATA),\n        \"avg_revenue_per_site\": 0, \"avg_cost_per_crew\": 0,\n        \"revenue_per_deployment\": 0, \"total_salt_lbs\": 0,\n        \"cost_per_hour\": 0, \"revenue_per_hour\": 0, \"profit_per_hour\": 0,\n    }\n    if df.empty or not deployments_list:\n        return empty_result\n\n    df_clean = df[df[\"noise_type\"] == \"clean\"].copy()\n    non_trackable = set(snow_config.get(\"non_trackable_senders\", []))\n    if non_trackable:\n        df_clean = df_clean[~df_clean[\"sender_resolved\"].isin(non_trackable)]\n\n    dep_financials = []\n    crew_data = {}\n    total_revenue = 0\n    total_costs = 0\n    total_salt = 0\n    total_hours = 0\n    all_serviced_locations = set()\n\n    chat_crew_type = {}\n    for chat in df_clean[\"chat\"].unique():\n        _, is_sw, is_pl = _extract_crew_from_chat(chat)\n        if is_pl:\n            chat_crew_type[chat] = \"Parking Lot\"\n        elif is_sw:\n            chat_crew_type[chat] = \"Sidewalk\"\n        else:\n            chat_crew_type[chat] = \"Sidewalk\"\n\n    invoices_list = snow_config.get(\"invoices\", [])\n    invoice_by_dep = {}\n    for inv in invoices_list:\n        dep = inv.get(\"matched_deployment\")\n        if dep:\n            if dep not in invoice_by_dep:\n                invoice_by_dep[dep] = []\n            invoice_by_dep[dep].append(inv)\n\n    for dep in deployments_list:\n        label = dep[\"label\"]\n        start = pd.Timestamp(dep[\"start_date\"])\n        end = pd.Timestamp(dep[\"end_date\"])\n\n        dep_invoices = invoice_by_dep.get(label, [])\n        if dep_invoices:\n            tier = dep_invoices[0].get(\"snow_tier\", default_tier)\n        else:\n            tier = deployment_tiers.get(label, default_tier)\n\n        dep_df = df_clean[(df_clean[\"msg_date\"] >= start) & (df_clean[\"msg_date\"] <= end + pd.Timedelta(days=1))]\n        if dep_df.empty:\n            continue\n\n        dep_locations = dep_df[dep_df[\"location\"].str.len() > 0][\"location\"].unique()\n\n        if dep_invoices:\n            dep_revenue = sum(inv.get(\"total_billed\", 0) for inv in dep_invoices)\n            matched_sites = sum(inv.get(\"site_count\", 0) for inv in dep_invoices)\n            for loc in dep_locations:\n                all_serviced_locations.add(loc)\n        else:\n            dep_revenue = 0\n            matched_sites = 0\n            loc_service_pairs = set()\n            for loc in dep_locations:\n                loc_chats = dep_df[dep_df[\"location\"] == loc][\"chat\"].unique()\n                sas = set()\n                for ch in loc_chats:\n                    sas.add(chat_crew_type.get(ch, \"Sidewalk\"))\n                for sa in sas:\n                    loc_service_pairs.add((loc, sa))\n            for loc, sa in loc_service_pairs:\n                pricing = _match_location_to_pricing(loc, pricing_index)\n                if pricing:\n                    price = pricing.get(tier, pricing.get(\"price_under_6in\", 0))\n                    dep_revenue += price\n                    matched_sites += 1\n                    all_serviced_locations.add(loc)\n\n        dep_crews = dep_df[dep_df[\"sender_resolved\"] != \"\"][\"sender_resolved\"].unique()\n        n_crews = len(dep_crews)\n\n        dep_labor = 0\n        dep_machine = 0\n        dep_salt = 0\n        dep_hours = 0\n        sw_sites = 0\n        pl_sites = 0\n\n        billable_routes_seen = set()\n        for loc in dep_locations:\n            loc_chats = dep_df[dep_df[\"location\"] == loc][\"chat\"].unique()\n            service_areas_at_loc = set()\n            for ch in loc_chats:\n                ct = chat_crew_type.get(ch, \"Sidewalk\")\n                service_areas_at_loc.add(ct)\n            for sa in service_areas_at_loc:\n                route_key = f\"{label}|{loc}|{sa}\"\n                if route_key not in billable_routes_seen:\n                    billable_routes_seen.add(route_key)\n                    if sa == \"Parking Lot\":\n                        pl_sites += 1\n                    else:\n                        sw_sites += 1\n\n        billable_site_count = sw_sites + pl_sites\n        dep_salt_lbs = (sw_sites * salt_lbs_sw) + (pl_sites * salt_lbs_pl)\n        dep_salt = dep_salt_lbs * salt_cost_lb\n\n        labor_overrides = snow_config.get(\"labor_overrides\", {}).get(label, {})\n\n        for crew in dep_crews:\n            crew_msgs = dep_df[dep_df[\"sender_resolved\"] == crew]\n            if len(crew_msgs) < 2:\n                continue\n            time_range = crew_msgs[\"time\"].dropna()\n            if len(time_range) < 2:\n                continue\n            hrs = (time_range.max() - time_range.min()).total_seconds() / 3600\n            dep_hours += hrs\n\n            crew_chats = crew_msgs[\"chat\"].unique()\n            crew_type = \"Sidewalk\"\n            for ch in crew_chats:\n                if chat_crew_type.get(ch) == \"Parking Lot\":\n                    crew_type = \"Parking Lot\"\n                    break\n\n            crew_chat_name = crew_chats[0] if len(crew_chats) > 0 else \"\"\n            override = labor_overrides.get(crew_chat_name, {})\n            ov_rate = override.get(\"labor_rate\")\n            ov_workers = override.get(\"workers\")\n            ov_hours = override.get(\"hours\")\n\n            actual_hrs = ov_hours if ov_hours else hrs\n\n            if crew_type == \"Parking Lot\":\n                w = ov_workers if ov_workers else min(workers_pl, 2)\n                rate = ov_rate if ov_rate else labor_rate_pl\n                crew_labor = actual_hrs * rate * w\n                crew_machine = actual_hrs * machine_rate\n            else:\n                w = ov_workers if ov_workers else workers_sw\n                rate = ov_rate if ov_rate else labor_rate_sw\n                crew_labor = actual_hrs * rate * w\n                crew_machine = 0\n\n            dep_labor += crew_labor\n            dep_machine += crew_machine\n\n            if crew not in crew_data:\n                crew_data[crew] = {\"hours\": 0, \"sites\": 0, \"revenue\": 0, \"deployments\": 0,\n                                   \"labor_cost\": 0, \"machine_cost\": 0, \"salt_cost\": 0,\n                                   \"crew_type\": crew_type, \"workers\": w}\n            crew_data[crew][\"hours\"] += actual_hrs\n            crew_data[crew][\"deployments\"] += 1\n            crew_data[crew][\"labor_cost\"] += crew_labor\n            crew_data[crew][\"machine_cost\"] += crew_machine\n\n        for crew in dep_crews:\n            if crew in crew_data:\n                crew_data[crew][\"sites\"] += matched_sites / max(n_crews, 1)\n                crew_data[crew][\"revenue\"] += dep_revenue / max(n_crews, 1)\n                crew_data[crew][\"salt_cost\"] += dep_salt / max(n_crews, 1)\n\n        direct_cost = dep_labor + dep_machine + dep_salt\n        dep_overhead = direct_cost * overhead_pct / 100\n        dep_total_cost = direct_cost + dep_overhead\n        dep_profit = dep_revenue - dep_total_cost\n        dep_margin = (dep_profit / dep_revenue * 100) if dep_revenue > 0 else 0\n\n        rev_source = \"Invoice\" if dep_invoices else \"Estimated\"\n        dep_type = dep_invoices[0].get(\"deployment_type\", \"Snow Removal\") if dep_invoices else \"Snow Removal\"\n\n        dep_financials.append({\n            \"deployment\": label,\n            \"type\": dep_type,\n            \"snow_tier\": tier.replace(\"price_\", \"\").replace(\"_\", \" \").title(),\n            \"source\": rev_source,\n            \"sites_serviced\": matched_sites,\n            \"total_sites\": billable_site_count,\n            \"sw_routes\": sw_sites,\n            \"pl_routes\": pl_sites,\n            \"crews\": n_crews,\n            \"hours\": round(dep_hours, 1),\n            \"revenue\": round(dep_revenue, 2),\n            \"labor_cost\": round(dep_labor, 2),\n            \"machine_cost\": round(dep_machine, 2),\n            \"material_cost\": round(dep_salt, 2),\n            \"overhead\": round(dep_overhead, 2),\n            \"total_cost\": round(dep_total_cost, 2),\n            \"profit\": round(dep_profit, 2),\n            \"margin\": round(dep_margin, 1),\n            \"salt_lbs\": round(dep_salt_lbs, 0),\n        })\n\n        total_revenue += dep_revenue\n        total_costs += dep_total_cost\n        total_salt += dep_salt_lbs\n        total_hours += dep_hours\n\n    crew_financials = []\n    for crew, data in sorted(crew_data.items(), key=lambda x: x[1][\"revenue\"], reverse=True):\n        crew_total_cost = data[\"labor_cost\"] + data[\"machine_cost\"] + data[\"salt_cost\"]\n        crew_profit = data[\"revenue\"] - crew_total_cost\n        crew_financials.append({\n            \"crew\": crew,\n            \"type\": data[\"crew_type\"],\n            \"workers\": data[\"workers\"],\n            \"deployments\": data[\"deployments\"],\n            \"sites\": round(data[\"sites\"], 1),\n            \"hours\": round(data[\"hours\"], 1),\n            \"revenue\": round(data[\"revenue\"], 2),\n            \"labor_cost\": round(data[\"labor_cost\"], 2),\n            \"machine_cost\": round(data[\"machine_cost\"], 2),\n            \"total_cost\": round(crew_total_cost, 2),\n            \"profit\": round(crew_profit, 2),\n            \"rate_per_hour\": round(data[\"revenue\"] / max(data[\"hours\"], 0.1), 2),\n        })\n\n    total_profit = total_revenue - total_costs\n    n_deps = len([d for d in dep_financials])\n    n_crews_total = len(crew_data)\n\n    return {\n        \"deployment_financials\": dep_financials,\n        \"crew_financials\": crew_financials,\n        \"total_revenue\": round(total_revenue, 2),\n        \"total_costs\": round(total_costs, 2),\n        \"total_profit\": round(total_profit, 2),\n        \"profit_margin\": round((total_profit / total_revenue * 100) if total_revenue > 0 else 0, 1),\n        \"sites_serviced\": len(all_serviced_locations),\n        \"sites_contracted\": len(PRICING_DATA),\n        \"avg_revenue_per_site\": round(total_revenue / max(len(all_serviced_locations), 1), 2),\n        \"avg_cost_per_crew\": round(total_costs / max(n_crews_total, 1), 2),\n        \"revenue_per_deployment\": round(total_revenue / max(n_deps, 1), 2),\n        \"total_salt_lbs\": round(total_salt, 0),\n        \"cost_per_hour\": round(total_costs / max(total_hours, 0.1), 2),\n        \"revenue_per_hour\": round(total_revenue / max(total_hours, 0.1), 2),\n        \"profit_per_hour\": round(total_profit / max(total_hours, 0.1), 2),\n    }\n\n\n# Precompute lists for filters\nALL_CHATS = sorted(DF_ALL[\"chat\"].unique()) if not DF_ALL.empty else []\n_non_trackable_init = set(SNOW_CONFIG.get(\"non_trackable_senders\", []))\nALL_SENDERS = sorted(s for s in DF_ALL[DF_ALL[\"sender\"] != \"\"][\"sender\"].unique()\n                     if s not in _non_trackable_init) if not DF_ALL.empty else []\nALL_SENDERS_RESOLVED = sorted(s for s in DF_ALL[DF_ALL[\"sender_resolved\"] != \"\"][\"sender_resolved\"].unique()\n                              if s not in _non_trackable_init) if not DF_ALL.empty else []\nALL_SENDERS_RESOLVED_FULL = sorted(\n    DF_ALL[DF_ALL[\"sender_resolved\"] != \"\"][\"sender_resolved\"].unique()) if not DF_ALL.empty else []\nALL_TYPES = sorted(DF_ALL[\"type\"].unique()) if not DF_ALL.empty else []\nNOISE_TYPES = sorted(DF_ALL[\"noise_type\"].unique()) if not DF_ALL.empty else []\n\n# Date range (prefer msg_date when available, fall back to export_date)\n_date_col = DF_ALL[\"msg_date\"].dropna()\nif _date_col.empty:\n    _date_col = DF_ALL[\"export_date\"].dropna()\nDATE_MIN = _date_col.min().date() if not _date_col.empty else date(2026, 2, 8)\nDATE_MAX = _date_col.max().date() if not _date_col.empty else date(2026, 2, 8)\nALL_DATES = sorted(DF_ALL[\"msg_date\"].dropna().dt.date.unique()) if not DF_ALL.empty else []\n\n\n# â”€â”€ Deployment computation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _compute_deployments(dates, gap_hours=24):\n    \"\"\"Group sorted dates into deployments separated by gaps > gap_hours.\n\n    Returns list of dicts: {id, label, start_date, end_date, days}.\n    \"\"\"\n    if not dates:\n        return []\n    deployments = []\n    current_start = dates[0]\n    current_end = dates[0]\n    for d in dates[1:]:\n        gap_days = (d - current_end).days\n        if gap_days <= (gap_hours / 24):\n            current_end = d\n        else:\n            deployments.append({\n                \"start_date\": current_start,\n                \"end_date\": current_end,\n            })\n            current_start = d\n            current_end = d\n    deployments.append({\n        \"start_date\": current_start,\n        \"end_date\": current_end,\n    })\n    for i, dep in enumerate(deployments, 1):\n        dep[\"id\"] = i\n        s = dep[\"start_date\"]\n        e = dep[\"end_date\"]\n        dep[\"days\"] = (e - s).days + 1\n        if s.month == e.month:\n            dep[\"label\"] = f\"{s.strftime('%b %d')}\\u2013{e.strftime('%d')}\"\n        else:\n            dep[\"label\"] = f\"{s.strftime('%b %d')}\\u2013{e.strftime('%b %d')}\"\n    return deployments\n\n\nALL_DEPLOYMENTS_LIST = _compute_deployments(ALL_DATES)\n\n_date_to_deployment = {}\nfor _dep in ALL_DEPLOYMENTS_LIST:\n    _d = _dep[\"start_date\"]\n    while _d <= _dep[\"end_date\"]:\n        _date_to_deployment[_d] = _dep[\"label\"]\n        _d += timedelta(days=1)\n\nDF_ALL[\"deployment\"] = DF_ALL[\"msg_date\"].dt.date.map(_date_to_deployment)\nALL_DEPLOYMENTS = [dep[\"label\"] for dep in ALL_DEPLOYMENTS_LIST]\n\n# Stats\nTOTAL_MSGS = len(DF_ALL)\nCLEAN_MSGS = len(DF_ALL[DF_ALL[\"noise_type\"] == \"clean\"])\nNOISE_MSGS = TOTAL_MSGS - CLEAN_MSGS\nNUM_CHATS = DF_ALL[\"chat\"].nunique()\nNUM_SENDERS = len(ALL_SENDERS_RESOLVED)\n\n\n# â”€â”€ Helper: filtered DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef get_filtered_df(chats, senders, noise_types, msg_types,\n                    time_range, use_resolved, date_start=None, date_end=None,\n                    deployments=None):\n    \"\"\"Apply all sidebar filters to DF_ALL and return filtered copy.\"\"\"\n    df = DF_ALL.copy()\n\n    non_trackable = SNOW_CONFIG.get(\"non_trackable_senders\", [])\n    if non_trackable:\n        df = df[~df[\"sender_resolved\"].isin(non_trackable)]\n\n    # Date range filter (use msg_date for accurate per-message filtering)\n    if date_start:\n        df = df[df[\"msg_date\"] >= pd.Timestamp(date_start)]\n    if date_end:\n        df = df[df[\"msg_date\"] <= pd.Timestamp(date_end)]\n\n    # Deployment filter\n    if deployments:\n        df = df[df[\"deployment\"].isin(deployments)]\n\n    # Chat filter\n    if chats:\n        df = df[df[\"chat\"].isin(chats)]\n\n    # Sender filter\n    sender_col = \"sender_resolved\" if use_resolved else \"sender\"\n    if senders:\n        df = df[df[sender_col].isin(senders)]\n\n    # Noise type filter\n    if noise_types:\n        df = df[df[\"noise_type\"].isin(noise_types)]\n\n    # Message type filter\n    if msg_types:\n        df = df[df[\"type\"].isin(msg_types)]\n\n    # Time range filter (hour slider)\n    if time_range and len(time_range) == 2:\n        lo, hi = time_range\n        df = df[(df[\"hour\"].notna()) & (df[\"hour\"] >= lo) & (df[\"hour\"] < hi)]\n\n    return df\n\n\n# â”€â”€ Dash app â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\napp = Dash(\n    __name__,\n    external_stylesheets=[\n        dbc.themes.FLATLY,\n        \"https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css\",\n    ],\n    suppress_callback_exceptions=True,\n)\napp.title = \"WhatsApp Chat Dashboard\"\nserver = app.server\nserver.config[\"MAX_CONTENT_LENGTH\"] = 500 * 1024 * 1024\n\n\n@server.route(\"/api/export-report\", methods=[\"GET\"])\ndef api_export_report():\n    try:\n        return _do_export_report()\n    except Exception as e:\n        print(f\"[export] Error: {e}\")\n        return flask_jsonify({\"error\": str(e)}), 500\n\n\ndef _do_export_report():\n    fmt = flask_request.args.get(\"format\", \"json\")\n    df = DF_ALL.copy()\n    if df.empty:\n        return flask_jsonify({\"error\": \"No data loaded\"}), 400\n\n    non_trackable = set(SNOW_CONFIG.get(\"non_trackable_senders\", []))\n    if non_trackable and \"sender_resolved\" in df.columns:\n        df = df[~df[\"sender_resolved\"].isin(non_trackable)]\n\n    scol = \"sender_resolved\"\n    ds = _build_daily_summary(df, scol)\n    visits_df = _build_site_visits(df, scol)\n    trans_df = _build_transitions(visits_df) if not visits_df.empty else pd.DataFrame()\n    sc = _build_crew_scorecard(visits_df, ds) if not visits_df.empty and not ds.empty else pd.DataFrame()\n    prod_df = _build_daily_productivity_score(df, scol)\n\n    total_msgs = len(df)\n    clean_msgs = len(df[df[\"noise_type\"] == \"clean\"])\n    active_crews = df[\"chat\"].nunique()\n    total_sites = _count_billable_routes_from_df(df)\n    avg_sites_hr = round(sc[\"avg_sites_per_hour\"].mean(), 2) if not sc.empty else 0\n    avg_trans = round(sc[\"avg_transition_min\"].mean(), 1) if not sc.empty else 0\n    avg_first_str = \"N/A\"\n    if not ds.empty:\n        avg_first = ds[\"first_hour\"].mean()\n        fh = int(avg_first)\n        fm = int((avg_first - fh) * 60)\n        period = \"AM\" if fh < 12 else \"PM\"\n        dh = fh % 12 or 12\n        avg_first_str = f\"{dh}:{fm:02d} {period}\"\n\n    crew_metrics = []\n    if not sc.empty:\n        for _, r in sc.iterrows():\n            crew_metrics.append({\n                \"crew\": r[\"sender\"],\n                \"days_active\": int(r[\"days_active\"]),\n                \"total_sites\": int(r[\"total_sites\"]),\n                \"avg_sites_per_day\": r[\"avg_sites_per_day\"],\n                \"avg_sites_per_hour\": r[\"avg_sites_per_hour\"],\n                \"avg_transition_min\": r[\"avg_transition_min\"],\n                \"total_active_hrs\": r[\"total_active_hrs\"],\n            })\n\n    daily_breakdown = []\n    if not ds.empty:\n        for _, r in ds.iterrows():\n            daily_breakdown.append({\n                \"crew\": r[\"sender\"],\n                \"date\": str(r[\"date\"]),\n                \"first_report\": r[\"first_time\"].strftime(\"%I:%M %p\") if pd.notna(r[\"first_time\"]) else \"N/A\",\n                \"last_report\": r[\"last_time\"].strftime(\"%I:%M %p\") if pd.notna(r[\"last_time\"]) else \"N/A\",\n                \"window_hrs\": round(r[\"window_hrs\"], 1),\n                \"messages\": int(r[\"msg_count\"]),\n                \"avg_gap_min\": round(r[\"avg_gap_min\"], 1),\n            })\n\n    productivity_scores = []\n    if not prod_df.empty:\n        for _, r in prod_df.iterrows():\n            d = r[\"date\"]\n            date_str = str(d.date()) if hasattr(d, \"date\") and callable(d.date) else str(d)\n            productivity_scores.append({\n                \"crew\": r[\"sender\"],\n                \"date\": date_str,\n                \"productivity_score\": float(r[\"productivity_score\"]),\n                \"pace_score\": round(float(r[\"pace_score\"]), 1),\n                \"punctuality_score\": round(float(r[\"punctuality_score\"]), 1),\n                \"coverage_score\": round(float(r.get(\"coverage\", 0)), 1),\n                \"sites_visited\": int(r[\"sites_visited\"]),\n                \"sites_per_hour\": round(float(r[\"sites_per_hour\"]), 2),\n            })\n\n    site_details = []\n    if not visits_df.empty:\n        for _, r in visits_df.iterrows():\n            d = r[\"date\"]\n            date_str = str(d.date()) if hasattr(d, \"date\") and callable(d.date) else str(d)\n            site_details.append({\n                \"crew\": r[\"sender\"],\n                \"date\": date_str,\n                \"location\": r[\"location\"],\n                \"start_time\": r[\"start_time\"].strftime(\"%I:%M %p\") if pd.notna(r[\"start_time\"]) else \"\",\n                \"end_time\": r[\"end_time\"].strftime(\"%I:%M %p\") if pd.notna(r[\"end_time\"]) else \"\",\n                \"duration_min\": round(r[\"duration_min\"], 1),\n                \"messages\": int(r[\"msg_count\"]),\n            })\n\n    transitions = []\n    if not trans_df.empty:\n        for _, r in trans_df.iterrows():\n            d = r[\"date\"]\n            date_str = str(d.date()) if hasattr(d, \"date\") and callable(d.date) else str(d)\n            transitions.append({\n                \"crew\": r[\"sender\"],\n                \"date\": date_str,\n                \"from\": r[\"from_location\"],\n                \"to\": r[\"to_location\"],\n                \"transition_min\": round(r[\"transition_min\"], 1),\n            })\n\n    report = {\n        \"generated_at\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"summary\": {\n            \"total_messages\": total_msgs,\n            \"clean_messages\": clean_msgs,\n            \"noise_messages\": total_msgs - clean_msgs,\n            \"active_crews\": active_crews,\n            \"total_sites_visited\": total_sites,\n            \"avg_sites_per_hour\": avg_sites_hr,\n            \"avg_first_report\": avg_first_str,\n            \"avg_transition_min\": avg_trans,\n        },\n        \"crew_metrics\": crew_metrics,\n        \"daily_breakdown\": daily_breakdown,\n        \"productivity_scores\": productivity_scores,\n        \"site_visits\": site_details,\n        \"transitions\": transitions,\n    }\n\n    job_logs = build_job_logs(df, SNOW_CONFIG)\n    if not job_logs.empty and job_logs[\"is_recall\"].any():\n        recalls = job_logs[job_logs[\"is_recall\"]]\n        crew_recall_counts = recalls.groupby(\"crew\").agg(\n            recall_count=(\"is_recall\", \"sum\"),\n            total_added_min=(\"recall_added_time_mins\", \"sum\"),\n        ).reset_index()\n        crew_recall_counts[\"total_added_min\"] = crew_recall_counts[\"total_added_min\"].round(1)\n        report[\"recalls\"] = crew_recall_counts.to_dict(\"records\")\n    else:\n        report[\"recalls\"] = []\n\n    loc_stats = build_location_type_stats(job_logs)\n    if not loc_stats.empty:\n        report[\"location_type_stats\"] = loc_stats.to_dict(\"records\")\n    else:\n        report[\"location_type_stats\"] = []\n\n    if fmt == \"csv\":\n        import io, csv\n        output = io.StringIO()\n        writer = csv.writer(output)\n        writer.writerow([\"Section: Summary\"])\n        for k, v in report[\"summary\"].items():\n            writer.writerow([k, v])\n        writer.writerow([])\n\n        if crew_metrics:\n            writer.writerow([\"Section: Crew Metrics\"])\n            writer.writerow(crew_metrics[0].keys())\n            for row in crew_metrics:\n                writer.writerow(row.values())\n            writer.writerow([])\n\n        if daily_breakdown:\n            writer.writerow([\"Section: Daily Breakdown\"])\n            writer.writerow(daily_breakdown[0].keys())\n            for row in daily_breakdown:\n                writer.writerow(row.values())\n            writer.writerow([])\n\n        if productivity_scores:\n            writer.writerow([\"Section: Productivity Scores\"])\n            writer.writerow(productivity_scores[0].keys())\n            for row in productivity_scores:\n                writer.writerow(row.values())\n            writer.writerow([])\n\n        if site_details:\n            writer.writerow([\"Section: Site Visits\"])\n            writer.writerow(site_details[0].keys())\n            for row in site_details:\n                writer.writerow(row.values())\n            writer.writerow([])\n\n        if transitions:\n            writer.writerow([\"Section: Transitions\"])\n            writer.writerow(transitions[0].keys())\n            for row in transitions:\n                writer.writerow(row.values())\n            writer.writerow([])\n\n        if report.get(\"recalls\"):\n            writer.writerow([\"Section: Recalls\"])\n            writer.writerow(report[\"recalls\"][0].keys())\n            for row in report[\"recalls\"]:\n                writer.writerow(row.values())\n            writer.writerow([])\n\n        if report.get(\"location_type_stats\"):\n            writer.writerow([\"Section: Location Type Stats\"])\n            writer.writerow(report[\"location_type_stats\"][0].keys())\n            for row in report[\"location_type_stats\"]:\n                writer.writerow(row.values())\n\n        resp = server.make_response(output.getvalue())\n        resp.headers[\"Content-Type\"] = \"text/csv\"\n        resp.headers[\"Content-Disposition\"] = \"attachment; filename=metrics_report.csv\"\n        return resp\n\n    resp = server.make_response(json.dumps(report, indent=2, ensure_ascii=False))\n    resp.headers[\"Content-Type\"] = \"application/json\"\n    resp.headers[\"Content-Disposition\"] = \"attachment; filename=metrics_report.json\"\n    return resp\n\n\n@server.route(\"/api/upload-folder\", methods=[\"POST\"])\ndef api_upload_folder():\n    archive_dir = os.path.join(DATA_DIR, \"archive\")\n    os.makedirs(archive_dir, exist_ok=True)\n    saved = []\n    skipped = []\n    errors = []\n    files = flask_request.files.getlist(\"files\")\n    best_by_chat = {}\n    for f in files:\n        try:\n            raw = f.read()\n            data = json.loads(raw.decode(\"utf-8\"))\n\n            if isinstance(data, dict) and \"summary\" in data and \"crew_metrics\" in data:\n                save_path = os.path.join(archive_dir, \"metrics_report.json\")\n                with open(save_path, \"w\", encoding=\"utf-8\") as out:\n                    json.dump(data, out, ensure_ascii=False)\n                saved.append(\"metrics_report.json\")\n                continue\n\n            if \"exportInfo\" not in data or \"messages\" not in data:\n                errors.append(f.filename)\n                continue\n            chat_name = data.get(\"exportInfo\", {}).get(\"chatName\", \"\").strip().lower()\n            msg_count = len(data.get(\"messages\", []))\n            safe_name = os.path.basename(f.filename)\n            safe_name = re.sub(r\"[^\\w.\\-]\", \"_\", safe_name)\n            if not safe_name.endswith(\".json\"):\n                safe_name += \".json\"\n            if chat_name and chat_name in best_by_chat:\n                prev_count, prev_name, _ = best_by_chat[chat_name]\n                if msg_count > prev_count:\n                    skipped.append(prev_name)\n                    best_by_chat[chat_name] = (msg_count, safe_name, data)\n                else:\n                    skipped.append(safe_name)\n                    continue\n            else:\n                key = chat_name if chat_name else safe_name\n                best_by_chat[key] = (msg_count, safe_name, data)\n        except Exception as e:\n            errors.append(str(f.filename))\n    for _, (_, safe_name, data) in best_by_chat.items():\n        try:\n            save_path = os.path.join(archive_dir, safe_name)\n            existing_count = 0\n            if os.path.exists(save_path):\n                try:\n                    with open(save_path, \"r\", encoding=\"utf-8\") as ef:\n                        existing = json.load(ef)\n                    existing_count = len(existing.get(\"messages\", []))\n                except Exception:\n                    pass\n                if len(data.get(\"messages\", [])) <= existing_count:\n                    skipped.append(safe_name)\n                    continue\n            with open(save_path, \"w\", encoding=\"utf-8\") as out:\n                json.dump(data, out, ensure_ascii=False)\n            saved.append(safe_name)\n        except Exception as e:\n            errors.append(safe_name)\n    if saved:\n        _reload_global_data()\n    return flask_jsonify({\"saved\": saved, \"skipped\": len(skipped), \"errors\": errors})\n\n\n# â”€â”€ Main content (tabs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nmain_content = dbc.Tabs(\n    id=\"main-tabs\",\n    active_tab=\"tab-overview\",\n    children=[\n        dbc.Tab(label=\"Overview\", tab_id=\"tab-overview\", children=[\n            html.Div([\n                html.H4(\"Overview\", className=\"mb-1\"),\n                html.P(\"High-level view of message volume, types, and activity patterns across all crews.\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-msgs-per-chat\"), md=6),\n                dbc.Col(dcc.Graph(id=\"chart-type-donut\"), md=6),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-heatmap\"), md=6),\n                dbc.Col(dcc.Graph(id=\"chart-sender-chat\"), md=6),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-timeline\"), md=12),\n            ]),\n        ]),\n        dbc.Tab(label=\"Productivity\", tab_id=\"tab-productivity\", children=[\n            html.Div([\n                html.H4(\"Productivity\", className=\"mb-1\"),\n                html.P(\"Track crew efficiency with daily scores, punctuality, pace, and idle time analysis.\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"efficiency-report-card\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-first-report\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-report-window-box\"), md=6),\n                dbc.Col(dcc.Graph(id=\"chart-daily-count-trend\"), md=6),\n            ]),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"productivity-score-container\"), md=12),\n            ], className=\"mt-3\"),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"crew-leaderboard-container\"), md=12),\n            ], className=\"mt-3\"),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-idle-gaps\"), md=12),\n            ]),\n        ]),\n        dbc.Tab(label=\"Crew Analysis\", tab_id=\"tab-crew\", children=[\n            html.Div([\n                html.H4(\"Crew Analysis\", className=\"mb-1\"),\n                html.P(\"Deep dive into individual crew performance, site visits, routes, and transition times.\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-msgs-per-sender\"), md=6),\n                dbc.Col(dcc.Graph(id=\"chart-gap-box\"), md=6),\n            ]),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"crew-scorecard-container\"), md=12),\n            ], className=\"mt-3\"),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-sites-per-hour\"), md=6),\n                dbc.Col(dcc.Graph(id=\"chart-transition-time\"), md=6),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-route-timeline\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-pace-consistency\"), md=6),\n                dbc.Col(dcc.Graph(id=\"chart-top-locations\"), md=6),\n            ]),\n        ]),\n        dbc.Tab(label=\"Deployments\", tab_id=\"tab-deployments\", children=[\n            html.Div([\n                html.H4(\"Deployments\", className=\"mb-1\"),\n                html.P(\"Compare crew performance across deployment periods with timeline and trend analysis.\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col(\n                    dbc.Button(\"Download Report\", id=\"btn-download-deployment-pdf\",\n                               color=\"primary\", size=\"sm\"),\n                    width=\"auto\",\n                ),\n                dcc.Download(id=\"download-deployment-pdf\"),\n            ], className=\"mb-2 ms-1\"),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"deployment-summary-container\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-deployment-timeline\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-deployment-first-report\"), md=6),\n                dbc.Col(dcc.Graph(id=\"chart-deployment-transition-box\"), md=6),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-deployment-crew-trend\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-deployment-crew-comparison\"), md=6),\n                dbc.Col(dcc.Graph(id=\"chart-deployment-sites-heatmap\"), md=6),\n            ]),\n            html.Hr(className=\"my-3\"),\n            html.H5(\"Deployment Breakdown\", className=\"mb-2\"),\n            html.P(\"Select a deployment to see which locations were serviced and by which crew. Reassign crews using the dropdowns.\",\n                   className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n            dbc.Row([\n                dbc.Col([\n                    dbc.Label(\"Deployment\", className=\"mb-1\"),\n                    dcc.Dropdown(\n                        id=\"dep-breakdown-select\",\n                        options=[{\"label\": d, \"value\": d} for d in ALL_DEPLOYMENTS],\n                        value=ALL_DEPLOYMENTS[-1] if ALL_DEPLOYMENTS else None,\n                        clearable=False,\n                    ),\n                ], md=4),\n                dbc.Col([\n                    dbc.Button(\"Save Crew Assignments\", id=\"dep-save-crews-btn\", \n                               color=\"success\", size=\"sm\", className=\"mt-4\", n_clicks=0),\n                    html.Span(id=\"dep-save-crews-status\", className=\"ms-2 align-middle\"),\n                ], md=4),\n            ], className=\"mb-3\"),\n            html.Div(id=\"dep-breakdown-table-container\"),\n        ]),\n        dbc.Tab(label=\"Operations\", tab_id=\"tab-operations\", children=[\n            html.Div([\n                html.H4(\"Operations\", className=\"mb-1\"),\n                html.P(\"Snow removal operations analysis: routing, burndown, location performance, and delay tracking.\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-routing-gantt\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-burndown\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"location-type-stats-container\"), md=6),\n                dbc.Col(html.Div(id=\"traffic-analysis-container\"), md=6),\n            ], className=\"mt-3\"),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"delay-report-container\"), md=12),\n            ], className=\"mt-3\"),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"recall-summary-container\"), md=12),\n            ], className=\"mt-3\"),\n        ]),\n        dbc.Tab(label=\"Map\", tab_id=\"tab-map\", children=[\n            html.Div([\n                html.H4(\"DC Service Map\", className=\"mb-1\"),\n                html.P(\"Geographic view of service locations. Filter by deployment to see which sites were active.\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col([\n                    dbc.Label(\"Filter by Deployment\", className=\"mb-1\"),\n                    dcc.Dropdown(\n                        id=\"map-deployment-filter\",\n                        options=[{\"label\": \"All Deployments\", \"value\": \"ALL\"}] + \n                                [{\"label\": d, \"value\": d} for d in ALL_DEPLOYMENTS],\n                        value=\"ALL\",\n                        clearable=False,\n                    ),\n                ], md=4),\n            ], className=\"mb-2\"),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-service-map\", style={\"height\": \"700px\"}), md=12),\n            ]),\n            dcc.Store(id=\"map-data-store\"),\n        ]),\n        dbc.Tab(label=\"Finances\", tab_id=\"tab-finances\", children=[\n            html.Div([\n                html.H4(\"Financial Overview\", className=\"mb-1\"),\n                html.P(\"Revenue forecasting and cost analysis with crew-type-aware costing (labor, machines, salt, overhead).\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col(dbc.Card([dbc.CardBody([\n                    html.P(\"Total Revenue\", className=\"text-muted mb-1\", style={\"fontSize\": \"11px\"}),\n                    html.H5(id=\"fin-kpi-revenue\", className=\"mb-0\", style={\"color\": \"#28a745\"}),\n                ])], className=\"shadow-sm\"), md=3, lg=True),\n                dbc.Col(dbc.Card([dbc.CardBody([\n                    html.P(\"Total Costs\", className=\"text-muted mb-1\", style={\"fontSize\": \"11px\"}),\n                    html.H5(id=\"fin-kpi-costs\", className=\"mb-0\", style={\"color\": \"#dc3545\"}),\n                ])], className=\"shadow-sm\"), md=3, lg=True),\n                dbc.Col(dbc.Card([dbc.CardBody([\n                    html.P(\"Profit\", className=\"text-muted mb-1\", style={\"fontSize\": \"11px\"}),\n                    html.H5(id=\"fin-kpi-profit\", className=\"mb-0\"),\n                ])], className=\"shadow-sm\"), md=3, lg=True),\n                dbc.Col(dbc.Card([dbc.CardBody([\n                    html.P(\"Margin %\", className=\"text-muted mb-1\", style={\"fontSize\": \"11px\"}),\n                    html.H5(id=\"fin-kpi-margin\", className=\"mb-0\"),\n                ])], className=\"shadow-sm\"), md=3, lg=True),\n                dbc.Col(dbc.Card([dbc.CardBody([\n                    html.P(\"Sites Matched\", className=\"text-muted mb-1\", style={\"fontSize\": \"11px\"}),\n                    html.H5(id=\"fin-kpi-sites\", className=\"mb-0\"),\n                ])], className=\"shadow-sm\"), md=3, lg=True),\n                dbc.Col(dbc.Card([dbc.CardBody([\n                    html.P(\"Salt Used (lbs)\", className=\"text-muted mb-1\", style={\"fontSize\": \"11px\"}),\n                    html.H5(id=\"fin-kpi-salt\", className=\"mb-0\", style={\"color\": \"#6f42c1\"}),\n                ])], className=\"shadow-sm\"), md=3, lg=True),\n                dbc.Col(dbc.Card([dbc.CardBody([\n                    html.P(\"Cost / Hour\", className=\"text-muted mb-1\", style={\"fontSize\": \"11px\"}),\n                    html.H5(id=\"fin-kpi-cost-hr\", className=\"mb-0\", style={\"color\": \"#e83e8c\"}),\n                ])], className=\"shadow-sm\"), md=3, lg=True),\n                dbc.Col(dbc.Card([dbc.CardBody([\n                    html.P(\"Rev / Deployment\", className=\"text-muted mb-1\", style={\"fontSize\": \"11px\"}),\n                    html.H5(id=\"fin-kpi-rev-dep\", className=\"mb-0\", style={\"color\": \"#17a2b8\"}),\n                ])], className=\"shadow-sm\"), md=3, lg=True),\n            ], className=\"mb-3 g-2\"),\n            html.H5(\"Cost Configuration\", className=\"mt-3 mb-2\"),\n            html.P(\"Separate rates for sidewalk and parking lot crews. Parking lot workers capped at 2 (truck-based).\",\n                   className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n            dbc.Row([\n                dbc.Col([\n                    dbc.Label(\"Sidewalk Labor ($/hr)\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-labor-sw\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"labor_rate_sidewalk\", 25.0),\n                              min=0, step=0.5, size=\"sm\"),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Label(\"Parking Labor ($/hr)\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-labor-pl\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"labor_rate_parking\", 30.0),\n                              min=0, step=0.5, size=\"sm\"),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Label(\"Machine ($/hr)\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-machine-rate\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"machine_hourly_rate\", 75.0),\n                              min=0, step=1, size=\"sm\"),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Label(\"Salt ($/lb)\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-salt-cost\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"salt_cost_per_lb\", 0.15),\n                              min=0, step=0.01, size=\"sm\"),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Label(\"Salt lbs/site (SW)\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-salt-sw\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"salt_lbs_per_site_sidewalk\", 50.0),\n                              min=0, step=5, size=\"sm\"),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Label(\"Salt lbs/site (PL)\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-salt-pl\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"salt_lbs_per_site_parking\", 200.0),\n                              min=0, step=10, size=\"sm\"),\n                ], md=2, lg=True),\n            ], className=\"mb-2 g-2\"),\n            dbc.Row([\n                dbc.Col([\n                    dbc.Label(\"Workers (Sidewalk)\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-workers-sw\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"workers_sidewalk\", 3),\n                              min=1, max=10, step=1, size=\"sm\"),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Label(\"Workers (Parking)\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-workers-pl\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"workers_parking\", 2),\n                              min=1, max=2, step=1, size=\"sm\"),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Label(\"Overhead %\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dbc.Input(id=\"fin-overhead\", type=\"number\",\n                              value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"overhead_pct\", 10.0),\n                              min=0, max=100, step=0.5, size=\"sm\"),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Label(\"Default Snow Tier\", className=\"mb-1\", style={\"fontSize\": \"12px\"}),\n                    dcc.Dropdown(\n                        id=\"fin-default-tier\",\n                        options=[\n                            {\"label\": \"Melt Only\", \"value\": \"price_melt_only\"},\n                            {\"label\": \"Under 6\\\"\", \"value\": \"price_under_6in\"},\n                            {\"label\": \"6\\\"-12\\\"\", \"value\": \"price_6_to_12in\"},\n                            {\"label\": \"12\\\"-24\\\"\", \"value\": \"price_12_to_24in\"},\n                        ],\n                        value=SNOW_CONFIG.get(\"finance_config\", {}).get(\"default_snow_tier\", \"price_under_6in\"),\n                        clearable=False,\n                    ),\n                ], md=2, lg=True),\n                dbc.Col([\n                    dbc.Button(\"Recalculate\", id=\"fin-recalc-btn\", color=\"primary\",\n                               size=\"sm\", className=\"mt-4 me-1\", n_clicks=0),\n                    dbc.Button(\"Save\", id=\"fin-save-btn\", color=\"success\",\n                               size=\"sm\", className=\"mt-4\", n_clicks=0),\n                    html.Span(id=\"fin-save-status\", className=\"ms-2 align-middle\"),\n                ], md=2, lg=True),\n            ], className=\"mb-3 g-2\"),\n            html.Hr(className=\"my-3\"),\n            html.H5(\"Deployment Financials\", className=\"mb-2\"),\n            html.Div(id=\"fin-deployment-table-container\"),\n            html.Hr(className=\"my-3\"),\n            html.H5(\"Crew Financials\", className=\"mb-2\"),\n            html.Div(id=\"fin-crew-table-container\"),\n            html.Hr(className=\"my-3\"),\n            html.H5(\"Revenue vs Costs by Deployment\", className=\"mb-2\"),\n            dcc.Graph(id=\"fin-chart-revenue\"),\n            html.Hr(className=\"my-3\"),\n            html.H5(\"Per-Deployment Labor Costs\", className=\"mb-2\"),\n            html.P(\"Enter actual labor costs per crew per deployment. Leave blank to use default rates above.\",\n                   className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n            html.Div(id=\"fin-labor-overrides-container\"),\n            dbc.Button(\"Save Labor Overrides\", id=\"fin-save-labor-overrides\", color=\"success\",\n                       size=\"sm\", className=\"mt-2 mb-3\", n_clicks=0),\n            html.Span(id=\"fin-labor-overrides-status\", className=\"ms-2\"),\n            html.Hr(className=\"my-3\"),\n            html.H5(\"Invoice Reconciliation\", className=\"mb-2\"),\n            html.P(\"Compare invoiced sites against chat data to identify discrepancies.\",\n                   className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n            dbc.Row([\n                dbc.Col([\n                    dcc.Dropdown(id=\"fin-recon-deployment\", placeholder=\"Select deployment to reconcile...\",\n                                 style={\"fontSize\": \"13px\"}),\n                ], md=4),\n                dbc.Col([\n                    dbc.Button(\"Reconcile\", id=\"fin-recon-btn\", color=\"primary\",\n                               size=\"sm\", className=\"mt-0\", n_clicks=0),\n                ], md=2),\n            ], className=\"mb-2\"),\n            html.Div(id=\"fin-recon-container\"),\n            html.Hr(className=\"my-3\"),\n            html.H5(\"Completion Verification\", className=\"mb-2\"),\n            html.P(\"Cross-reference portal completion reports with chat data to validate crew claims.\",\n                   className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n            dbc.Row([\n                dbc.Col([\n                    dcc.Dropdown(id=\"fin-verify-deployment\", placeholder=\"Select deployment to verify...\",\n                                 style={\"fontSize\": \"13px\"}),\n                ], md=4),\n                dbc.Col([\n                    dbc.Button(\"Verify\", id=\"fin-verify-btn\", color=\"info\",\n                               size=\"sm\", className=\"mt-0\", n_clicks=0),\n                ], md=2),\n            ], className=\"mb-2\"),\n            html.Div(id=\"fin-verify-container\"),\n        ]),\n        dbc.Tab(label=\"Data Quality\", tab_id=\"tab-quality\", children=[\n            html.Div([\n                html.H4(\"Data Quality\", className=\"mb-1\"),\n                html.P(\"Inspect raw data, noise filtering results, and message-level details.\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col(dcc.Graph(id=\"chart-quality\"), md=12),\n            ]),\n            dbc.Row([\n                dbc.Col(html.Div(id=\"data-table-container\"), md=12),\n            ], className=\"mt-3\"),\n        ]),\n        dbc.Tab(label=\"Settings\", tab_id=\"tab-settings\", children=[\n            html.Div([\n                html.H4(\"Settings\", className=\"mb-1\"),\n                html.P(\"Configure crew location types and sender tracking. Changes are saved automatically and applied to all analytics.\",\n                       className=\"text-muted mb-3\", style={\"fontSize\": \"14px\"}),\n            ], className=\"mt-3 mb-2 px-1\"),\n            dbc.Row([\n                dbc.Col([\n                    html.H5(\"Crew Location Types\", className=\"mb-2\"),\n                    html.P(\"Assign each crew/chat group to Sidewalk or Parking Lot. 'Auto-detect' infers from the chat name.\",\n                           className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n                    html.Div(id=\"settings-crew-types-container\"),\n                ], md=7),\n                dbc.Col([\n                    html.H5(\"Non-Trackable Senders\", className=\"mb-2\"),\n                    html.P(\"Check senders whose messages should NOT be counted in operations metrics (e.g. supervisors, admins).\",\n                           className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n                    html.Div(id=\"settings-non-trackable-container\"),\n                ], md=5),\n            ]),\n            dbc.Row([\n                dbc.Col([\n                    dbc.Button(\"Save Settings\", id=\"settings-save-btn\", color=\"primary\",\n                               className=\"mt-3 me-2\", n_clicks=0),\n                    html.Span(id=\"settings-save-status\", className=\"ms-2 align-middle\"),\n                ], md=12),\n            ], className=\"mt-2\"),\n            html.Hr(className=\"my-3\"),\n            dbc.Row([\n                dbc.Col([\n                    html.H5(\"Expected Deployment Duration\", className=\"mb-2\"),\n                    html.P(\"Hours expected for a full deployment (used for burn-down baseline).\",\n                           className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n                    dbc.InputGroup([\n                        dbc.Input(id=\"settings-expected-hours\", type=\"number\",\n                                  value=SNOW_CONFIG.get(\"expected_deployment_hours\", 12.0),\n                                  min=1, max=48, step=0.5),\n                        dbc.InputGroupText(\"hours\"),\n                    ], style={\"maxWidth\": \"250px\"}),\n                ], md=4),\n                dbc.Col([\n                    html.H5(\"Expected Service Times (minutes)\", className=\"mb-2\"),\n                    html.P(\"Average minutes to complete service at each location type.\",\n                           className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n                    dbc.Row([\n                        dbc.Col([\n                            dbc.Label(\"Sidewalk\", className=\"mb-1\"),\n                            dbc.Input(id=\"settings-svc-sidewalk\", type=\"number\",\n                                      value=SNOW_CONFIG.get(\"expected_service_times\", {}).get(\"Sidewalk\", 30),\n                                      min=1, max=240, step=1),\n                        ], md=4),\n                        dbc.Col([\n                            dbc.Label(\"Parking Lot\", className=\"mb-1\"),\n                            dbc.Input(id=\"settings-svc-parking\", type=\"number\",\n                                      value=SNOW_CONFIG.get(\"expected_service_times\", {}).get(\"Parking Lot\", 45),\n                                      min=1, max=240, step=1),\n                        ], md=4),\n                    ]),\n                ], md=6),\n            ]),\n            html.Hr(className=\"my-3\"),\n            dbc.Row([\n                dbc.Col([\n                    html.H5(\"Crew Merge\", className=\"mb-2\"),\n                    html.P(\"Merge two crews that are the same team but appear under different chat names across deployments. \"\n                           \"Select the primary crew (name to keep) and the secondary crew (to merge into primary). \"\n                           \"All messages from the secondary crew will be reassigned to the primary.\",\n                           className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n                    dbc.Row([\n                        dbc.Col([\n                            dbc.Label(\"Primary Crew (keep this name)\", className=\"mb-1 fw-bold\"),\n                            dcc.Dropdown(id=\"merge-primary-crew\", placeholder=\"Select primary crew...\",\n                                         style={\"fontSize\": \"13px\"}),\n                        ], md=5),\n                        dbc.Col([\n                            dbc.Label(\"Secondary Crew (merge into primary)\", className=\"mb-1 fw-bold\"),\n                            dcc.Dropdown(id=\"merge-secondary-crew\", placeholder=\"Select crew to merge...\",\n                                         style={\"fontSize\": \"13px\"}),\n                        ], md=5),\n                        dbc.Col([\n                            dbc.Button(\"Merge\", id=\"merge-crews-btn\", color=\"warning\",\n                                       className=\"mt-4\", n_clicks=0),\n                        ], md=2),\n                    ], className=\"mb-2\"),\n                    html.Div(id=\"merge-status\", className=\"mb-2\"),\n                    html.Div(id=\"merge-list-container\"),\n                ], md=12),\n            ]),\n            html.Hr(className=\"my-3\"),\n            dbc.Row([\n                dbc.Col([\n                    html.H5(\"Invoice Import\", className=\"mb-2\"),\n                    html.P(\"Upload deployment invoice files (.xlsx or .csv). Each invoice is matched to a deployment by date. \"\n                           \"Supports snow removal, ice removal, and pre-treatment formats.\",\n                           className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n                    dcc.Upload(\n                        id=\"upload-invoice\",\n                        children=html.Div([\n                            \"Drag & drop invoice .xlsx or .csv files here, or \",\n                            html.A(\"click to browse\", className=\"text-primary fw-bold\"),\n                        ], style={\"lineHeight\": \"40px\"}),\n                        style={\n                            \"borderWidth\": \"2px\", \"borderStyle\": \"dashed\",\n                            \"borderRadius\": \"8px\", \"borderColor\": \"#adb5bd\",\n                            \"textAlign\": \"center\", \"padding\": \"8px\",\n                            \"backgroundColor\": \"#fafbfc\", \"cursor\": \"pointer\",\n                        },\n                        multiple=True,\n                    ),\n                    html.Div(id=\"invoice-upload-status\", className=\"mt-2\"),\n                    html.Div(id=\"invoice-preview-container\", className=\"mt-2\"),\n                    html.Div(id=\"invoice-list-container\", className=\"mt-3\"),\n                ], md=12),\n            ]),\n            html.Hr(className=\"my-3\"),\n            dbc.Row([\n                dbc.Col([\n                    html.H5(\"Location Registry\", className=\"mb-2\"),\n                    html.P(\"Upload a CSV of locations (Building Name, Billing Street, Crew Sidewalk, Crew Parking Lot). \"\n                           \"Locations are fuzzy-matched to chat data automatically.\",\n                           className=\"text-muted mb-2\", style={\"fontSize\": \"13px\"}),\n                    dcc.Upload(\n                        id=\"upload-locations-csv\",\n                        children=html.Div([\n                            \"Drag & drop a CSV file here, or \",\n                            html.A(\"click to browse\", className=\"text-primary fw-bold\"),\n                        ], style={\"lineHeight\": \"40px\"}),\n                        style={\n                            \"borderWidth\": \"2px\",\n                            \"borderStyle\": \"dashed\",\n                            \"borderRadius\": \"8px\",\n                            \"borderColor\": \"#adb5bd\",\n                            \"textAlign\": \"center\",\n                            \"padding\": \"10px 15px\",\n                            \"cursor\": \"pointer\",\n                            \"backgroundColor\": \"#f8f9fa\",\n                            \"height\": \"60px\",\n                        },\n                        multiple=False,\n                        accept=\".csv\",\n                    ),\n                    html.Div(id=\"locations-upload-status\", className=\"mt-2\"),\n                    html.Div(\n                        dash_table.DataTable(\n                            id=\"locations-registry-table\",\n                            columns=[\n                                {\"name\": \"Building Name\", \"id\": \"location_name\", \"editable\": False},\n                                {\"name\": \"Address\", \"id\": \"address\", \"editable\": False},\n                                {\"name\": \"Matched Chat Location\", \"id\": \"matched_chat_location\", \"editable\": False},\n                                {\"name\": \"Crew Sidewalk\", \"id\": \"crew_sidewalk\", \"editable\": True},\n                                {\"name\": \"Crew Parking Lot\", \"id\": \"crew_parking_lot\", \"editable\": True},\n                            ],\n                            data=[\n                                {k: r.get(k, \"\") for k in [\"location_name\", \"address\", \"matched_chat_location\", \"crew_sidewalk\", \"crew_parking_lot\"]}\n                                for r in SNOW_CONFIG.get(\"location_registry\", [])\n                            ],\n                            style_table={\"overflowX\": \"auto\"},\n                            style_cell={\"textAlign\": \"left\", \"padding\": \"5px\", \"fontSize\": \"13px\"},\n                            style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f1f3f5\"},\n                            page_size=15,\n                            editable=True,\n                        ),\n                        className=\"mt-3\",\n                    ),\n                    dbc.Button(\"Auto-Assign Crews\", id=\"locations-auto-assign-btn\", color=\"info\",\n                               className=\"mt-2 me-2\", n_clicks=0),\n                    dbc.Button(\"Save Locations\", id=\"locations-save-btn\", color=\"success\",\n                               className=\"mt-2 me-2\", n_clicks=0),\n                    html.Span(id=\"locations-auto-assign-status\", className=\"ms-2 align-middle\"),\n                    html.Span(id=\"locations-save-status\", className=\"ms-2 align-middle\"),\n                ], md=12),\n            ]),\n        ]),\n    ],\n)\n\n\n# â”€â”€ Upload component â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nupload_section = html.Div([\n    dbc.Row([\n        dbc.Col([\n            dcc.Upload(\n                id=\"upload-data\",\n                children=html.Div([\n                    \"Drag & drop JSON files here, or \",\n                    html.A(\"click to browse files\", className=\"text-primary fw-bold\"),\n                ], style={\"lineHeight\": \"40px\"}),\n                style={\n                    \"borderWidth\": \"2px\",\n                    \"borderStyle\": \"dashed\",\n                    \"borderRadius\": \"8px\",\n                    \"borderColor\": \"#adb5bd\",\n                    \"textAlign\": \"center\",\n                    \"padding\": \"10px 15px\",\n                    \"cursor\": \"pointer\",\n                    \"backgroundColor\": \"#f8f9fa\",\n                    \"height\": \"60px\",\n                },\n                multiple=True,\n                accept=\".json\",\n            ),\n        ], width=5),\n        dbc.Col([\n            html.Div(\n                [\n                    html.Span(\"or \"),\n                    html.A(\"Upload a Folder\", id=\"folder-upload-btn\",\n                           className=\"text-primary fw-bold\",\n                           style={\"cursor\": \"pointer\", \"textDecoration\": \"underline\"}),\n                    html.Span(\" containing JSON files\"),\n                ],\n                style={\n                    \"borderWidth\": \"2px\",\n                    \"borderStyle\": \"dashed\",\n                    \"borderRadius\": \"8px\",\n                    \"borderColor\": \"#adb5bd\",\n                    \"textAlign\": \"center\",\n                    \"padding\": \"10px 15px\",\n                    \"backgroundColor\": \"#f8f9fa\",\n                    \"height\": \"60px\",\n                    \"lineHeight\": \"40px\",\n                },\n            ),\n        ], width=5),\n        dbc.Col([\n            html.Div(id=\"upload-status\", className=\"text-center\",\n                     style={\"lineHeight\": \"60px\"}),\n        ], width=2),\n    ], className=\"g-2 mb-2\"),\n])\n\n_FOLDER_UPLOAD_JS = \"\"\"\n<script>\n(function() {\n    function setupFolderUpload() {\n        var btn = document.getElementById('folder-upload-btn');\n        if (!btn) { setTimeout(setupFolderUpload, 500); return; }\n        if (btn.dataset.bound) return;\n        btn.dataset.bound = '1';\n\n        var inp = document.createElement('input');\n        inp.type = 'file';\n        inp.setAttribute('webkitdirectory', '');\n        inp.setAttribute('directory', '');\n        inp.setAttribute('multiple', '');\n        inp.style.display = 'none';\n        document.body.appendChild(inp);\n\n        var totalUploaded = 0;\n        var folderCount = 0;\n        var uploading = false;\n\n        var doneBtn = document.createElement('a');\n        doneBtn.textContent = 'Done â€” Reload Dashboard';\n        doneBtn.style.cssText = 'display:none;cursor:pointer;text-decoration:underline;color:#198754;font-weight:bold;margin-left:10px;';\n        btn.parentNode.appendChild(document.createElement('br'));\n        btn.parentNode.appendChild(doneBtn);\n\n        doneBtn.addEventListener('click', function() { window.location.reload(); });\n\n        btn.addEventListener('click', function() {\n            if (uploading) return;\n            inp.click();\n        });\n\n        inp.addEventListener('change', function() {\n            if (!inp.files || inp.files.length === 0) return;\n            var formData = new FormData();\n            var count = 0;\n            for (var i = 0; i < inp.files.length; i++) {\n                if (inp.files[i].name.endsWith('.json')) {\n                    formData.append('files', inp.files[i]);\n                    count++;\n                }\n            }\n            if (count === 0) {\n                alert('No JSON files found in the selected folder.');\n                inp.value = '';\n                return;\n            }\n            uploading = true;\n            btn.textContent = 'Uploading ' + count + ' file(s)...';\n            btn.style.color = '#0d6efd';\n\n            fetch('/api/upload-folder', { method: 'POST', body: formData })\n            .then(function(r) { return r.json(); })\n            .then(function(data) {\n                uploading = false;\n                if (data.saved && data.saved.length > 0) {\n                    totalUploaded += data.saved.length;\n                    folderCount++;\n                    var msg = totalUploaded + ' file(s) from ' + folderCount + ' folder(s) uploaded';\n                    if (data.skipped && data.skipped > 0) msg += ' (' + data.skipped + ' duplicates skipped)';\n                    msg += '<br>Click to add another folder';\n                    btn.innerHTML = msg;\n                    btn.style.color = '#198754';\n                    doneBtn.style.display = 'inline';\n                } else if (data.skipped && data.skipped > 0) {\n                    btn.innerHTML = data.skipped + ' duplicate(s) skipped â€” no new files<br>Click to add another folder';\n                    btn.style.color = '#6c757d';\n                    if (totalUploaded > 0) doneBtn.style.display = 'inline';\n                } else {\n                    btn.textContent = 'No valid exports found â€” click to try again';\n                    btn.style.color = '#dc3545';\n                }\n            })\n            .catch(function(e) {\n                uploading = false;\n                btn.textContent = 'Upload failed â€” click to retry';\n                btn.style.color = '#dc3545';\n            });\n            inp.value = '';\n        });\n    }\n    if (document.readyState === 'loading') {\n        document.addEventListener('DOMContentLoaded', setupFolderUpload);\n    } else {\n        setupFolderUpload();\n    }\n})();\n</script>\n\"\"\"\n\napp.index_string = app.index_string.replace(\"</body>\", _FOLDER_UPLOAD_JS + \"</body>\")\n\n\n# â”€â”€ App layout (function so it refreshes after data upload) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _build_sidebar():\n    return dbc.Card(\n        [\n            dbc.CardHeader(html.H5(\"Filters\", className=\"mb-0\")),\n            dbc.CardBody(\n                [\n                    dbc.Accordion(\n                        [\n                            dbc.AccordionItem(\n                                [\n                                    html.Label(\"Chat Groups\", className=\"fw-bold mb-1\"),\n                                    dcc.Dropdown(\n                                        id=\"filter-chats\",\n                                        options=[{\"label\": c, \"value\": c} for c in ALL_CHATS],\n                                        value=ALL_CHATS,\n                                        multi=True,\n                                        placeholder=\"Select chats...\",\n                                        className=\"mb-3\",\n                                    ),\n                                    html.Label(\"Senders\", className=\"fw-bold mb-1\"),\n                                    dcc.Dropdown(\n                                        id=\"filter-senders\",\n                                        options=[{\"label\": s, \"value\": s}\n                                                 for s in ALL_SENDERS_RESOLVED],\n                                        value=ALL_SENDERS_RESOLVED,\n                                        multi=True,\n                                        placeholder=\"Select senders...\",\n                                        className=\"mb-3\",\n                                    ),\n                                    dbc.Switch(\n                                        id=\"filter-resolved\",\n                                        label=\"Use resolved senders\",\n                                        value=True,\n                                        className=\"mb-1\",\n                                    ),\n                                ],\n                                title=\"People & Chats\",\n                            ),\n                            dbc.AccordionItem(\n                                [\n                                    html.Label(\"Time Range (hour)\", className=\"fw-bold mb-1\"),\n                                    dcc.RangeSlider(\n                                        id=\"filter-time\",\n                                        min=0, max=24, step=1,\n                                        value=[0, 24],\n                                        marks={h: f\"{h % 12 or 12}{'a' if h < 12 else 'p'}\"\n                                               for h in range(0, 25, 3)},\n                                        className=\"mb-3\",\n                                    ),\n                                    html.Label(\"Date Range\", className=\"fw-bold mb-1\"),\n                                    dcc.DatePickerRange(\n                                        id=\"filter-dates\",\n                                        min_date_allowed=DATE_MIN,\n                                        max_date_allowed=DATE_MAX,\n                                        start_date=DATE_MIN,\n                                        end_date=DATE_MAX,\n                                        display_format=\"YYYY-MM-DD\",\n                                        className=\"mb-3\",\n                                        style={\"fontSize\": \"12px\"},\n                                    ),\n                                    html.Label(\"Deployment\", className=\"fw-bold mb-1\"),\n                                    dcc.Dropdown(\n                                        id=\"filter-deployment\",\n                                        options=[{\"label\": d, \"value\": d} for d in ALL_DEPLOYMENTS],\n                                        value=ALL_DEPLOYMENTS,\n                                        multi=True,\n                                        placeholder=\"Select deployments...\",\n                                        className=\"mb-1\",\n                                    ),\n                                ],\n                                title=\"Time & Dates\",\n                            ),\n                            dbc.AccordionItem(\n                                [\n                                    html.Label(\"Message Type\", className=\"fw-bold mb-1\"),\n                                    dbc.Checklist(\n                                        id=\"filter-types\",\n                                        options=[{\"label\": t.title(), \"value\": t}\n                                                 for t in ALL_TYPES],\n                                        value=ALL_TYPES,\n                                        inline=True,\n                                        className=\"mb-3\",\n                                    ),\n                                    html.Label(\"Data Quality\", className=\"fw-bold mb-1\"),\n                                    dbc.RadioItems(\n                                        id=\"filter-quality\",\n                                        options=[\n                                            {\"label\": \"Clean only\", \"value\": \"clean\"},\n                                            {\"label\": \"All messages\", \"value\": \"all\"},\n                                            {\"label\": \"Noise only\", \"value\": \"noise\"},\n                                        ],\n                                        value=\"clean\",\n                                        className=\"mb-1\",\n                                    ),\n                                ],\n                                title=\"Message Filters\",\n                            ),\n                        ],\n                        always_open=True,\n                        active_item=[\"item-0\"],\n                        className=\"mb-3\",\n                    ),\n                    html.Div(id=\"summary-stats\"),\n                ],\n                style={\"overflowY\": \"auto\", \"maxHeight\": \"85vh\"},\n            ),\n        ],\n        className=\"shadow-sm\",\n        style={\"height\": \"100vh\"},\n    )\n\n\ndef serve_layout():\n    return dbc.Container(\n        [\n            upload_section,\n            dbc.Row([\n                dbc.Col(html.Div(id=\"kpi-bar\"), md=10),\n                dbc.Col(html.Div([\n                    html.A(\n                        [html.I(className=\"bi bi-download me-1\"), \"Export JSON\"],\n                        href=\"/api/export-report?format=json\",\n                        className=\"btn btn-outline-primary btn-sm d-block mb-1\",\n                        download=\"metrics_report.json\",\n                    ),\n                    html.A(\n                        [html.I(className=\"bi bi-file-earmark-spreadsheet me-1\"), \"Export CSV\"],\n                        href=\"/api/export-report?format=csv\",\n                        className=\"btn btn-outline-success btn-sm d-block\",\n                        download=\"metrics_report.csv\",\n                    ),\n                ], className=\"d-flex flex-column align-items-end pt-1\"), md=2),\n            ], className=\"mb-2 g-0\"),\n            dbc.Row(\n                [\n                    dbc.Col(_build_sidebar(), md=3, className=\"pe-0\"),\n                    dbc.Col(main_content, md=9, className=\"ps-3\"),\n                ],\n                className=\"g-0\",\n            ),\n            dcc.Location(id=\"page-reload\", refresh=True),\n        ],\n        fluid=True,\n        className=\"p-2\",\n    )\n\n\napp.layout = serve_layout\n\n\n# â”€â”€ Upload callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _reload_global_data():\n    \"\"\"Reload all global data variables after new files are uploaded.\"\"\"\n    global DF_ALL, ALL_CHATS, ALL_SENDERS, ALL_SENDERS_RESOLVED, ALL_SENDERS_RESOLVED_FULL, ALL_TYPES\n    global NOISE_TYPES, DATE_MIN, DATE_MAX, ALL_DATES, ALL_DEPLOYMENTS_LIST\n    global ALL_DEPLOYMENTS, _date_to_deployment\n    global TOTAL_MSGS, CLEAN_MSGS, NOISE_MSGS, NUM_CHATS, NUM_SENDERS\n    global SNOW_CONFIG\n    global PRICING_DATA, PRICING_INDEX\n    global METRICS_REPORT\n\n    DF_ALL = load_all_data(DATA_DIR)\n    METRICS_REPORT = load_metrics_report(DATA_DIR)\n    ALL_CHATS = sorted(DF_ALL[\"chat\"].unique()) if not DF_ALL.empty else []\n    _nt = set(SNOW_CONFIG.get(\"non_trackable_senders\", []))\n    ALL_SENDERS = sorted(s for s in DF_ALL[DF_ALL[\"sender\"] != \"\"][\"sender\"].unique()\n                         if s not in _nt) if not DF_ALL.empty else []\n    ALL_SENDERS_RESOLVED = sorted(s for s in DF_ALL[DF_ALL[\"sender_resolved\"] != \"\"][\"sender_resolved\"].unique()\n                                  if s not in _nt) if not DF_ALL.empty else []\n    ALL_SENDERS_RESOLVED_FULL = sorted(\n        DF_ALL[DF_ALL[\"sender_resolved\"] != \"\"][\"sender_resolved\"].unique()) if not DF_ALL.empty else []\n    ALL_TYPES = sorted(DF_ALL[\"type\"].unique()) if not DF_ALL.empty else []\n    NOISE_TYPES = sorted(DF_ALL[\"noise_type\"].unique()) if not DF_ALL.empty else []\n\n    _date_col = DF_ALL[\"msg_date\"].dropna()\n    if _date_col.empty:\n        _date_col = DF_ALL[\"export_date\"].dropna()\n    DATE_MIN = _date_col.min().date() if not _date_col.empty else date(2026, 2, 8)\n    DATE_MAX = _date_col.max().date() if not _date_col.empty else date(2026, 2, 8)\n    ALL_DATES = sorted(DF_ALL[\"msg_date\"].dropna().dt.date.unique()) if not DF_ALL.empty else []\n\n    ALL_DEPLOYMENTS_LIST = _compute_deployments(ALL_DATES)\n    _date_to_deployment = {}\n    for _dep in ALL_DEPLOYMENTS_LIST:\n        _d = _dep[\"start_date\"]\n        while _d <= _dep[\"end_date\"]:\n            _date_to_deployment[_d] = _dep[\"label\"]\n            _d += timedelta(days=1)\n\n    if not DF_ALL.empty:\n        DF_ALL[\"deployment\"] = DF_ALL[\"msg_date\"].dt.date.map(_date_to_deployment)\n    ALL_DEPLOYMENTS = [dep[\"label\"] for dep in ALL_DEPLOYMENTS_LIST]\n\n    TOTAL_MSGS = len(DF_ALL)\n    CLEAN_MSGS = len(DF_ALL[DF_ALL[\"noise_type\"] == \"clean\"]) if not DF_ALL.empty else 0\n    NOISE_MSGS = TOTAL_MSGS - CLEAN_MSGS\n    NUM_CHATS = DF_ALL[\"chat\"].nunique() if not DF_ALL.empty else 0\n    NUM_SENDERS = len(ALL_SENDERS_RESOLVED)\n    SNOW_CONFIG = load_config(os.path.join(DATA_DIR, \"config\", \"snow_removal.json\"))\n\n    PRICING_DATA = load_pricing(os.path.join(DATA_DIR, \"config\", \"pricing.json\"))\n    PRICING_INDEX = {}\n    for p in PRICING_DATA:\n        addr_norm = _normalize_location(p.get(\"address\", \"\"))\n        if addr_norm:\n            PRICING_INDEX[addr_norm] = p\n        name_norm = _normalize_location(p.get(\"building_name\", \"\"))\n        if name_norm:\n            PRICING_INDEX[name_norm] = p\n\n\n@app.callback(\n    Output(\"upload-status\", \"children\"),\n    Output(\"page-reload\", \"href\"),\n    Input(\"upload-data\", \"contents\"),\n    State(\"upload-data\", \"filename\"),\n    prevent_initial_call=True,\n)\ndef handle_upload(contents_list, filenames_list):\n    if not contents_list:\n        return no_update, no_update\n\n    archive_dir = os.path.join(DATA_DIR, \"archive\")\n    os.makedirs(archive_dir, exist_ok=True)\n\n    saved = []\n    errors = []\n    for content, filename in zip(contents_list, filenames_list):\n        try:\n            content_type, content_string = content.split(\",\", 1)\n            decoded = base64.b64decode(content_string)\n            data = json.loads(decoded.decode(\"utf-8\"))\n\n            if isinstance(data, dict) and \"summary\" in data and \"crew_metrics\" in data:\n                save_path = os.path.join(archive_dir, \"metrics_report.json\")\n                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(data, f, ensure_ascii=False)\n                saved.append(\"metrics_report.json\")\n                continue\n\n            if not isinstance(data, dict) or \"exportInfo\" not in data or \"messages\" not in data:\n                errors.append(f\"{filename}: not a valid chat export or metrics file\")\n                continue\n\n            safe_name = os.path.basename(filename)\n            safe_name = re.sub(r\"[^\\w.\\-]\", \"_\", safe_name)\n            if not safe_name.endswith(\".json\"):\n                safe_name += \".json\"\n            save_path = os.path.join(archive_dir, safe_name)\n            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, ensure_ascii=False)\n            saved.append(safe_name)\n        except Exception as e:\n            errors.append(f\"{filename}: {str(e)[:50]}\")\n\n    _reload_global_data()\n\n    msgs = []\n    if saved:\n        msgs.append(html.Span(\n            f\"Uploaded {len(saved)} file(s). \",\n            className=\"text-success fw-bold\"\n        ))\n    if errors:\n        msgs.append(html.Span(\n            f\"{len(errors)} error(s). \",\n            className=\"text-danger\"\n        ))\n\n    if saved:\n        return html.Div(msgs), \"/\"\n    return html.Div(msgs), None\n\n\n# â”€â”€ Shared filter inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nFILTER_INPUTS = [\n    Input(\"filter-chats\", \"value\"),\n    Input(\"filter-senders\", \"value\"),\n    Input(\"filter-quality\", \"value\"),\n    Input(\"filter-types\", \"value\"),\n    Input(\"filter-time\", \"value\"),\n    Input(\"filter-resolved\", \"value\"),\n    Input(\"filter-dates\", \"start_date\"),\n    Input(\"filter-dates\", \"end_date\"),\n    Input(\"filter-deployment\", \"value\"),\n]\n\n\ndef _apply_quality_filter(quality):\n    \"\"\"Convert quality radio selection to list of noise_types.\"\"\"\n    if quality == \"clean\":\n        return [\"clean\"]\n    elif quality == \"noise\":\n        return [n for n in NOISE_TYPES if n != \"clean\"]\n    else:  # \"all\"\n        return NOISE_TYPES\n\n\ndef _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n            date_start=None, date_end=None, deployments=None):\n    \"\"\"Convenience wrapper that translates quality radio to noise_types.\"\"\"\n    noise_types = _apply_quality_filter(quality)\n    return get_filtered_df(chats, senders, noise_types, msg_types,\n                           time_range, use_resolved, date_start, date_end, deployments)\n\n\ndef _sender_col(use_resolved):\n    return \"sender_resolved\" if use_resolved else \"sender\"\n\n\n# â”€â”€ Callback: Summary stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"summary-stats\", \"children\"),\n    FILTER_INPUTS,\n)\ndef update_summary(chats, senders, quality, msg_types, time_range,\n                   use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    n_clean = len(df[df[\"noise_type\"] == \"clean\"])\n    n_noise = len(df[df[\"noise_type\"] != \"clean\"])\n    n_senders = df[df[scol] != \"\"][scol].nunique()\n    n_chats = df[\"chat\"].nunique()\n\n    # Compute average interval per person\n    df_timed = df[(df[scol] != \"\") & df[\"time\"].notna()]\n    gap_parts = []\n    for sender, grp in df_timed.groupby(scol):\n        times = grp[\"time\"].sort_values()\n        if len(times) >= 2:\n            diffs = times.diff().dropna().dt.total_seconds() / 60.0\n            gap_parts.append(diffs.mean())\n    avg_gap = f\"{np.mean(gap_parts):.1f} min\" if gap_parts else \"N/A\"\n\n    # Compute efficiency metrics\n    ds = _build_daily_summary(df, scol)\n    if not ds.empty:\n        avg_first = ds[\"first_hour\"].mean()\n        avg_first_h = int(avg_first)\n        avg_first_m = int((avg_first - avg_first_h) * 60)\n        period = \"AM\" if avg_first_h < 12 else \"PM\"\n        disp_h = avg_first_h % 12 or 12\n        avg_first_str = f\"{disp_h}:{avg_first_m:02d} {period}\"\n        avg_window = f\"{ds['window_hrs'].mean():.1f} hrs\"\n    else:\n        avg_first_str = \"N/A\"\n        avg_window = \"N/A\"\n\n    return dbc.Card(\n        dbc.CardBody([\n            html.H6(\"Summary\", className=\"card-title\"),\n            html.P(f\"Total: {len(df)}\", className=\"mb-1\"),\n            html.P(f\"Clean: {n_clean}\", className=\"mb-1 text-success\"),\n            html.P(f\"Noise: {n_noise}\", className=\"mb-1 text-danger\"),\n            html.P(f\"Senders: {n_senders}\", className=\"mb-1\"),\n            html.P(f\"Chats: {n_chats}\", className=\"mb-1\"),\n            html.P(f\"Avg interval: {avg_gap}\", className=\"mb-1 text-info\"),\n            html.Hr(),\n            html.H6(\"Efficiency\", className=\"card-title\"),\n            html.P(f\"Avg first report: {avg_first_str}\", className=\"mb-1\"),\n            html.P(f\"Avg report window: {avg_window}\", className=\"mb-0\"),\n        ]),\n        color=\"light\",\n    )\n\n\n# â”€â”€ Chart 1: Messages per Chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-msgs-per-chat\", \"figure\"), FILTER_INPUTS)\ndef chart_msgs_per_chat(chats, senders, quality, msg_types, time_range,\n                        use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[df[scol] != \"\"]\n    if df_valid.empty:\n        return _empty_fig(\"Messages per Chat\")\n\n    counts = (df_valid.groupby([\"chat\", scol])\n              .size().reset_index(name=\"count\"))\n    # Totals per chat for labels\n    totals = counts.groupby(\"chat\")[\"count\"].sum().reset_index()\n    totals.columns = [\"chat\", \"total\"]\n    fig = px.bar(\n        counts, y=\"chat\", x=\"count\", color=scol,\n        orientation=\"h\", text=\"count\",\n        title=\"Messages per Chat\",\n        labels={\"count\": \"Messages\", \"chat\": \"Chat Group\", scol: \"Sender\"},\n    )\n    fig.update_traces(textposition=\"inside\", textfont_size=10)\n    # Add total annotations on the right\n    for _, row in totals.iterrows():\n        fig.add_annotation(\n            x=row[\"total\"], y=row[\"chat\"],\n            text=f\"  {int(row['total'])}\",\n            showarrow=False, font=dict(size=11, color=\"#2c3e50\"),\n            xanchor=\"left\",\n        )\n    fig.update_layout(\n        barmode=\"stack\",\n        yaxis={\"categoryorder\": \"total ascending\"},\n        margin=dict(l=10, r=40, t=40, b=10),\n        legend=dict(font=dict(size=10)),\n        height=400,\n    )\n    return fig\n\n\n# â”€â”€ Chart 2: Type Distribution (donut) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-type-donut\", \"figure\"), FILTER_INPUTS)\ndef chart_type_donut(chats, senders, quality, msg_types, time_range,\n                     use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    if df.empty:\n        return _empty_fig(\"Message Type Distribution\")\n\n    counts = df[\"type\"].value_counts().reset_index()\n    counts.columns = [\"type\", \"count\"]\n    fig = px.pie(\n        counts, names=\"type\", values=\"count\", hole=0.4,\n        title=\"Message Type Distribution\",\n    )\n    fig.update_traces(textinfo=\"label+value+percent\", textfont_size=12)\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n    )\n    return fig\n\n\n# â”€â”€ Chart 3: Hourly Heatmap (sender Ã— hour) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-heatmap\", \"figure\"), FILTER_INPUTS)\ndef chart_heatmap(chats, senders, quality, msg_types, time_range,\n                  use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[(df[scol] != \"\") & df[\"hour_int\"].notna()]\n    if df_valid.empty:\n        return _empty_fig(\"Hourly Activity Heatmap\")\n\n    df_valid = df_valid.copy()\n    df_valid[\"hour_int\"] = df_valid[\"hour_int\"].astype(int)\n    ct = pd.crosstab(df_valid[scol], df_valid[\"hour_int\"])\n    # Ensure all 24 hours\n    for h in range(24):\n        if h not in ct.columns:\n            ct[h] = 0\n    ct = ct[sorted(ct.columns)]\n    hour_labels = [f\"{h % 12 or 12}{'a' if h < 12 else 'p'}\" for h in range(24)]\n\n    fig = go.Figure(data=go.Heatmap(\n        z=ct.values,\n        x=hour_labels,\n        y=ct.index.tolist(),\n        colorscale=\"YlOrRd\",\n        text=ct.values,\n        texttemplate=\"%{text}\",\n        hovertemplate=\"Sender: %{y}<br>Hour: %{x}<br>Messages: %{z}<extra></extra>\",\n    ))\n    fig.update_layout(\n        title=\"Hourly Activity Heatmap\",\n        xaxis_title=\"Hour of Day\",\n        yaxis_title=\"Sender\",\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n    )\n    return fig\n\n\n# â”€â”€ Chart 4: Sender Ã— Chat Matrix (bubble) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-sender-chat\", \"figure\"), FILTER_INPUTS)\ndef chart_sender_chat(chats, senders, quality, msg_types, time_range,\n                      use_resolved, date_start, date_end, deployments):\n    try:\n        df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                     date_start, date_end, deployments)\n        scol = _sender_col(use_resolved)\n        if scol not in df.columns:\n            return _empty_fig(\"Sender Ã— Chat Participation\")\n        df_valid = df[df[scol] != \"\"]\n        if df_valid.empty:\n            return _empty_fig(\"Sender Ã— Chat Participation\")\n\n        counts = (df_valid.groupby([scol, \"chat\"])\n                  .size().reset_index(name=\"count\"))\n        top_combos = counts.nlargest(50, \"count\")\n        fig = px.bar(\n            top_combos, x=\"chat\", y=\"count\", color=scol,\n            text=\"count\", barmode=\"group\",\n            title=\"Sender Ã— Chat Participation\",\n            labels={\"count\": \"Messages\", \"chat\": \"Chat\", scol: \"Sender\"},\n        )\n        fig.update_traces(textposition=\"outside\", textfont_size=10)\n        fig.update_layout(\n            margin=dict(l=10, r=10, t=40, b=10),\n            height=400,\n            xaxis_tickangle=-30,\n            legend=dict(font=dict(size=10)),\n        )\n        return fig\n    except Exception as e:\n        print(f\"[chart-sender-chat] Error: {e}\")\n        return _empty_fig(\"Sender Ã— Chat Participation\")\n\n\n# â”€â”€ Chart 5: Activity Timeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-timeline\", \"figure\"), FILTER_INPUTS)\ndef chart_timeline(chats, senders, quality, msg_types, time_range,\n                   use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[(df[scol] != \"\") & df[\"time\"].notna()].copy()\n    if df_valid.empty:\n        return _empty_fig(\"Activity Timeline\")\n\n    fig = px.scatter(\n        df_valid, x=\"time\", y=scol, color=\"chat\",\n        hover_data=[\"content\", \"type\", \"noise_type\"],\n        title=\"Activity Timeline\",\n        labels={\"time\": \"Time\", scol: \"Sender\", \"chat\": \"Chat\"},\n    )\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=450,\n        xaxis_tickformat=\"%I:%M %p\",\n        legend=dict(font=dict(size=10)),\n    )\n    fig.update_traces(marker=dict(size=10, opacity=0.7))\n    return fig\n\n\n# â”€â”€ Chart 6: KDE Density â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-kde\", \"figure\"), FILTER_INPUTS)\ndef chart_kde(chats, senders, quality, msg_types, time_range, use_resolved,\n              date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[(df[scol] != \"\") & df[\"hour\"].notna()]\n    if df_valid.empty:\n        return _empty_fig(\"Message Density by Sender\")\n\n    fig = go.Figure()\n    sender_list = sorted(df_valid[scol].unique())\n    colors = px.colors.qualitative.Plotly\n    for i, sender in enumerate(sender_list):\n        hours = df_valid[df_valid[scol] == sender][\"hour\"].values\n        if len(hours) < 2:\n            continue\n        # Compute KDE using numpy histogram for smoothing\n        hist, bin_edges = np.histogram(hours, bins=48, range=(0, 24),\n                                       density=True)\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n        # Simple moving average smoothing\n        kernel_size = 3\n        smoothed = np.convolve(hist, np.ones(kernel_size) / kernel_size,\n                               mode=\"same\")\n        color = colors[i % len(colors)]\n        fig.add_trace(go.Scatter(\n            x=bin_centers, y=smoothed,\n            mode=\"lines\", name=sender,\n            fill=\"tozeroy\", opacity=0.4,\n            line=dict(color=color, width=2),\n        ))\n\n    fig.update_layout(\n        title=\"Message Density by Sender (KDE-style)\",\n        xaxis_title=\"Hour of Day\",\n        yaxis_title=\"Density\",\n        xaxis=dict(range=[0, 24], dtick=2,\n                   ticktext=[f\"{h % 12 or 12} {'AM' if h < 12 else 'PM'}\"\n                             for h in range(0, 25, 2)],\n                   tickvals=list(range(0, 25, 2))),\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\n# â”€â”€ Chart 6b: Hourly Message Count (line) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-line-hourly\", \"figure\"), FILTER_INPUTS)\ndef chart_line_hourly(chats, senders, quality, msg_types, time_range,\n                      use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[(df[scol] != \"\") & df[\"hour_int\"].notna()].copy()\n    if df_valid.empty:\n        return _empty_fig(\"Messages per Hour (Line)\")\n\n    df_valid[\"hour_int\"] = df_valid[\"hour_int\"].astype(int)\n    counts = (df_valid.groupby([scol, \"hour_int\"])\n              .size().reset_index(name=\"count\"))\n    # Fill missing hours with 0 then compute cumulative sum per sender\n    sender_list = counts[scol].unique()\n    full_index = pd.MultiIndex.from_product(\n        [sender_list, range(24)], names=[scol, \"hour_int\"])\n    counts = (counts.set_index([scol, \"hour_int\"])\n              .reindex(full_index, fill_value=0)\n              .reset_index())\n    counts = counts.sort_values([scol, \"hour_int\"])\n    counts[\"cumulative\"] = counts.groupby(scol)[\"count\"].cumsum()\n\n    hour_labels = [f\"{h % 12 or 12} {'AM' if h < 12 else 'PM'}\" for h in range(24)]\n\n    fig = px.line(\n        counts, x=\"hour_int\", y=\"cumulative\", color=scol,\n        markers=True,\n        title=\"Cumulative Messages by Hour per Sender\",\n        labels={\"hour_int\": \"Hour of Day\", \"cumulative\": \"Cumulative Messages\", scol: \"Sender\"},\n    )\n    fig.update_layout(\n        xaxis=dict(\n            tickmode=\"array\",\n            tickvals=list(range(0, 24, 2)),\n            ticktext=[hour_labels[h] for h in range(0, 24, 2)],\n        ),\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\n# â”€â”€ Chart 9b: Content Length vs Hour (scatter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-scatter-len-hour\", \"figure\"), FILTER_INPUTS)\ndef chart_scatter_len_hour(chats, senders, quality, msg_types, time_range,\n                           use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[(df[scol] != \"\") & df[\"hour\"].notna() &\n                  (df[\"content_len\"] > 0)].copy()\n    if df_valid.empty:\n        return _empty_fig(\"Content Length vs Hour (Scatter)\")\n\n    fig = px.scatter(\n        df_valid, x=\"hour\", y=\"content_len\", color=scol,\n        opacity=0.6,\n        hover_data=[\"chat\", \"type\", \"content\"],\n        title=\"Content Length vs Hour of Day\",\n        labels={\"hour\": \"Hour of Day\", \"content_len\": \"Message Length (chars)\",\n                scol: \"Sender\"},\n    )\n    fig.update_layout(\n        xaxis=dict(range=[0, 24], dtick=2,\n                   ticktext=[f\"{h % 12 or 12} {'AM' if h < 12 else 'PM'}\"\n                             for h in range(0, 25, 2)],\n                   tickvals=list(range(0, 25, 2))),\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=450,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\n# â”€â”€ Chart 7: Messages per Sender â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-msgs-per-sender\", \"figure\"), FILTER_INPUTS)\ndef chart_msgs_per_sender(chats, senders, quality, msg_types, time_range,\n                          use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[df[scol] != \"\"]\n    if df_valid.empty:\n        return _empty_fig(\"Messages per Sender\")\n\n    # Count + average interval per sender\n    counts = df_valid[scol].value_counts().reset_index()\n    counts.columns = [\"sender\", \"count\"]\n    df_timed = df_valid[df_valid[\"time\"].notna()]\n    avg_gaps = {}\n    for sender, grp in df_timed.groupby(scol):\n        times = grp[\"time\"].sort_values()\n        if len(times) >= 2:\n            avg_gaps[sender] = times.diff().dropna().dt.total_seconds().mean() / 60.0\n    counts[\"avg_gap\"] = counts[\"sender\"].map(avg_gaps)\n    counts[\"label\"] = counts.apply(\n        lambda r: f\"{r['count']}  (avg {r['avg_gap']:.0f}m)\"\n        if pd.notna(r[\"avg_gap\"]) else str(r[\"count\"]),\n        axis=1)\n\n    fig = px.bar(\n        counts, y=\"sender\", x=\"count\", orientation=\"h\",\n        text=\"label\",\n        title=\"Messages per Sender (count + avg interval)\",\n        labels={\"count\": \"Messages\", \"sender\": \"Sender\"},\n        color=\"count\",\n        color_continuous_scale=\"Viridis\",\n    )\n    fig.update_traces(textposition=\"outside\", textfont_size=10)\n    fig.update_layout(\n        yaxis={\"categoryorder\": \"total ascending\"},\n        margin=dict(l=10, r=80, t=40, b=10),\n        height=400,\n    )\n    return fig\n\n\n# â”€â”€ Chart 8: Message Gap Analysis (box plot) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-gap-box\", \"figure\"), FILTER_INPUTS)\ndef chart_gap_box(chats, senders, quality, msg_types, time_range,\n                  use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[(df[scol] != \"\") & df[\"time\"].notna()].copy()\n    if df_valid.empty:\n        return _empty_fig(\"Message Gap Analysis\")\n\n    # Compute gaps per sender\n    gap_rows = []\n    for sender, grp in df_valid.groupby(scol):\n        times = grp[\"time\"].sort_values()\n        if len(times) < 2:\n            continue\n        diffs = times.diff().dropna().dt.total_seconds() / 60.0  # minutes\n        for gap in diffs:\n            gap_rows.append({\"sender\": sender, \"gap_min\": gap})\n\n    if not gap_rows:\n        return _empty_fig(\"Message Gap Analysis\")\n\n    df_gaps = pd.DataFrame(gap_rows)\n    fig = px.box(\n        df_gaps, x=\"sender\", y=\"gap_min\", points=\"all\",\n        title=\"Message Gap Analysis (minutes between messages)\",\n        labels={\"gap_min\": \"Gap (minutes)\", \"sender\": \"Sender\"},\n    )\n    # Add mean annotation per sender\n    means = df_gaps.groupby(\"sender\")[\"gap_min\"].mean()\n    for sender, avg in means.items():\n        fig.add_annotation(\n            x=sender, y=avg,\n            text=f\"avg {avg:.0f}m\",\n            showarrow=True, arrowhead=2, arrowcolor=\"#e74c3c\",\n            font=dict(size=10, color=\"#e74c3c\"),\n            ax=30, ay=-20,\n        )\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        xaxis_tickangle=-30,\n    )\n    return fig\n\n\n# â”€â”€ Chart 9: Content Length Distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-content-len\", \"figure\"), FILTER_INPUTS)\ndef chart_content_len(chats, senders, quality, msg_types, time_range,\n                      use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    df_valid = df[df[scol] != \"\"].copy()\n    if df_valid.empty:\n        return _empty_fig(\"Content Length Distribution\")\n\n    fig = px.histogram(\n        df_valid, x=\"content_len\", color=scol,\n        marginal=\"box\",\n        title=\"Content Length Distribution\",\n        labels={\"content_len\": \"Content Length (chars)\", scol: \"Sender\"},\n        nbins=30,\n    )\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\n# â”€â”€ Chart 10: Data Quality Overview (always shows all data) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"chart-quality\", \"figure\"),\n    [Input(\"filter-chats\", \"value\"),\n     Input(\"filter-dates\", \"start_date\"),\n     Input(\"filter-dates\", \"end_date\")],\n)\ndef chart_quality(chats, date_start, date_end):\n    df = DF_ALL.copy()\n    if date_start:\n        df = df[df[\"msg_date\"] >= pd.Timestamp(date_start)]\n    if date_end:\n        df = df[df[\"msg_date\"] <= pd.Timestamp(date_end)]\n    if chats:\n        df = df[df[\"chat\"].isin(chats)]\n    if df.empty:\n        return _empty_fig(\"Data Quality Overview\")\n\n    counts = (df.groupby([\"chat\", \"noise_type\"])\n              .size().reset_index(name=\"count\"))\n\n    color_map = {\n        \"clean\": \"#2ecc71\",\n        \"css_html\": \"#e74c3c\",\n        \"load_error\": \"#e67e22\",\n        \"system_metadata\": \"#9b59b6\",\n        \"empty_sender_caption\": \"#3498db\",\n    }\n\n    fig = px.bar(\n        counts, x=\"chat\", y=\"count\", color=\"noise_type\",\n        text=\"count\",\n        title=\"Data Quality Overview (noise types per chat)\",\n        labels={\"count\": \"Messages\", \"chat\": \"Chat\", \"noise_type\": \"Noise Type\"},\n        color_discrete_map=color_map,\n        barmode=\"stack\",\n    )\n    fig.update_traces(textposition=\"inside\", textfont_size=10)\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        xaxis_tickangle=-30,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\n# â”€â”€ Data Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"data-table-container\", \"children\"), FILTER_INPUTS)\ndef update_data_table(chats, senders, quality, msg_types, time_range,\n                      use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n\n    # Select columns for display\n    display_cols = [\"chat\", scol, \"timestamp\", \"type\", \"noise_type\",\n                    \"content_len\", \"content\"]\n    df_display = df[display_cols].copy()\n    df_display.columns = [\"Chat\", \"Sender\", \"Time\", \"Type\", \"Noise\",\n                          \"Length\", \"Content\"]\n    # Truncate content for display\n    df_display[\"Content\"] = df_display[\"Content\"].str[:120]\n\n    table = dash_table.DataTable(\n        data=df_display.to_dict(\"records\"),\n        columns=[{\"name\": c, \"id\": c} for c in df_display.columns],\n        filter_action=\"native\",\n        sort_action=\"native\",\n        sort_mode=\"multi\",\n        page_size=20,\n        style_table={\"overflowX\": \"auto\"},\n        style_cell={\n            \"textAlign\": \"left\",\n            \"padding\": \"8px\",\n            \"fontSize\": \"13px\",\n            \"maxWidth\": \"300px\",\n            \"overflow\": \"hidden\",\n            \"textOverflow\": \"ellipsis\",\n        },\n        style_header={\n            \"backgroundColor\": \"#2c3e50\",\n            \"color\": \"white\",\n            \"fontWeight\": \"bold\",\n        },\n        style_data_conditional=[\n            {\n                \"if\": {\n                    \"filter_query\": \"{Noise} != 'clean'\",\n                },\n                \"backgroundColor\": \"#fde8e8\",\n                \"color\": \"#c0392b\",\n            },\n            {\n                \"if\": {\n                    \"filter_query\": \"{Noise} = 'clean'\",\n                },\n                \"backgroundColor\": \"#eafaf1\",\n            },\n        ],\n    )\n    return table\n\n\n# â”€â”€ Efficiency: helper to build daily summary per sender â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _build_daily_summary(df, scol):\n    \"\"\"Return a DataFrame with one row per (sender, date) with efficiency stats.\"\"\"\n    df_valid = df[(df[scol] != \"\") & df[\"time\"].notna() & df[\"msg_date\"].notna()].copy()\n    if df_valid.empty:\n        return pd.DataFrame()\n\n    rows = []\n    for (sender, d), grp in df_valid.groupby([scol, df_valid[\"msg_date\"].dt.date]):\n        times = grp[\"time\"].sort_values()\n        first = times.iloc[0]\n        last = times.iloc[-1]\n        window_hrs = (last - first).total_seconds() / 3600.0\n        n = len(times)\n        avg_gap = (times.diff().dropna().dt.total_seconds().mean() / 60.0\n                   if n >= 2 else 0.0)\n        rows.append({\n            \"sender\": sender,\n            \"date\": d,\n            \"first_time\": first,\n            \"last_time\": last,\n            \"first_hour\": first.hour + first.minute / 60.0,\n            \"last_hour\": last.hour + last.minute / 60.0,\n            \"window_hrs\": window_hrs,\n            \"msg_count\": n,\n            \"avg_gap_min\": avg_gap,\n        })\n    return pd.DataFrame(rows)\n\n\n# â”€â”€ Crew Metrics: site visit analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _count_billable_routes_from_df(df):\n    if df is None or df.empty:\n        return 0\n    loc_df = df[df[\"location\"].astype(str).str.len() > 0].copy()\n    if loc_df.empty:\n        return 0\n    def _sa(chat):\n        _, is_sw, is_pl = _extract_crew_from_chat(chat)\n        if is_pl:\n            return \"Parking Lot\"\n        return \"Sidewalk\"\n    loc_df[\"_service_area\"] = loc_df[\"chat\"].apply(_sa)\n    dep_col = \"deployment\" if \"deployment\" in loc_df.columns else None\n    if dep_col:\n        routes = loc_df.groupby([dep_col, \"location\", \"_service_area\"]).ngroups\n    else:\n        routes = loc_df.groupby([\"location\", \"_service_area\"]).ngroups\n    return routes\n\n\ndef _build_site_visits(df, scol):\n    \"\"\"Build a DataFrame of site visits from messages with locations.\n\n    Walks messages sorted by (sender, date, time). A new location or a new\n    calendar date = new site visit.  Messages without a location inherit the\n    current site.\n\n    Returns DataFrame: sender, date, location, start_time, end_time,\n                       duration_min, msg_count, visit_order\n    \"\"\"\n    df_valid = df[\n        (df[scol] != \"\") & df[\"time\"].notna() & df[\"msg_date\"].notna()\n    ].copy()\n    if df_valid.empty:\n        return pd.DataFrame()\n\n    df_valid = df_valid.sort_values([scol, \"msg_date\", \"time\"])\n\n    def _flush(visits, sender, current_loc, visit_start, visit_end,\n               visit_msgs, visit_date, visit_order):\n        if current_loc and visit_start is not None:\n            dur = (visit_end - visit_start).total_seconds() / 60.0\n            visits.append({\n                \"sender\": sender,\n                \"date\": visit_date,\n                \"location\": current_loc,\n                \"start_time\": visit_start,\n                \"end_time\": visit_end,\n                \"duration_min\": max(dur, 1.0),\n                \"msg_count\": visit_msgs,\n                \"visit_order\": visit_order,\n            })\n\n    visits = []\n    for sender, grp in df_valid.groupby(scol):\n        current_loc = \"\"\n        visit_start = None\n        visit_end = None\n        visit_msgs = 0\n        visit_date = None\n        visit_order = 0\n        current_day = None\n\n        for _, row in grp.iterrows():\n            loc = row[\"location\"]\n            t = row[\"time\"]\n            d = row[\"msg_date\"]\n            row_day = d.date() if hasattr(d, \"date\") else d\n\n            # Reset on date change\n            if current_day is not None and row_day != current_day:\n                _flush(visits, sender, current_loc, visit_start, visit_end,\n                       visit_msgs, visit_date, visit_order)\n                current_loc = \"\"\n                visit_start = None\n                visit_end = None\n                visit_msgs = 0\n                visit_order = 0\n\n            current_day = row_day\n\n            if loc and loc != current_loc:\n                # Save previous visit if any\n                _flush(visits, sender, current_loc, visit_start, visit_end,\n                       visit_msgs, visit_date, visit_order)\n                # Start new visit\n                current_loc = loc\n                visit_start = t\n                visit_end = t\n                visit_msgs = 1\n                visit_date = d\n                visit_order += 1\n            elif current_loc:\n                # Continue current visit\n                visit_end = t\n                visit_msgs += 1\n\n        # Save last visit\n        _flush(visits, sender, current_loc, visit_start, visit_end,\n               visit_msgs, visit_date, visit_order)\n\n    return pd.DataFrame(visits)\n\n\ndef _build_transitions(visits_df):\n    \"\"\"Compute transition times between consecutive site visits per sender.\n\n    Returns DataFrame: sender, date, from_location, to_location, transition_min\n    \"\"\"\n    if visits_df.empty:\n        return pd.DataFrame()\n\n    rows = []\n    for sender, grp in visits_df.groupby(\"sender\"):\n        grp_sorted = grp.sort_values([\"date\", \"start_time\"])\n        prev = None\n        for _, row in grp_sorted.iterrows():\n            if prev is not None:\n                gap = (row[\"start_time\"] - prev[\"end_time\"]).total_seconds() / 60.0\n                # Only count transitions within same day and reasonable gap\n                if gap >= 0 and gap < 480:  # max 8 hours\n                    rows.append({\n                        \"sender\": sender,\n                        \"date\": row[\"date\"],\n                        \"from_location\": prev[\"location\"],\n                        \"to_location\": row[\"location\"],\n                        \"transition_min\": gap,\n                    })\n            prev = row\n\n    return pd.DataFrame(rows)\n\n\n_WORK_BLOCK_GAP_MIN = 120  # gaps > 2 h split into separate work blocks\n\n\ndef _compute_active_hours(day_visits):\n    \"\"\"Sum work-block durations for one sender-day.\n\n    Consecutive site visits with gaps < _WORK_BLOCK_GAP_MIN are grouped into\n    a work block.  Off-duty gaps (> threshold) start a new block.\n    \"\"\"\n    vs = day_visits.sort_values(\"start_time\")\n    total_sec = 0.0\n    block_start = vs.iloc[0][\"start_time\"]\n    block_end = vs.iloc[0][\"end_time\"]\n\n    for i in range(1, len(vs)):\n        row = vs.iloc[i]\n        gap = (row[\"start_time\"] - block_end).total_seconds() / 60.0\n        if gap < _WORK_BLOCK_GAP_MIN:\n            # Extend current work block\n            block_end = max(block_end, row[\"end_time\"])\n        else:\n            # Flush current block, start new one\n            total_sec += (block_end - block_start).total_seconds()\n            block_start = row[\"start_time\"]\n            block_end = row[\"end_time\"]\n\n    # Flush last block\n    total_sec += (block_end - block_start).total_seconds()\n    return max(total_sec / 3600.0, 1 / 60.0)  # at least 1 minute\n\n\ndef _build_crew_scorecard(visits_df, ds_df):\n    \"\"\"Aggregate crew efficiency metrics per sender.\n\n    Active hours are the sum of work-block durations per day: consecutive\n    site visits with gaps under 2 h are grouped into a work block; longer\n    off-duty gaps are excluded.\n\n    Returns DataFrame: sender, days_active, total_sites, avg_sites_per_day,\n                       avg_sites_per_hour, avg_transition_min, total_active_hrs\n    \"\"\"\n    if visits_df.empty:\n        return pd.DataFrame()\n\n    trans_df = _build_transitions(visits_df)\n\n    rows = []\n    for sender, grp in visits_df.groupby(\"sender\"):\n        total_sites = len(grp)\n        days_active = grp[\"date\"].nunique()\n        avg_sites_day = total_sites / days_active if days_active else 0\n\n        # Active hours = sum of work-block spans per day\n        total_hrs = 0.0\n        for _d, day_grp in grp.groupby(grp[\"date\"].dt.date):\n            total_hrs += _compute_active_hours(day_grp)\n\n        avg_sites_hr = total_sites / total_hrs if total_hrs > 0 else 0\n\n        # Average transition time\n        if not trans_df.empty and sender in trans_df[\"sender\"].values:\n            avg_trans = trans_df[trans_df[\"sender\"] == sender][\"transition_min\"].mean()\n        else:\n            avg_trans = 0.0\n\n        rows.append({\n            \"sender\": sender,\n            \"days_active\": days_active,\n            \"total_sites\": total_sites,\n            \"avg_sites_per_day\": round(avg_sites_day, 1),\n            \"avg_sites_per_hour\": round(avg_sites_hr, 1),\n            \"avg_transition_min\": round(avg_trans, 1),\n            \"total_active_hrs\": round(total_hrs, 1),\n        })\n\n    return pd.DataFrame(rows).sort_values(\"total_sites\", ascending=False)\n\n\n# â”€â”€ Efficiency Chart 1: Daily Report Card (table) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"efficiency-report-card\", \"children\"), FILTER_INPUTS)\ndef efficiency_report_card(chats, senders, quality, msg_types, time_range,\n                           use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    ds = _build_daily_summary(df, scol)\n    if ds.empty:\n        return html.P(\"No data for current filters\", className=\"text-muted p-3\")\n\n    ds_display = ds.copy()\n    ds_display[\"first_time\"] = ds_display[\"first_time\"].dt.strftime(\"%I:%M %p\")\n    ds_display[\"last_time\"] = ds_display[\"last_time\"].dt.strftime(\"%I:%M %p\")\n    ds_display[\"window_hrs\"] = ds_display[\"window_hrs\"].round(1)\n    ds_display[\"avg_gap_min\"] = ds_display[\"avg_gap_min\"].round(1)\n    ds_display[\"date\"] = ds_display[\"date\"].astype(str)\n    ds_display = ds_display[[\"sender\", \"date\", \"first_time\", \"last_time\",\n                              \"window_hrs\", \"msg_count\", \"avg_gap_min\"]]\n    ds_display.columns = [\"Sender\", \"Date\", \"First Msg\", \"Last Msg\",\n                           \"Window (hrs)\", \"Messages\", \"Avg Gap (min)\"]\n\n    table = dash_table.DataTable(\n        data=ds_display.to_dict(\"records\"),\n        columns=[{\"name\": c, \"id\": c} for c in ds_display.columns],\n        filter_action=\"native\",\n        sort_action=\"native\",\n        sort_mode=\"multi\",\n        page_size=20,\n        style_table={\"overflowX\": \"auto\"},\n        style_cell={\"textAlign\": \"left\", \"padding\": \"8px\", \"fontSize\": \"13px\"},\n        style_header={\n            \"backgroundColor\": \"#2c3e50\", \"color\": \"white\", \"fontWeight\": \"bold\",\n        },\n        style_data_conditional=[\n            {\"if\": {\"filter_query\": \"{Window (hrs)} < 1\"},\n             \"backgroundColor\": \"#fde8e8\", \"color\": \"#c0392b\"},\n            {\"if\": {\"filter_query\": \"{Window (hrs)} >= 4\"},\n             \"backgroundColor\": \"#eafaf1\"},\n        ],\n    )\n    return html.Div([\n        html.H5(\"Daily Report Card\", className=\"mb-2\"),\n        table,\n    ])\n\n\n# â”€â”€ Efficiency Chart 2: First Report Time scatter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-first-report\", \"figure\"), FILTER_INPUTS)\ndef chart_first_report(chats, senders, quality, msg_types, time_range,\n                       use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    ds = _build_daily_summary(df, scol)\n    if ds.empty:\n        return _empty_fig(\"First Report Time by Sender\")\n\n    ds[\"date_str\"] = ds[\"date\"].astype(str)\n    fig = px.scatter(\n        ds, x=\"date_str\", y=\"first_hour\", color=\"sender\",\n        size=\"msg_count\",\n        hover_data={\"first_hour\": False, \"date_str\": False,\n                    \"first_time\": True, \"msg_count\": True, \"window_hrs\": \":.1f\"},\n        title=\"First Report Time by Sender (per day)\",\n        labels={\"date_str\": \"Date\", \"first_hour\": \"First Message (hour)\",\n                \"sender\": \"Sender\"},\n        size_max=18,\n    )\n    fig.update_layout(\n        yaxis=dict(\n            range=[24, 0], dtick=2,\n            ticktext=[f\"{h % 12 or 12} {'AM' if h < 12 else 'PM'}\"\n                      for h in range(0, 25, 2)],\n            tickvals=list(range(0, 25, 2)),\n        ),\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=420,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\n# â”€â”€ Efficiency Chart 3: Reporting Window box plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-report-window-box\", \"figure\"), FILTER_INPUTS)\ndef chart_report_window_box(chats, senders, quality, msg_types, time_range,\n                            use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    ds = _build_daily_summary(df, scol)\n    if ds.empty:\n        return _empty_fig(\"Reporting Window by Sender\")\n\n    fig = px.box(\n        ds, x=\"sender\", y=\"window_hrs\", points=\"all\",\n        title=\"Reporting Window by Sender (hours)\",\n        labels={\"window_hrs\": \"Window (hours)\", \"sender\": \"Sender\"},\n        color=\"sender\",\n    )\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        xaxis_tickangle=-30,\n        showlegend=False,\n    )\n    return fig\n\n\n# â”€â”€ Efficiency Chart 4: Daily Message Count Trend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-daily-count-trend\", \"figure\"), FILTER_INPUTS)\ndef chart_daily_count_trend(chats, senders, quality, msg_types, time_range,\n                            use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    ds = _build_daily_summary(df, scol)\n    if ds.empty:\n        return _empty_fig(\"Daily Message Count Trend\")\n\n    ds[\"date_str\"] = ds[\"date\"].astype(str)\n    fig = px.line(\n        ds.sort_values(\"date\"), x=\"date_str\", y=\"msg_count\",\n        color=\"sender\", markers=True,\n        title=\"Daily Message Count per Sender\",\n        labels={\"date_str\": \"Date\", \"msg_count\": \"Messages\", \"sender\": \"Sender\"},\n    )\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\n# â”€â”€ Crew Metrics Callback 1: Crew Scorecard Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"crew-scorecard-container\", \"children\"), FILTER_INPUTS)\ndef crew_scorecard(chats, senders, quality, msg_types, time_range,\n                   use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    visits_df = _build_site_visits(df, \"chat\")\n    if visits_df.empty:\n        return html.P(\"No site visit data for current filters\",\n                       className=\"text-muted p-3\")\n\n    ds = _build_daily_summary(df, \"chat\")\n    sc = _build_crew_scorecard(visits_df, ds)\n    if sc.empty:\n        return html.P(\"No crew scorecard data\", className=\"text-muted p-3\")\n\n    sc_display = sc.rename(columns={\n        \"sender\": \"Crew\",\n        \"days_active\": \"Days Active\",\n        \"total_sites\": \"Total Sites\",\n        \"avg_sites_per_day\": \"Sites/Day\",\n        \"avg_sites_per_hour\": \"Sites/Hour\",\n        \"avg_transition_min\": \"Avg Transition (min)\",\n        \"total_active_hrs\": \"Active Hours\",\n    })\n\n    table = dash_table.DataTable(\n        data=sc_display.to_dict(\"records\"),\n        columns=[{\"name\": c, \"id\": c} for c in sc_display.columns],\n        sort_action=\"native\",\n        sort_mode=\"multi\",\n        style_table={\"overflowX\": \"auto\"},\n        style_cell={\"textAlign\": \"left\", \"padding\": \"8px\", \"fontSize\": \"13px\"},\n        style_header={\n            \"backgroundColor\": \"#2c3e50\", \"color\": \"white\", \"fontWeight\": \"bold\",\n        },\n        style_data_conditional=[\n            {\"if\": {\"filter_query\": \"{Sites/Hour} >= 3\"},\n             \"backgroundColor\": \"#eafaf1\"},\n            {\"if\": {\"filter_query\": \"{Sites/Hour} < 1\"},\n             \"backgroundColor\": \"#fde8e8\", \"color\": \"#c0392b\"},\n        ],\n    )\n    return html.Div([\n        html.H5(\"Crew Scorecard\", className=\"mb-2\"),\n        table,\n    ])\n\n\n# â”€â”€ Crew Metrics Callback 2: Sites per Hour Bar Chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-sites-per-hour\", \"figure\"), FILTER_INPUTS)\ndef chart_sites_per_hour(chats, senders, quality, msg_types, time_range,\n                         use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    visits_df = _build_site_visits(df, \"chat\")\n    if visits_df.empty:\n        return _empty_fig(\"Sites per Hour\")\n\n    ds = _build_daily_summary(df, \"chat\")\n    sc = _build_crew_scorecard(visits_df, ds)\n    if sc.empty:\n        return _empty_fig(\"Sites per Hour\")\n\n    sc_sorted = sc.sort_values(\"avg_sites_per_hour\")\n    fig = px.bar(\n        sc_sorted, y=\"sender\", x=\"avg_sites_per_hour\", orientation=\"h\",\n        text=\"avg_sites_per_hour\",\n        title=\"Average Sites per Hour by Crew\",\n        labels={\"avg_sites_per_hour\": \"Sites / Hour\", \"sender\": \"Crew\"},\n        color=\"avg_sites_per_hour\",\n        color_continuous_scale=\"RdYlGn\",\n    )\n    fig.update_traces(textposition=\"outside\", textfont_size=11,\n                      texttemplate=\"%{text:.1f}\")\n    fig.update_layout(\n        margin=dict(l=10, r=60, t=40, b=10),\n        height=400,\n        yaxis={\"categoryorder\": \"total ascending\"},\n    )\n    return fig\n\n\n# â”€â”€ Crew Metrics Callback 3: Transition Time Box Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-transition-time\", \"figure\"), FILTER_INPUTS)\ndef chart_transition_time(chats, senders, quality, msg_types, time_range,\n                          use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    visits_df = _build_site_visits(df, \"chat\")\n    if visits_df.empty:\n        return _empty_fig(\"Transition Time Between Sites\")\n\n    trans_df = _build_transitions(visits_df)\n    if trans_df.empty:\n        return _empty_fig(\"Transition Time Between Sites\")\n\n    fig = px.box(\n        trans_df, x=\"sender\", y=\"transition_min\", points=\"all\",\n        title=\"Transition Time Between Sites (minutes)\",\n        labels={\"transition_min\": \"Transition (min)\", \"sender\": \"Crew\"},\n        color=\"sender\",\n    )\n    # Add mean annotations\n    means = trans_df.groupby(\"sender\")[\"transition_min\"].mean()\n    for sender, avg in means.items():\n        fig.add_annotation(\n            x=sender, y=avg,\n            text=f\"avg {avg:.0f}m\",\n            showarrow=True, arrowhead=2, arrowcolor=\"#e74c3c\",\n            font=dict(size=10, color=\"#e74c3c\"),\n            ax=30, ay=-20,\n        )\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        xaxis_tickangle=-30,\n        showlegend=False,\n    )\n    return fig\n\n\n# â”€â”€ Crew Metrics Callback 4: Route Timeline (Gantt-style) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-route-timeline\", \"figure\"), FILTER_INPUTS)\ndef chart_route_timeline(chats, senders, quality, msg_types, time_range,\n                         use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    visits_df = _build_site_visits(df, \"chat\")\n    if visits_df.empty:\n        return _empty_fig(\"Route Timeline\")\n\n    # Build Gantt-style timeline using px.timeline\n    gantt_data = visits_df.copy()\n    gantt_data[\"start_str\"] = gantt_data[\"start_time\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n    # Ensure end > start for visibility (min 2-minute block)\n    gantt_data[\"end_adj\"] = gantt_data.apply(\n        lambda r: r[\"end_time\"] if (r[\"end_time\"] - r[\"start_time\"]).total_seconds() > 60\n        else r[\"start_time\"] + pd.Timedelta(minutes=2),\n        axis=1,\n    )\n    gantt_data[\"end_str\"] = gantt_data[\"end_adj\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n\n    fig = px.timeline(\n        gantt_data,\n        x_start=\"start_time\",\n        x_end=\"end_adj\",\n        y=\"sender\",\n        color=\"location\",\n        hover_data=[\"location\", \"msg_count\", \"duration_min\"],\n        title=\"Route Timeline (site visits over time)\",\n        labels={\"sender\": \"Crew\"},\n    )\n    fig.update_layout(\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=450,\n        legend=dict(font=dict(size=9), orientation=\"h\", yanchor=\"bottom\",\n                    y=-0.3, xanchor=\"center\", x=0.5),\n        xaxis_tickformat=\"%I:%M %p\",\n    )\n    return fig\n\n\n# â”€â”€ Crew Metrics Callback 5: Pace Consistency â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-pace-consistency\", \"figure\"), FILTER_INPUTS)\ndef chart_pace_consistency(chats, senders, quality, msg_types, time_range,\n                           use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    visits_df = _build_site_visits(df, \"chat\")\n    if visits_df.empty:\n        return _empty_fig(\"Pace Consistency\")\n\n    # For each sender-day, compute cumulative sites vs hours into shift\n    fig = go.Figure()\n    colors = px.colors.qualitative.Plotly\n    sender_list = sorted(visits_df[\"sender\"].unique())\n\n    for i, sender in enumerate(sender_list):\n        sv = visits_df[visits_df[\"sender\"] == sender].sort_values(\"start_time\")\n        if sv.empty:\n            continue\n\n        # Group by date, compute hours-into-shift and cumulative site count\n        for d, day_visits in sv.groupby(\"date\"):\n            day_sorted = day_visits.sort_values(\"start_time\")\n            shift_start = day_sorted[\"start_time\"].iloc[0]\n            hours_in = []\n            cum_sites = []\n            for j, (_, row) in enumerate(day_sorted.iterrows(), 1):\n                h = (row[\"start_time\"] - shift_start).total_seconds() / 3600.0\n                hours_in.append(h)\n                cum_sites.append(j)\n\n            color = colors[i % len(colors)]\n            date_str = str(d)[:10] if hasattr(d, 'strftime') else str(d)[:10]\n            fig.add_trace(go.Scatter(\n                x=hours_in, y=cum_sites,\n                mode=\"lines+markers\",\n                name=f\"{sender} ({date_str})\",\n                line=dict(color=color),\n                marker=dict(size=6),\n                legendgroup=sender,\n                showlegend=(d == day_sorted[\"date\"].iloc[0]),\n            ))\n\n    fig.update_layout(\n        title=\"Pace Consistency (cumulative sites vs hours into shift)\",\n        xaxis_title=\"Hours Into Shift\",\n        yaxis_title=\"Cumulative Sites Visited\",\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        legend=dict(font=dict(size=9)),\n    )\n    return fig\n\n\n# â”€â”€ Crew Metrics Callback 6: Most Visited Locations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-top-locations\", \"figure\"), FILTER_INPUTS)\ndef chart_top_locations(chats, senders, quality, msg_types, time_range,\n                        use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    visits_df = _build_site_visits(df, \"chat\")\n    if visits_df.empty:\n        return _empty_fig(\"Most Visited Locations\")\n\n    loc_counts = (visits_df.groupby(\"location\")\n                  .agg(visits=(\"sender\", \"size\"),\n                       crews=(\"sender\", \"nunique\"))\n                  .sort_values(\"visits\", ascending=False)\n                  .head(20)\n                  .reset_index())\n\n    fig = px.bar(\n        loc_counts, y=\"location\", x=\"visits\", orientation=\"h\",\n        text=\"visits\",\n        title=\"Top 20 Most Visited Locations\",\n        labels={\"visits\": \"Visit Count\", \"location\": \"Location\"},\n        color=\"crews\",\n        color_continuous_scale=\"Blues\",\n        hover_data=[\"crews\"],\n    )\n    fig.update_traces(textposition=\"outside\", textfont_size=10)\n    fig.update_layout(\n        yaxis={\"categoryorder\": \"total ascending\"},\n        margin=dict(l=10, r=60, t=40, b=10),\n        height=max(400, len(loc_counts) * 25),\n        coloraxis_colorbar_title=\"Unique<br>Crews\",\n    )\n    return fig\n\n\n# â”€â”€ Deployment builder functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\ndef _build_deployment_summary_data(df):\n    \"\"\"Build deployment summary DataFrame and per-deployment crew breakdowns.\n\n    Returns (dep_df, crew_breakdowns) where crew_breakdowns is a dict\n    mapping deployment label to its crew scorecard DataFrame.\n    \"\"\"\n    if df.empty or df[\"deployment\"].isna().all():\n        return pd.DataFrame(), {}\n\n    rows = []\n    crew_breakdowns = {}\n    for dep_label, grp in df.groupby(\"deployment\"):\n        dep_info = next(\n            (d for d in ALL_DEPLOYMENTS_LIST if d[\"label\"] == dep_label), None)\n        days = dep_info[\"days\"] if dep_info else 0\n        crews = grp[\"chat\"].nunique()\n        n_msgs = len(grp)\n        visits_df = _build_site_visits(grp, \"chat\")\n        total_sites = _count_billable_routes_from_df(grp)\n        ds = _build_daily_summary(grp, \"chat\")\n        sc = _build_crew_scorecard(visits_df, ds)\n        avg_sites_hr = sc[\"avg_sites_per_hour\"].mean() if not sc.empty else 0\n\n        if not ds.empty:\n            avg_first = ds[\"first_hour\"].mean()\n            fh = int(avg_first)\n            fm = int((avg_first - fh) * 60)\n            period = \"AM\" if fh < 12 else \"PM\"\n            dh = fh % 12 or 12\n            avg_first_str = f\"{dh}:{fm:02d} {period}\"\n            avg_window = round(ds[\"window_hrs\"].mean(), 1)\n            consistency = round(ds[\"window_hrs\"].std(), 1) if len(ds) > 1 else 0.0\n        else:\n            avg_first_str = \"N/A\"\n            avg_window = 0.0\n            consistency = 0.0\n        avg_trans = round(sc[\"avg_transition_min\"].mean(), 1) if not sc.empty else 0.0\n        active_per_crew = round(sc[\"total_active_hrs\"].mean(), 1) if not sc.empty else 0.0\n\n        rows.append({\n            \"Deployment\": dep_label,\n            \"Days\": days,\n            \"Crews Active\": crews,\n            \"Total Messages\": n_msgs,\n            \"Billable Routes\": total_sites,\n            \"Avg Sites/Hr\": round(avg_sites_hr, 1),\n            \"Avg First Report\": avg_first_str,\n            \"Avg Window (hrs)\": avg_window,\n            \"Avg Transition (min)\": avg_trans,\n            \"Active Hrs/Crew\": active_per_crew,\n            \"Consistency (std)\": consistency,\n        })\n        if not sc.empty:\n            crew_breakdowns[dep_label] = sc\n\n    return pd.DataFrame(rows), crew_breakdowns\n\n\ndef _build_deployment_timeline_fig(df):\n    \"\"\"Build deployment timeline Gantt chart.\"\"\"\n    if df.empty or df[\"deployment\"].isna().all():\n        return _empty_fig(\"Deployment Timeline\")\n\n    dep_stats = []\n    for dep_info in ALL_DEPLOYMENTS_LIST:\n        dep_label = dep_info[\"label\"]\n        grp = df[df[\"deployment\"] == dep_label]\n        if grp.empty:\n            continue\n        dep_stats.append({\n            \"Deployment\": dep_label,\n            \"Start\": pd.Timestamp(dep_info[\"start_date\"]),\n            \"End\": pd.Timestamp(dep_info[\"end_date\"]) + pd.Timedelta(days=1),\n            \"Messages\": len(grp),\n            \"Crews\": grp[\"chat\"].nunique(),\n        })\n\n    if not dep_stats:\n        return _empty_fig(\"Deployment Timeline\")\n\n    dep_df = pd.DataFrame(dep_stats)\n    fig = px.timeline(\n        dep_df,\n        x_start=\"Start\",\n        x_end=\"End\",\n        y=\"Deployment\",\n        color=\"Deployment\",\n        hover_data=[\"Messages\", \"Crews\"],\n        title=\"Deployment Timeline\",\n        color_discrete_sequence=px.colors.qualitative.T10,\n    )\n    for i, row in dep_df.iterrows():\n        fig.add_annotation(\n            x=row[\"Start\"] + (row[\"End\"] - row[\"Start\"]) / 2,\n            y=row[\"Deployment\"],\n            text=f\"{row['Messages']} msgs / {row['Crews']} crews\",\n            showarrow=False,\n            font=dict(size=11, color=\"white\"),\n        )\n    fig.update_layout(\n        template=\"plotly_white\",\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=300,\n        showlegend=False,\n        yaxis={\"categoryorder\": \"array\",\n               \"categoryarray\": [d[\"label\"] for d in ALL_DEPLOYMENTS_LIST][::-1]},\n    )\n    return fig\n\n\ndef _build_deployment_first_report_fig(df):\n    \"\"\"Build grouped bar chart of avg first report time by deployment.\"\"\"\n    if df.empty or df[\"deployment\"].isna().all():\n        return _empty_fig(\"First Report Time by Deployment\")\n\n    dep_order = [d[\"label\"] for d in ALL_DEPLOYMENTS_LIST]\n    rows = []\n    for dep_label, grp in df.groupby(\"deployment\"):\n        ds = _build_daily_summary(grp, \"chat\")\n        if ds.empty:\n            continue\n        for crew, crew_grp in ds.groupby(\"sender\"):\n            rows.append({\n                \"Deployment\": dep_label,\n                \"Crew\": crew,\n                \"Avg First Hour\": crew_grp[\"first_hour\"].mean(),\n            })\n\n    if not rows:\n        return _empty_fig(\"First Report Time by Deployment\")\n\n    fr_df = pd.DataFrame(rows)\n    overall_mean = fr_df[\"Avg First Hour\"].mean()\n\n    fig = px.bar(\n        fr_df, x=\"Deployment\", y=\"Avg First Hour\", color=\"Crew\",\n        barmode=\"group\",\n        title=\"Avg First Report Time by Deployment\",\n        labels={\"Avg First Hour\": \"First Report (hour)\", \"Crew\": \"Crew\"},\n        color_discrete_sequence=px.colors.qualitative.T10,\n    )\n    fig.add_hline(\n        y=overall_mean, line_dash=\"dash\", line_color=\"#2c3e50\",\n        annotation_text=f\"Overall avg {int(overall_mean)}:{int((overall_mean % 1) * 60):02d}\",\n        annotation_position=\"top left\",\n    )\n    fig.update_layout(\n        template=\"plotly_white\",\n        yaxis=dict(\n            autorange=\"reversed\",\n            dtick=1,\n            ticktext=[f\"{h % 12 or 12} {'AM' if h < 12 else 'PM'}\"\n                      for h in range(0, 25)],\n            tickvals=list(range(0, 25)),\n        ),\n        xaxis={\"categoryorder\": \"array\", \"categoryarray\": dep_order},\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\ndef _build_deployment_transition_box_fig(df):\n    \"\"\"Build box plot of transition times by deployment.\"\"\"\n    if df.empty or df[\"deployment\"].isna().all():\n        return _empty_fig(\"Transition Time by Deployment\")\n\n    dep_order = [d[\"label\"] for d in ALL_DEPLOYMENTS_LIST]\n    all_trans = []\n    for dep_label, grp in df.groupby(\"deployment\"):\n        visits_df = _build_site_visits(grp, \"chat\")\n        if visits_df.empty:\n            continue\n        trans_df = _build_transitions(visits_df)\n        if trans_df.empty:\n            continue\n        trans_df = trans_df.copy()\n        trans_df[\"Deployment\"] = dep_label\n        all_trans.append(trans_df)\n\n    if not all_trans:\n        return _empty_fig(\"Transition Time by Deployment\")\n\n    trans_all = pd.concat(all_trans, ignore_index=True)\n    overall_median = trans_all[\"transition_min\"].median()\n\n    fig = px.box(\n        trans_all, x=\"Deployment\", y=\"transition_min\", points=\"all\",\n        title=\"Transition Time by Deployment (minutes)\",\n        labels={\"transition_min\": \"Transition (min)\", \"Deployment\": \"Deployment\"},\n        color=\"Deployment\",\n        color_discrete_sequence=px.colors.qualitative.T10,\n    )\n    fig.add_hline(\n        y=overall_median, line_dash=\"dash\", line_color=\"#2c3e50\",\n        annotation_text=f\"Median {overall_median:.0f} min\",\n        annotation_position=\"top left\",\n    )\n    fig.update_layout(\n        template=\"plotly_white\",\n        xaxis={\"categoryorder\": \"array\", \"categoryarray\": dep_order},\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        showlegend=False,\n    )\n    return fig\n\n\ndef _build_deployment_crew_comparison_fig(df):\n    \"\"\"Build cross-deployment crew comparison bar chart.\"\"\"\n    if df.empty or df[\"deployment\"].isna().all():\n        return _empty_fig(\"Cross-Deployment Crew Comparison\")\n\n    rows = []\n    for dep_label, grp in df.groupby(\"deployment\"):\n        visits_df = _build_site_visits(grp, \"chat\")\n        if visits_df.empty:\n            continue\n        for crew, crew_visits in visits_df.groupby(\"sender\"):\n            rows.append({\n                \"Crew\": crew,\n                \"Deployment\": dep_label,\n                \"Sites\": len(crew_visits),\n            })\n\n    if not rows:\n        return _empty_fig(\"Cross-Deployment Crew Comparison\")\n\n    comp_df = pd.DataFrame(rows)\n    fig = px.bar(\n        comp_df, x=\"Crew\", y=\"Sites\", color=\"Deployment\",\n        barmode=\"group\",\n        title=\"Cross-Deployment Crew Comparison (Sites Visited)\",\n        labels={\"Sites\": \"Sites Visited\", \"Crew\": \"Crew\"},\n        color_discrete_sequence=px.colors.qualitative.T10,\n    )\n    fig.update_layout(\n        template=\"plotly_white\",\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n        xaxis_tickangle=-30,\n        legend=dict(font=dict(size=10)),\n    )\n    return fig\n\n\ndef _build_deployment_sites_heatmap_fig(df):\n    \"\"\"Build deployment x sites heatmap figure.\"\"\"\n    title = \"Deployment \\u00d7 Sites Heatmap\"\n    if df.empty or df[\"deployment\"].isna().all():\n        return _empty_fig(title)\n\n    visits_df = _build_site_visits(df, \"chat\")\n    if visits_df.empty:\n        return _empty_fig(title)\n\n    visits_df = visits_df.copy()\n    visits_df[\"deployment\"] = visits_df[\"date\"].apply(\n        lambda d: _date_to_deployment.get(\n            d.date() if hasattr(d, \"date\") else d, \"\"))\n    visits_df = visits_df[visits_df[\"deployment\"] != \"\"]\n\n    heat = (visits_df.groupby([\"deployment\", \"location\"])\n            .size().reset_index(name=\"visits\"))\n    top_locs = (heat.groupby(\"location\")[\"visits\"].sum()\n                .sort_values(ascending=False).head(20).index.tolist())\n    heat = heat[heat[\"location\"].isin(top_locs)]\n\n    if heat.empty:\n        return _empty_fig(title)\n\n    pivot = heat.pivot_table(index=\"deployment\", columns=\"location\",\n                             values=\"visits\", fill_value=0)\n    dep_order = [d[\"label\"] for d in ALL_DEPLOYMENTS_LIST\n                 if d[\"label\"] in pivot.index]\n    pivot = pivot.reindex(dep_order)\n    col_order = [c for c in top_locs if c in pivot.columns]\n    pivot = pivot[col_order]\n\n    fig = go.Figure(data=go.Heatmap(\n        z=pivot.values,\n        x=[loc[:30] for loc in pivot.columns],\n        y=pivot.index.tolist(),\n        colorscale=\"Tealgrn\",\n        text=pivot.values,\n        texttemplate=\"%{text}\",\n        hovertemplate=(\n            \"Deployment: %{y}<br>Location: %{x}<br>\"\n            \"Visits: %{z}<extra></extra>\"),\n    ))\n    fig.update_layout(\n        template=\"plotly_white\",\n        title=f\"{title} (top 20 locations)\",\n        xaxis_title=\"Location\",\n        yaxis_title=\"Deployment\",\n        margin=dict(l=10, r=10, t=40, b=80),\n        height=400,\n        xaxis_tickangle=-45,\n    )\n    return fig\n\n\ndef _build_deployment_crew_trend_fig(df):\n    \"\"\"Build crew performance trend across deployments.\"\"\"\n    if df.empty or df[\"deployment\"].isna().all():\n        return _empty_fig(\"Crew Performance Trend Across Deployments\")\n\n    dep_order = [d[\"label\"] for d in ALL_DEPLOYMENTS_LIST]\n\n    rows = []\n    for dep_label, grp in df.groupby(\"deployment\"):\n        visits_df = _build_site_visits(grp, \"chat\")\n        if visits_df.empty:\n            continue\n        ds = _build_daily_summary(grp, \"chat\")\n        sc = _build_crew_scorecard(visits_df, ds)\n        if sc.empty:\n            continue\n        for _, crew_row in sc.iterrows():\n            rows.append({\n                \"Crew\": crew_row[\"sender\"],\n                \"Deployment\": dep_label,\n                \"Sites/Hr\": crew_row[\"avg_sites_per_hour\"],\n                \"Total Sites\": crew_row[\"total_sites\"],\n                \"Active Hrs\": crew_row[\"total_active_hrs\"],\n            })\n\n    if not rows:\n        return _empty_fig(\"Crew Performance Trend Across Deployments\")\n\n    trend_df = pd.DataFrame(rows)\n    dep_avg = (trend_df.groupby(\"Deployment\")[\"Sites/Hr\"]\n               .mean().reset_index(name=\"Avg Sites/Hr\"))\n\n    trend_df[\"dep_idx\"] = trend_df[\"Deployment\"].map(\n        {label: i for i, label in enumerate(dep_order)})\n    trend_df = trend_df.sort_values(\"dep_idx\")\n    dep_avg[\"dep_idx\"] = dep_avg[\"Deployment\"].map(\n        {label: i for i, label in enumerate(dep_order)})\n    dep_avg = dep_avg.sort_values(\"dep_idx\")\n\n    fig = go.Figure()\n    tableau = px.colors.qualitative.T10\n    crews = sorted(trend_df[\"Crew\"].unique())\n\n    for i, crew in enumerate(crews):\n        crew_data = trend_df[trend_df[\"Crew\"] == crew]\n        color = tableau[i % len(tableau)]\n        fig.add_trace(go.Scatter(\n            x=crew_data[\"Deployment\"],\n            y=crew_data[\"Sites/Hr\"],\n            mode=\"lines+markers\",\n            name=crew,\n            line=dict(color=color, width=2),\n            marker=dict(size=8, symbol=\"circle\"),\n            hovertemplate=(\n                \"<b>%{fullData.name}</b><br>\"\n                \"Deployment: %{x}<br>\"\n                \"Sites/Hr: %{y:.1f}<br>\"\n                \"<extra></extra>\"),\n        ))\n\n    fig.add_trace(go.Scatter(\n        x=dep_avg[\"Deployment\"],\n        y=dep_avg[\"Avg Sites/Hr\"],\n        mode=\"lines+markers\",\n        name=\"Deployment Avg\",\n        line=dict(color=\"#2c3e50\", width=3, dash=\"dash\"),\n        marker=dict(size=10, symbol=\"diamond\", color=\"#2c3e50\"),\n    ))\n\n    fig.update_layout(\n        template=\"plotly_white\",\n        title=\"Crew Performance Trend Across Deployments (Avg Sites/Hr)\",\n        xaxis_title=\"Deployment\",\n        yaxis_title=\"Sites per Hour\",\n        xaxis={\"categoryorder\": \"array\", \"categoryarray\": dep_order},\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=450,\n        legend=dict(font=dict(size=10), orientation=\"h\",\n                    yanchor=\"bottom\", y=-0.25, xanchor=\"center\", x=0.5),\n        hovermode=\"x unified\",\n    )\n    return fig\n\n\n# â”€â”€ Deployment Callbacks (thin wrappers) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"deployment-summary-container\", \"children\"), FILTER_INPUTS)\ndef deployment_summary_table(chats, senders, quality, msg_types, time_range,\n                             use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    dep_df, _ = _build_deployment_summary_data(df)\n    if dep_df.empty:\n        return html.P(\"No deployment data for current filters\",\n                       className=\"text-muted p-3\")\n    table = dash_table.DataTable(\n        data=dep_df.to_dict(\"records\"),\n        columns=[{\"name\": c, \"id\": c} for c in dep_df.columns],\n        sort_action=\"native\",\n        style_table={\"overflowX\": \"auto\"},\n        style_cell={\"textAlign\": \"left\", \"padding\": \"8px\", \"fontSize\": \"13px\"},\n        style_header={\n            \"backgroundColor\": \"#2c3e50\", \"color\": \"white\", \"fontWeight\": \"bold\",\n        },\n    )\n    return html.Div([html.H5(\"Deployment Summary\", className=\"mb-2\"), table])\n\n\n@app.callback(Output(\"chart-deployment-timeline\", \"figure\"), FILTER_INPUTS)\ndef chart_deployment_timeline(chats, senders, quality, msg_types, time_range,\n                              use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    return _build_deployment_timeline_fig(df)\n\n\n@app.callback(Output(\"chart-deployment-first-report\", \"figure\"), FILTER_INPUTS)\ndef chart_deployment_first_report(chats, senders, quality, msg_types, time_range,\n                                  use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    return _build_deployment_first_report_fig(df)\n\n\n@app.callback(Output(\"chart-deployment-transition-box\", \"figure\"), FILTER_INPUTS)\ndef chart_deployment_transition_box(chats, senders, quality, msg_types, time_range,\n                                    use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    return _build_deployment_transition_box_fig(df)\n\n@app.callback(Output(\"chart-deployment-crew-comparison\", \"figure\"), FILTER_INPUTS)\ndef chart_deployment_crew_comparison(chats, senders, quality, msg_types,\n                                     time_range, use_resolved, date_start,\n                                     date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    return _build_deployment_crew_comparison_fig(df)\n\n\n@app.callback(Output(\"chart-deployment-sites-heatmap\", \"figure\"), FILTER_INPUTS)\ndef chart_deployment_sites_heatmap(chats, senders, quality, msg_types,\n                                   time_range, use_resolved, date_start,\n                                   date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    return _build_deployment_sites_heatmap_fig(df)\n\n\n@app.callback(Output(\"chart-deployment-crew-trend\", \"figure\"), FILTER_INPUTS)\ndef chart_deployment_crew_trend(chats, senders, quality, msg_types, time_range,\n                                use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    return _build_deployment_crew_trend_fig(df)\n\n\n# â”€â”€ Deployment Download Callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nFILTER_STATES = [\n    State(\"filter-chats\", \"value\"),\n    State(\"filter-senders\", \"value\"),\n    State(\"filter-quality\", \"value\"),\n    State(\"filter-types\", \"value\"),\n    State(\"filter-time\", \"value\"),\n    State(\"filter-resolved\", \"value\"),\n    State(\"filter-dates\", \"start_date\"),\n    State(\"filter-dates\", \"end_date\"),\n    State(\"filter-deployment\", \"value\"),\n]\n\n\n@app.callback(\n    Output(\"download-deployment-pdf\", \"data\"),\n    Input(\"btn-download-deployment-pdf\", \"n_clicks\"),\n    FILTER_STATES,\n    prevent_initial_call=True,\n)\ndef download_deployment_report(n_clicks, chats, senders, quality, msg_types,\n                               time_range, use_resolved, date_start, date_end,\n                               deployments):\n    if not n_clicks:\n        return None\n\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n\n    # Build all figures\n    dep_df, crew_breakdowns = _build_deployment_summary_data(df)\n    fig_timeline = _build_deployment_timeline_fig(df)\n    fig_first_report = _build_deployment_first_report_fig(df)\n    fig_transition = _build_deployment_transition_box_fig(df)\n    fig_crew_trend = _build_deployment_crew_trend_fig(df)\n    fig_crew_comp = _build_deployment_crew_comparison_fig(df)\n    fig_heatmap = _build_deployment_sites_heatmap_fig(df)\n\n    # Convert figures to embedded HTML\n    pjs = \"cdn\"\n    chart_timeline = pio.to_html(fig_timeline, full_html=False, include_plotlyjs=pjs)\n    chart_first = pio.to_html(fig_first_report, full_html=False, include_plotlyjs=False)\n    chart_trans = pio.to_html(fig_transition, full_html=False, include_plotlyjs=False)\n    chart_trend = pio.to_html(fig_crew_trend, full_html=False, include_plotlyjs=False)\n    chart_comp = pio.to_html(fig_crew_comp, full_html=False, include_plotlyjs=False)\n    chart_heat = pio.to_html(fig_heatmap, full_html=False, include_plotlyjs=False)\n\n    # Summary table HTML\n    summary_html = \"\"\n    if not dep_df.empty:\n        summary_html = dep_df.to_html(index=False, classes=\"summary-table\",\n                                       border=0)\n\n    # Per-deployment crew breakdown tables\n    breakdown_html = \"\"\n    for dep_label, sc in crew_breakdowns.items():\n        sc_display = sc.rename(columns={\n            \"sender\": \"Crew\",\n            \"days_active\": \"Days\",\n            \"total_sites\": \"Sites\",\n            \"avg_sites_per_day\": \"Sites/Day\",\n            \"avg_sites_per_hour\": \"Sites/Hr\",\n            \"avg_transition_min\": \"Transition (min)\",\n            \"total_active_hrs\": \"Active Hrs\",\n        })\n        breakdown_html += f\"<h3>{dep_label}</h3>\\n\"\n        breakdown_html += sc_display.to_html(index=False, classes=\"breakdown-table\",\n                                              border=0)\n\n    now_str = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    filename = f\"deployment_report_{datetime.now().strftime('%Y%m%d_%H%M')}.html\"\n\n    html_content = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<title>Deployment Report â€” {now_str}</title>\n<style>\n  body {{ font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif;\n         margin: 2em; color: #2c3e50; }}\n  h1 {{ color: #2c3e50; border-bottom: 2px solid #2c3e50; padding-bottom: 0.3em; }}\n  h2 {{ color: #34495e; margin-top: 2em; }}\n  h3 {{ color: #7f8c8d; }}\n  .summary-table, .breakdown-table {{\n    border-collapse: collapse; width: 100%; margin: 1em 0; font-size: 13px;\n  }}\n  .summary-table th, .breakdown-table th {{\n    background: #2c3e50; color: white; padding: 8px 12px; text-align: left;\n  }}\n  .summary-table td, .breakdown-table td {{\n    padding: 6px 12px; border-bottom: 1px solid #ecf0f1;\n  }}\n  .summary-table tr:nth-child(even), .breakdown-table tr:nth-child(even) {{\n    background: #f8f9fa;\n  }}\n  .chart-row {{ display: flex; gap: 1em; margin: 1em 0; }}\n  .chart-row > div {{ flex: 1; }}\n  @media print {{\n    body {{ margin: 0.5em; font-size: 11px; }}\n    .chart-row {{ page-break-inside: avoid; }}\n    h2 {{ page-break-before: auto; }}\n    .plotly-graph-div {{ max-height: 350px !important; }}\n    .modebar {{ display: none !important; }}\n  }}\n</style>\n</head>\n<body>\n<h1>Deployment Report</h1>\n<p>Generated: {now_str}</p>\n\n<h2>Summary</h2>\n{summary_html}\n\n<h2>Deployment Timeline</h2>\n{chart_timeline}\n\n<div class=\"chart-row\">\n  <div>\n    <h2>First Report Time</h2>\n    {chart_first}\n  </div>\n  <div>\n    <h2>Transition Time</h2>\n    {chart_trans}\n  </div>\n</div>\n\n<h2>Crew Performance Trend</h2>\n{chart_trend}\n\n<div class=\"chart-row\">\n  <div>\n    <h2>Crew Comparison</h2>\n    {chart_comp}\n  </div>\n  <div>\n    <h2>Sites Heatmap</h2>\n    {chart_heat}\n  </div>\n</div>\n\n<h2>Per-Deployment Crew Breakdown</h2>\n{breakdown_html}\n\n</body>\n</html>\"\"\"\n\n    return dcc.send_bytes(html_content.encode(\"utf-8\"), filename)\n\n\n# â”€â”€ KPI Bar Callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"kpi-bar\", \"children\"), FILTER_INPUTS)\ndef update_kpi_cards(chats, senders, quality, msg_types, time_range,\n                     use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n\n    total_msgs = len(df)\n    active_crews = df[\"chat\"].nunique()\n\n    visits_df = _build_site_visits(df, \"chat\")\n    ds = _build_daily_summary(df, scol)\n\n    total_sites = _count_billable_routes_from_df(df)\n    avg_sites_hr = 0.0\n    avg_trans = 0.0\n    if not visits_df.empty:\n        sc = _build_crew_scorecard(visits_df, ds)\n        if not sc.empty:\n            avg_sites_hr = sc[\"avg_sites_per_hour\"].mean()\n            avg_trans = sc[\"avg_transition_min\"].mean()\n\n    avg_first_str = \"N/A\"\n    if not ds.empty:\n        avg_first = ds[\"first_hour\"].mean()\n        fh = int(avg_first)\n        fm = int((avg_first - fh) * 60)\n        period = \"AM\" if fh < 12 else \"PM\"\n        dh = fh % 12 or 12\n        avg_first_str = f\"{dh}:{fm:02d} {period}\"\n\n    kpi_data = [\n        (\"Total Messages\", f\"{total_msgs:,}\", \"#3498db\"),\n        (\"Active Crews\", str(active_crews), \"#2ecc71\"),\n        (\"Avg Sites/Hour\", f\"{avg_sites_hr:.1f}\", \"#e67e22\"),\n        (\"Avg First Report\", avg_first_str, \"#9b59b6\"),\n        (\"Billable Routes\", f\"{total_sites:,}\", \"#1abc9c\"),\n        (\"Avg Transition\", f\"{avg_trans:.1f} min\", \"#e74c3c\"),\n    ]\n\n    cards = []\n    for label, value, color in kpi_data:\n        cards.append(dbc.Col(\n            dbc.Card(\n                dbc.CardBody([\n                    html.H3(value, className=\"mb-0 fw-bold text-center\",\n                            style={\"fontSize\": \"1.5rem\"}),\n                    html.P(label, className=\"text-muted text-center mb-0\",\n                           style={\"fontSize\": \"12px\"}),\n                ], className=\"py-2 px-2\"),\n                style={\"borderTop\": f\"3px solid {color}\"},\n                className=\"shadow-sm\",\n            ),\n            md=2, sm=4, xs=6, className=\"mb-2\",\n        ))\n\n    return dbc.Row(cards, className=\"g-2\")\n\n\n# â”€â”€ Productivity: Daily Productivity Score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _build_daily_productivity_score(df, scol):\n    ds = _build_daily_summary(df, scol)\n    visits_df = _build_site_visits(df, scol)\n    if ds.empty or visits_df.empty:\n        return pd.DataFrame()\n\n    site_counts = (visits_df.groupby([\"sender\", \"date\"])\n                   .size().reset_index(name=\"sites_visited\"))\n    site_counts[\"date\"] = pd.to_datetime(site_counts[\"date\"])\n\n    active_hrs = []\n    for (sender, d), day_grp in visits_df.groupby(\n            [\"sender\", visits_df[\"date\"].dt.date]):\n        hrs = _compute_active_hours(day_grp)\n        active_hrs.append({\n            \"sender\": sender,\n            \"date\": pd.Timestamp(d),\n            \"active_hrs\": hrs,\n        })\n    active_df = pd.DataFrame(active_hrs) if active_hrs else pd.DataFrame(\n        columns=[\"sender\", \"date\", \"active_hrs\"])\n\n    ds[\"date\"] = pd.to_datetime(ds[\"date\"])\n    merged = ds.merge(site_counts, on=[\"sender\", \"date\"], how=\"left\")\n    merged[\"sites_visited\"] = merged[\"sites_visited\"].fillna(0).astype(int)\n\n    if not active_df.empty:\n        merged = merged.merge(active_df, on=[\"sender\", \"date\"], how=\"left\")\n        merged[\"active_hrs\"] = merged[\"active_hrs\"].fillna(1 / 60)\n    else:\n        merged[\"active_hrs\"] = 1 / 60\n\n    merged[\"sites_per_hour\"] = merged[\"sites_visited\"] / merged[\"active_hrs\"]\n\n    merged[\"punctuality_score\"] = merged[\"first_hour\"].apply(\n        lambda h: max(0, min(100, 100 - (h - 7) * 50)) if h >= 7 else 100\n    )\n\n    max_sph = merged[\"sites_per_hour\"].max()\n    if max_sph > 0:\n        merged[\"pace_score\"] = (merged[\"sites_per_hour\"] / max_sph * 100).clip(0, 100)\n    else:\n        merged[\"pace_score\"] = 0.0\n\n    avg_sites = merged[\"sites_visited\"].mean()\n    if avg_sites > 0:\n        merged[\"coverage\"] = (merged[\"sites_visited\"] / avg_sites * 100).clip(0, 100)\n    else:\n        merged[\"coverage\"] = 0.0\n\n    merged[\"productivity_score\"] = (\n        0.5 * merged[\"pace_score\"]\n        + 0.3 * merged[\"punctuality_score\"]\n        + 0.2 * merged[\"coverage\"]\n    ).round(1)\n\n    result = merged[[\"sender\", \"date\", \"sites_visited\", \"sites_per_hour\",\n                      \"first_hour\", \"punctuality_score\", \"pace_score\",\n                      \"coverage\", \"productivity_score\"]].copy()\n    result[\"date\"] = result[\"date\"].dt.strftime(\"%Y-%m-%d\")\n    result[\"sites_per_hour\"] = result[\"sites_per_hour\"].round(1)\n    result[\"first_hour\"] = result[\"first_hour\"].round(2)\n    result[\"punctuality_score\"] = result[\"punctuality_score\"].round(0).astype(int)\n    result[\"pace_score\"] = result[\"pace_score\"].round(0).astype(int)\n    return result\n\n\n@app.callback(Output(\"productivity-score-container\", \"children\"), FILTER_INPUTS)\ndef update_productivity_score(chats, senders, quality, msg_types, time_range,\n                              use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    ps = _build_daily_productivity_score(df, scol)\n    if ps.empty:\n        return html.P(\"No productivity data for current filters\",\n                       className=\"text-muted p-3\")\n\n    ps_display = ps.rename(columns={\n        \"sender\": \"Sender\", \"date\": \"Date\", \"sites_visited\": \"Sites\",\n        \"sites_per_hour\": \"Sites/Hr\", \"first_hour\": \"First Hour\",\n        \"punctuality_score\": \"Punctuality\", \"pace_score\": \"Pace\",\n        \"productivity_score\": \"Score\",\n    })\n\n    table = dash_table.DataTable(\n        data=ps_display.to_dict(\"records\"),\n        columns=[{\"name\": c, \"id\": c} for c in ps_display.columns],\n        filter_action=\"native\",\n        sort_action=\"native\",\n        sort_mode=\"multi\",\n        page_size=20,\n        style_table={\"overflowX\": \"auto\"},\n        style_cell={\"textAlign\": \"left\", \"padding\": \"8px\", \"fontSize\": \"13px\"},\n        style_header={\n            \"backgroundColor\": \"#2c3e50\", \"color\": \"white\", \"fontWeight\": \"bold\",\n        },\n        style_data_conditional=[\n            {\"if\": {\"filter_query\": \"{Score} >= 70\"},\n             \"backgroundColor\": \"#eafaf1\"},\n            {\"if\": {\"filter_query\": \"{Score} >= 40 && {Score} < 70\"},\n             \"backgroundColor\": \"#fef9e7\"},\n            {\"if\": {\"filter_query\": \"{Score} < 40\"},\n             \"backgroundColor\": \"#fde8e8\", \"color\": \"#c0392b\"},\n        ],\n    )\n    return html.Div([\n        html.H5(\"Daily Productivity Score\", className=\"mb-2\"),\n        table,\n    ])\n\n\n# â”€â”€ Productivity: Crew Leaderboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"crew-leaderboard-container\", \"children\"), FILTER_INPUTS)\ndef update_crew_leaderboard(chats, senders, quality, msg_types, time_range,\n                            use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    ps = _build_daily_productivity_score(df, scol)\n    if ps.empty:\n        return html.P(\"No leaderboard data for current filters\",\n                       className=\"text-muted p-3\")\n\n    agg = ps.groupby(\"sender\").agg(\n        avg_score=(\"productivity_score\", \"mean\"),\n        best_day=(\"productivity_score\", \"max\"),\n        days=(\"date\", \"count\"),\n    ).reset_index()\n\n    if len(ps[\"date\"].unique()) >= 2:\n        dates_sorted = sorted(ps[\"date\"].unique())\n        mid = len(dates_sorted) // 2\n        early_dates = set(dates_sorted[:mid])\n        late_dates = set(dates_sorted[mid:])\n        trends = []\n        for sender in agg[\"sender\"]:\n            s_data = ps[ps[\"sender\"] == sender]\n            early_avg = s_data[s_data[\"date\"].isin(early_dates)][\"productivity_score\"].mean()\n            late_avg = s_data[s_data[\"date\"].isin(late_dates)][\"productivity_score\"].mean()\n            if pd.isna(early_avg) or pd.isna(late_avg):\n                trends.append(\"â”€ stable\")\n            elif late_avg - early_avg > 3:\n                trends.append(\"â†‘ improving\")\n            elif early_avg - late_avg > 3:\n                trends.append(\"â†“ declining\")\n            else:\n                trends.append(\"â”€ stable\")\n        agg[\"trend\"] = trends\n    else:\n        agg[\"trend\"] = \"â”€ stable\"\n\n    agg = agg.sort_values(\"avg_score\", ascending=False).reset_index(drop=True)\n    agg.insert(0, \"rank\", range(1, len(agg) + 1))\n    agg[\"avg_score\"] = agg[\"avg_score\"].round(1)\n    agg[\"best_day\"] = agg[\"best_day\"].round(1)\n\n    lb_display = agg.rename(columns={\n        \"rank\": \"Rank\", \"sender\": \"Crew\", \"avg_score\": \"Avg Score\",\n        \"best_day\": \"Best Day\", \"trend\": \"Trend\", \"days\": \"Days\",\n    })\n\n    table = dash_table.DataTable(\n        data=lb_display.to_dict(\"records\"),\n        columns=[{\"name\": c, \"id\": c} for c in lb_display.columns],\n        sort_action=\"native\",\n        style_table={\"overflowX\": \"auto\"},\n        style_cell={\"textAlign\": \"left\", \"padding\": \"8px\", \"fontSize\": \"13px\"},\n        style_header={\n            \"backgroundColor\": \"#2c3e50\", \"color\": \"white\", \"fontWeight\": \"bold\",\n        },\n        style_data_conditional=[\n            {\"if\": {\"filter_query\": \"{Avg Score} >= 70\"},\n             \"backgroundColor\": \"#eafaf1\"},\n            {\"if\": {\"filter_query\": \"{Avg Score} >= 40 && {Avg Score} < 70\"},\n             \"backgroundColor\": \"#fef9e7\"},\n            {\"if\": {\"filter_query\": \"{Avg Score} < 40\"},\n             \"backgroundColor\": \"#fde8e8\", \"color\": \"#c0392b\"},\n        ],\n    )\n    return html.Div([\n        html.H5(\"Crew Leaderboard\", className=\"mb-2\"),\n        table,\n    ])\n\n\n# â”€â”€ Productivity: Idle Time Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _build_idle_gaps(df, scol, threshold_min=45):\n    df_valid = df[(df[scol] != \"\") & df[\"time\"].notna() & df[\"msg_date\"].notna()].copy()\n    if df_valid.empty:\n        return pd.DataFrame()\n\n    df_valid = df_valid.sort_values([scol, \"msg_date\", \"time\"])\n    rows = []\n    for sender, grp in df_valid.groupby(scol):\n        for d, day_grp in grp.groupby(grp[\"msg_date\"].dt.date):\n            times = day_grp[\"time\"].sort_values()\n            if len(times) < 2:\n                continue\n            diffs = times.diff().dropna()\n            for idx, gap in diffs.items():\n                gap_min = gap.total_seconds() / 60.0\n                if gap_min >= threshold_min:\n                    gap_end_time = times.loc[idx]\n                    gap_start_time = gap_end_time - gap\n                    rows.append({\n                        \"sender\": sender,\n                        \"date\": str(d),\n                        \"gap_start\": gap_start_time.strftime(\"%I:%M %p\"),\n                        \"gap_end\": gap_end_time.strftime(\"%I:%M %p\"),\n                        \"gap_minutes\": round(gap_min, 1),\n                    })\n\n    return pd.DataFrame(rows) if rows else pd.DataFrame(\n        columns=[\"sender\", \"date\", \"gap_start\", \"gap_end\", \"gap_minutes\"])\n\n\n@app.callback(Output(\"chart-idle-gaps\", \"figure\"), FILTER_INPUTS)\ndef chart_idle_gaps(chats, senders, quality, msg_types, time_range,\n                    use_resolved, date_start, date_end, deployments):\n    df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                 date_start, date_end, deployments)\n    scol = _sender_col(use_resolved)\n    gaps = _build_idle_gaps(df, scol)\n    if gaps.empty:\n        return _empty_fig(\"Idle Time Detection\")\n\n    incident_counts = gaps.groupby(\"sender\").agg(\n        incidents=(\"gap_minutes\", \"size\"),\n        avg_gap=(\"gap_minutes\", \"mean\"),\n        max_gap=(\"gap_minutes\", \"max\"),\n    ).reset_index().sort_values(\"incidents\")\n\n    fig = px.bar(\n        incident_counts, y=\"sender\", x=\"incidents\", orientation=\"h\",\n        text=\"incidents\",\n        title=\"Idle Time Incidents per Crew (gaps > 45 min)\",\n        labels={\"incidents\": \"Idle Incidents\", \"sender\": \"Crew\"},\n        color=\"avg_gap\",\n        color_continuous_scale=\"OrRd\",\n        hover_data={\"avg_gap\": \":.1f\", \"max_gap\": \":.1f\"},\n    )\n    fig.update_traces(textposition=\"outside\", textfont_size=11)\n    fig.update_layout(\n        margin=dict(l=10, r=60, t=40, b=10),\n        height=400,\n        yaxis={\"categoryorder\": \"total ascending\"},\n        coloraxis_colorbar_title=\"Avg Gap<br>(min)\",\n    )\n    return fig\n\n\n# â”€â”€ Operations Tab Callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(Output(\"chart-routing-gantt\", \"figure\"), FILTER_INPUTS)\ndef chart_routing_gantt(chats, senders, quality, msg_types, time_range,\n                        use_resolved, date_start, date_end, deployments):\n    try:\n        df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                     date_start, date_end, deployments)\n        job_logs = build_job_logs(df, SNOW_CONFIG)\n        if job_logs.empty:\n            return _empty_fig(\"Crew Routing Timeline\")\n        loc_coords = build_location_coords(SNOW_CONFIG.get(\"location_registry\", []))\n        segments = build_route_segments(job_logs, SNOW_CONFIG, loc_coords)\n        if segments.empty:\n            return _empty_fig(\"Crew Routing Timeline\")\n        gantt_data = []\n        for _, seg in segments.iterrows():\n            t0 = seg[\"start_time\"]\n            t1 = seg[\"end_time\"]\n            if pd.isna(t0) or pd.isna(t1):\n                continue\n            dist = seg.get(\"distance_km\", np.nan) if \"distance_km\" in segments.columns else np.nan\n            dur = seg.get(\"actual_duration_mins\", np.nan)\n            eff = seg.get(\"travel_efficiency\", np.nan) if \"travel_efficiency\" in segments.columns else np.nan\n            hover_extra = \"\"\n            if pd.notna(dist):\n                hover_extra += f\" | {dist:.1f}km\"\n            if pd.notna(dur):\n                hover_extra += f\" | {dur:.0f}min\"\n            if pd.notna(eff):\n                hover_extra += f\" | eff:{eff:.0f}%\"\n            gantt_data.append({\n                \"Crew\": seg[\"crew\"],\n                \"Start\": t0,\n                \"Finish\": t1,\n                \"Location\": seg[\"destination_location\"] or \"Unknown\",\n                \"Details\": f\"{seg['origin_location']} â†’ {seg['destination_location']}{hover_extra}\",\n            })\n        if not gantt_data:\n            return _empty_fig(\"Crew Routing Timeline\")\n        gantt_df = pd.DataFrame(gantt_data)\n        fig = px.timeline(\n            gantt_df, x_start=\"Start\", x_end=\"Finish\", y=\"Crew\", color=\"Location\",\n            title=\"Crew Routing Timeline\",\n            hover_data=[\"Details\"] if \"Details\" in gantt_df.columns else None,\n        )\n        fig.update_layout(\n            margin=dict(l=10, r=10, t=40, b=10),\n            height=450,\n            yaxis={\"categoryorder\": \"category ascending\"},\n        )\n        return fig\n    except Exception as e:\n        print(f\"[chart-routing-gantt] Error: {e}\")\n        return _empty_fig(\"Crew Routing Timeline\")\n\n\n@app.callback(Output(\"chart-burndown\", \"figure\"), FILTER_INPUTS)\ndef chart_burndown(chats, senders, quality, msg_types, time_range,\n                   use_resolved, date_start, date_end, deployments):\n    try:\n        df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                     date_start, date_end, deployments)\n        job_logs = build_job_logs(df, SNOW_CONFIG)\n        burndown = build_deployment_burndown(job_logs, ALL_DEPLOYMENTS_LIST, SNOW_CONFIG)\n        if burndown.empty:\n            return _empty_fig(\"Deployment Burn-Down\")\n        fig = go.Figure()\n        for dep_label in burndown[\"deployment\"].unique():\n            dep_data = burndown[burndown[\"deployment\"] == dep_label]\n            crew_count = int(dep_data[\"crew_count\"].iloc[0]) if \"crew_count\" in dep_data.columns and not dep_data.empty else 1\n            total_sites = int(dep_data[\"cumulative_completed\"].max()) if not dep_data.empty else 0\n            base_hrs = SNOW_CONFIG.get(\"expected_deployment_hours\", 12.0)\n            adj_hrs = base_hrs / max(crew_count, 1)\n            fig.add_trace(go.Scatter(\n                x=dep_data[\"timestamp\"], y=dep_data[\"cumulative_completed\"],\n                mode=\"lines+markers\",\n                name=f\"{dep_label} actual ({crew_count} crews, {total_sites} sites)\",\n                line=dict(dash=\"solid\"),\n            ))\n            fig.add_trace(go.Scatter(\n                x=dep_data[\"timestamp\"], y=dep_data[\"expected_completed\"],\n                mode=\"lines\",\n                name=f\"{dep_label} expected ({adj_hrs:.1f}h adj.)\",\n                line=dict(dash=\"dash\"),\n            ))\n        fig.update_layout(\n            title=\"Deployment Burn-Down: Actual vs Expected (crew-adjusted)\",\n            xaxis_title=\"Time\", yaxis_title=\"Cumulative Sites Completed\",\n            margin=dict(l=10, r=10, t=40, b=10),\n            height=450,\n            legend=dict(font=dict(size=11)),\n        )\n        return fig\n    except Exception as e:\n        print(f\"[chart-burndown] Error: {e}\")\n        return _empty_fig(\"Deployment Burn-Down\")\n\n\n@app.callback(Output(\"location-type-stats-container\", \"children\"), FILTER_INPUTS)\ndef location_type_stats(chats, senders, quality, msg_types, time_range,\n                        use_resolved, date_start, date_end, deployments):\n    try:\n        df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                     date_start, date_end, deployments)\n        job_logs = build_job_logs(df, SNOW_CONFIG)\n        stats = build_location_type_stats(job_logs)\n        if stats.empty:\n            return html.P(\"No location type data available.\", className=\"text-muted p-3\")\n        fig = px.bar(\n            stats, x=\"location_type\", y=\"avg_duration_min\",\n            text=\"avg_duration_min\", color=\"location_type\",\n            title=\"Avg Service Time by Location Type\",\n            labels={\"avg_duration_min\": \"Avg Duration (min)\", \"location_type\": \"Type\"},\n        )\n        fig.update_traces(textposition=\"outside\", texttemplate=\"%{text:.1f}\")\n        fig.update_layout(\n            margin=dict(l=10, r=10, t=40, b=10),\n            height=350, showlegend=False,\n        )\n        return dcc.Graph(figure=fig)\n    except Exception as e:\n        print(f\"[location-type-stats] Error: {e}\")\n        return html.P(\"Error loading location type stats.\", className=\"text-muted p-3\")\n\n\n@app.callback(Output(\"traffic-analysis-container\", \"children\"), FILTER_INPUTS)\ndef traffic_analysis(chats, senders, quality, msg_types, time_range,\n                     use_resolved, date_start, date_end, deployments):\n    try:\n        df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                     date_start, date_end, deployments)\n        job_logs = build_job_logs(df, SNOW_CONFIG)\n        loc_coords = build_location_coords(SNOW_CONFIG.get(\"location_registry\", []))\n        segments = build_route_segments(job_logs, SNOW_CONFIG, loc_coords)\n        traffic = build_traffic_analysis(segments)\n        if traffic.empty:\n            return html.P(\"No traffic data available.\", className=\"text-muted p-3\")\n\n        col_labels = {\n            \"origin\": \"From\", \"destination\": \"To\",\n            \"avg_travel_min\": \"Avg Actual (min)\", \"count\": \"Trips\",\n            \"std_travel_min\": \"Std Dev\", \"distance_km\": \"Distance (km)\",\n            \"est_travel_min\": \"Est. Travel (min)\", \"avg_efficiency\": \"Efficiency %\",\n        }\n        display_cols = [c for c in traffic.columns if not traffic[c].isna().all()]\n        table_cols = [{\"name\": col_labels.get(c, c), \"id\": c} for c in display_cols]\n\n        children = [\n            html.H6(\"Travel Time vs Distance Between Locations\", className=\"mb-2\"),\n            dash_table.DataTable(\n                data=traffic.to_dict(\"records\"),\n                columns=table_cols,\n                style_table={\"overflowX\": \"auto\", \"maxHeight\": \"350px\", \"overflowY\": \"auto\"},\n                style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"13px\"},\n                style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f8f9fa\"},\n                style_data_conditional=[\n                    {\"if\": {\"filter_query\": \"{avg_efficiency} < 50\", \"column_id\": \"avg_efficiency\"},\n                     \"color\": \"#e74c3c\", \"fontWeight\": \"bold\"},\n                    {\"if\": {\"filter_query\": \"{avg_efficiency} >= 80\", \"column_id\": \"avg_efficiency\"},\n                     \"color\": \"#27ae60\", \"fontWeight\": \"bold\"},\n                ],\n                page_size=15,\n                sort_action=\"native\",\n            ),\n        ]\n\n        has_dist = \"distance_km\" in segments.columns\n        dist_segs = segments.dropna(subset=[\"distance_km\", \"actual_duration_mins\"]) if has_dist else pd.DataFrame()\n        if not dist_segs.empty and len(dist_segs) >= 3:\n            fig_scatter = px.scatter(\n                dist_segs,\n                x=\"distance_km\",\n                y=\"actual_duration_mins\",\n                color=\"crew\",\n                hover_data=[\"origin_location\", \"destination_location\"],\n                labels={\"distance_km\": \"Distance (km)\", \"actual_duration_mins\": \"Actual Travel (min)\", \"crew\": \"Crew\"},\n                title=\"Actual Travel Time vs Distance\",\n                color_discrete_sequence=px.colors.qualitative.Bold,\n            )\n            max_dist = dist_segs[\"distance_km\"].max()\n            expected_x = [0, max_dist]\n            expected_y = [0, max_dist / 25.0 * 60]\n            fig_scatter.add_trace(go.Scatter(\n                x=expected_x, y=expected_y,\n                mode=\"lines\", name=\"Expected (25 km/h)\",\n                line=dict(dash=\"dash\", color=\"gray\", width=2),\n            ))\n            fig_scatter.update_layout(height=400, template=\"plotly_white\")\n            children.append(dcc.Graph(figure=fig_scatter))\n\n        return html.Div(children)\n    except Exception as e:\n        print(f\"[traffic-analysis] Error: {e}\")\n        return html.P(\"Error loading traffic analysis.\", className=\"text-muted p-3\")\n\n\n@app.callback(Output(\"delay-report-container\", \"children\"), FILTER_INPUTS)\ndef delay_report(chats, senders, quality, msg_types, time_range,\n                 use_resolved, date_start, date_end, deployments):\n    try:\n        df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                     date_start, date_end, deployments)\n        job_logs = build_job_logs(df, SNOW_CONFIG)\n        loc_coords = build_location_coords(SNOW_CONFIG.get(\"location_registry\", []))\n        segments = build_route_segments(job_logs, SNOW_CONFIG, loc_coords)\n        delays = build_delay_report(segments, SNOW_CONFIG)\n        if delays.empty:\n            return html.P(\"No delayed segments found.\", className=\"text-muted p-3\")\n        display = delays.copy()\n        if \"date\" in display.columns:\n            display[\"date\"] = display[\"date\"].astype(str)\n        return html.Div([\n            html.H6(\"Delay Report â€” Segments Exceeding Expected Time\", className=\"mb-2\"),\n            dash_table.DataTable(\n                data=display.to_dict(\"records\"),\n                columns=[{\"name\": c, \"id\": c} for c in display.columns],\n                style_table={\"overflowX\": \"auto\", \"maxHeight\": \"350px\", \"overflowY\": \"auto\"},\n                style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"13px\"},\n                style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f8f9fa\"},\n                page_size=15,\n            ),\n        ])\n    except Exception as e:\n        print(f\"[delay-report] Error: {e}\")\n        return html.P(\"Error loading delay report.\", className=\"text-muted p-3\")\n\n\n@app.callback(Output(\"recall-summary-container\", \"children\"), FILTER_INPUTS)\ndef recall_summary(chats, senders, quality, msg_types, time_range,\n                   use_resolved, date_start, date_end, deployments):\n    try:\n        df = _get_df(chats, senders, quality, msg_types, time_range, use_resolved,\n                     date_start, date_end, deployments)\n        job_logs = build_job_logs(df, SNOW_CONFIG)\n        if job_logs.empty or not job_logs[\"is_recall\"].any():\n            return html.P(\"No recalls found.\", className=\"text-muted p-3\")\n        recalls = job_logs[job_logs[\"is_recall\"]].copy()\n        total_recalls = len(recalls)\n        total_added = recalls[\"recall_added_time_mins\"].sum()\n        crew_recalls = recalls.groupby(\"crew\").agg(\n            recall_count=(\"is_recall\", \"sum\"),\n            total_added_min=(\"recall_added_time_mins\", \"sum\"),\n        ).reset_index()\n        crew_recalls[\"total_added_min\"] = crew_recalls[\"total_added_min\"].round(1)\n        loc_recalls = recalls.groupby(\"location\")[\"is_recall\"].count().reset_index()\n        loc_recalls.columns = [\"location\", \"recall_count\"]\n        loc_recalls = loc_recalls.sort_values(\"recall_count\", ascending=False).head(10)\n        return html.Div([\n            html.H6(\"Recall Summary\", className=\"mb-2\"),\n            dbc.Row([\n                dbc.Col(dbc.Card(dbc.CardBody([\n                    html.H3(str(total_recalls), className=\"text-primary\"),\n                    html.Small(\"Total Recalls\"),\n                ]), className=\"text-center\"), md=3),\n                dbc.Col(dbc.Card(dbc.CardBody([\n                    html.H3(f\"{total_added:.0f}\" if pd.notna(total_added) else \"N/A\", className=\"text-warning\"),\n                    html.Small(\"Total Added Minutes\"),\n                ]), className=\"text-center\"), md=3),\n                dbc.Col(dbc.Card(dbc.CardBody([\n                    html.H3(str(crew_recalls[\"crew\"].nunique()), className=\"text-info\"),\n                    html.Small(\"Crews with Recalls\"),\n                ]), className=\"text-center\"), md=3),\n            ], className=\"mb-3\"),\n            dbc.Row([\n                dbc.Col([\n                    html.P(\"Recalls by Crew\", className=\"fw-bold mb-1\"),\n                    dash_table.DataTable(\n                        data=crew_recalls.to_dict(\"records\"),\n                        columns=[{\"name\": c, \"id\": c} for c in crew_recalls.columns],\n                        style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"13px\"},\n                        style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f8f9fa\"},\n                        page_size=10,\n                    ),\n                ], md=6),\n                dbc.Col([\n                    html.P(\"Top Recalled Locations\", className=\"fw-bold mb-1\"),\n                    dash_table.DataTable(\n                        data=loc_recalls.to_dict(\"records\"),\n                        columns=[{\"name\": c, \"id\": c} for c in loc_recalls.columns],\n                        style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"13px\"},\n                        style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f8f9fa\"},\n                        page_size=10,\n                    ),\n                ], md=6),\n            ]),\n        ])\n    except Exception as e:\n        print(f\"[recall-summary] Error: {e}\")\n        return html.P(\"Error loading recall summary.\", className=\"text-muted p-3\")\n\n\n# â”€â”€ Settings Tab Callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"settings-crew-types-container\", \"children\"),\n    Input(\"main-tabs\", \"active_tab\"),\n)\ndef render_crew_type_settings(active_tab):\n    if active_tab != \"tab-settings\":\n        raise PreventUpdate\n    loc_types = SNOW_CONFIG.get(\"location_types\", {})\n    rows = []\n    for chat in ALL_CHATS:\n        current = loc_types.get(chat, \"\")\n        auto = infer_location_type(chat)\n        auto_label = f\"Auto-detect ({auto})\"\n        rows.append(\n            dbc.Row([\n                dbc.Col(html.Span(chat, style={\"fontSize\": \"13px\"}), md=6),\n                dbc.Col(dcc.Dropdown(\n                    id={\"type\": \"crew-type-dd\", \"chat\": chat},\n                    options=[\n                        {\"label\": auto_label, \"value\": \"\"},\n                        {\"label\": \"Sidewalk\", \"value\": \"Sidewalk\"},\n                        {\"label\": \"Parking Lot\", \"value\": \"Parking Lot\"},\n                    ],\n                    value=current,\n                    clearable=False,\n                    style={\"fontSize\": \"13px\"},\n                ), md=6),\n            ], className=\"mb-1 align-items-center\")\n        )\n    return html.Div(rows, style={\"maxHeight\": \"500px\", \"overflowY\": \"auto\"})\n\n\n@app.callback(\n    Output(\"settings-non-trackable-container\", \"children\"),\n    Input(\"main-tabs\", \"active_tab\"),\n)\ndef render_non_trackable_settings(active_tab):\n    if active_tab != \"tab-settings\":\n        raise PreventUpdate\n    non_trackable = SNOW_CONFIG.get(\"non_trackable_senders\", [])\n    items = []\n    for sender in ALL_SENDERS_RESOLVED_FULL:\n        items.append(\n            dbc.Checkbox(\n                id={\"type\": \"non-track-cb\", \"sender\": sender},\n                label=sender,\n                value=sender in non_trackable,\n                className=\"mb-1\",\n                style={\"fontSize\": \"13px\"},\n            )\n        )\n    return html.Div(items, style={\"maxHeight\": \"500px\", \"overflowY\": \"auto\"})\n\n\n@app.callback(\n    Output(\"settings-save-status\", \"children\"),\n    Input(\"settings-save-btn\", \"n_clicks\"),\n    [State({\"type\": \"crew-type-dd\", \"chat\": ALL}, \"value\"),\n     State({\"type\": \"crew-type-dd\", \"chat\": ALL}, \"id\"),\n     State({\"type\": \"non-track-cb\", \"sender\": ALL}, \"value\"),\n     State({\"type\": \"non-track-cb\", \"sender\": ALL}, \"id\"),\n     State(\"settings-expected-hours\", \"value\"),\n     State(\"settings-svc-sidewalk\", \"value\"),\n     State(\"settings-svc-parking\", \"value\")],\n    prevent_initial_call=True,\n)\ndef save_settings(n_clicks, crew_values, crew_ids, track_values, track_ids,\n                  expected_hours, svc_sidewalk, svc_parking):\n    global SNOW_CONFIG\n    if not n_clicks:\n        raise PreventUpdate\n\n    loc_types = {}\n    if crew_ids and crew_values:\n        for cid, val in zip(crew_ids, crew_values):\n            if val:\n                loc_types[cid[\"chat\"]] = val\n\n    non_trackable = []\n    if track_ids and track_values:\n        for tid, val in zip(track_ids, track_values):\n            if val:\n                non_trackable.append(tid[\"sender\"])\n\n    SNOW_CONFIG[\"location_types\"] = loc_types\n    SNOW_CONFIG[\"non_trackable_senders\"] = non_trackable\n    if expected_hours is not None:\n        SNOW_CONFIG[\"expected_deployment_hours\"] = float(expected_hours)\n    svc = SNOW_CONFIG.get(\"expected_service_times\", {})\n    if svc_sidewalk is not None:\n        svc[\"Sidewalk\"] = int(svc_sidewalk)\n    if svc_parking is not None:\n        svc[\"Parking Lot\"] = int(svc_parking)\n    SNOW_CONFIG[\"expected_service_times\"] = svc\n\n    config_path = os.path.join(DATA_DIR, \"config\", \"snow_removal.json\")\n    try:\n        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n        with open(config_path, \"w\") as f:\n            json.dump(SNOW_CONFIG, f, indent=2)\n        return html.Span(\"Settings saved!\", style={\"color\": \"green\", \"fontWeight\": \"bold\"})\n    except Exception as e:\n        return html.Span(f\"Error saving: {e}\", style={\"color\": \"red\"})\n\n\n# â”€â”€ Crew Merge Callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    [Output(\"merge-primary-crew\", \"options\"),\n     Output(\"merge-secondary-crew\", \"options\"),\n     Output(\"merge-list-container\", \"children\")],\n    Input(\"main-tabs\", \"active_tab\"),\n    Input(\"merge-status\", \"children\"),\n)\ndef render_crew_merge_ui(active_tab, _status_trigger):\n    if active_tab != \"tab-settings\":\n        raise PreventUpdate\n\n    all_chats_full = sorted(DF_ALL[\"chat\"].unique()) if not DF_ALL.empty else []\n    options = [{\"label\": c, \"value\": c} for c in all_chats_full]\n\n    merges = SNOW_CONFIG.get(\"crew_merges\", {})\n    if merges:\n        rows = []\n        for secondary, primary in merges.items():\n            rows.append(\n                dbc.Row([\n                    dbc.Col(html.Span(f\"{secondary}\", style={\"fontSize\": \"13px\"}), md=5),\n                    dbc.Col(html.Span(f\" \\u2192 {primary}\", style={\"fontSize\": \"13px\", \"fontWeight\": \"bold\"}), md=5),\n                    dbc.Col(\n                        dbc.Button(\"\\u00d7\", id={\"type\": \"unmerge-btn\", \"crew\": secondary},\n                                   color=\"danger\", size=\"sm\", outline=True, n_clicks=0),\n                        md=2),\n                ], className=\"mb-1 align-items-center\")\n            )\n        merge_list = html.Div([\n            html.H6(\"Active Merges\", className=\"mt-2 mb-1\"),\n            html.Div(rows, style={\"maxHeight\": \"200px\", \"overflowY\": \"auto\"}),\n        ])\n    else:\n        merge_list = html.Div()\n\n    return options, options, merge_list\n\n\n@app.callback(\n    Output(\"merge-status\", \"children\", allow_duplicate=True),\n    Input(\"merge-crews-btn\", \"n_clicks\"),\n    State(\"merge-primary-crew\", \"value\"),\n    State(\"merge-secondary-crew\", \"value\"),\n    prevent_initial_call=True,\n)\ndef merge_crews(n_clicks, primary, secondary):\n    global SNOW_CONFIG, DF_ALL\n    if not n_clicks or not primary or not secondary:\n        raise PreventUpdate\n    if primary == secondary:\n        return html.Span(\"Primary and secondary crews cannot be the same.\", style={\"color\": \"red\"})\n\n    merges = SNOW_CONFIG.get(\"crew_merges\", {})\n    merges[secondary] = primary\n    SNOW_CONFIG[\"crew_merges\"] = merges\n\n    config_path = os.path.join(DATA_DIR, \"config\", \"snow_removal.json\")\n    try:\n        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n        with open(config_path, \"w\") as f:\n            json.dump(SNOW_CONFIG, f, indent=2)\n    except Exception as e:\n        return html.Span(f\"Error saving: {e}\", style={\"color\": \"red\"})\n\n    if not DF_ALL.empty:\n        DF_ALL.loc[DF_ALL[\"chat\"] == secondary, \"chat\"] = primary\n    _reload_global_data()\n\n    return html.Span(\n        f\"Merged \\\"{secondary}\\\" into \\\"{primary}\\\". Data reloaded.\",\n        style={\"color\": \"green\", \"fontWeight\": \"bold\"}\n    )\n\n\n@app.callback(\n    Output(\"merge-status\", \"children\", allow_duplicate=True),\n    Input({\"type\": \"unmerge-btn\", \"crew\": ALL}, \"n_clicks\"),\n    State({\"type\": \"unmerge-btn\", \"crew\": ALL}, \"id\"),\n    prevent_initial_call=True,\n)\ndef unmerge_crew(n_clicks_list, id_list):\n    global SNOW_CONFIG\n    if not n_clicks_list or not any(n_clicks_list):\n        raise PreventUpdate\n\n    clicked_idx = None\n    for i, n in enumerate(n_clicks_list):\n        if n and n > 0:\n            clicked_idx = i\n            break\n    if clicked_idx is None:\n        raise PreventUpdate\n\n    secondary = id_list[clicked_idx][\"crew\"]\n    merges = SNOW_CONFIG.get(\"crew_merges\", {})\n    if secondary in merges:\n        removed_primary = merges.pop(secondary)\n        SNOW_CONFIG[\"crew_merges\"] = merges\n\n        config_path = os.path.join(DATA_DIR, \"config\", \"snow_removal.json\")\n        try:\n            with open(config_path, \"w\") as f:\n                json.dump(SNOW_CONFIG, f, indent=2)\n        except Exception:\n            pass\n\n        _reload_global_data()\n        return html.Span(\n            f\"Removed merge: \\\"{secondary}\\\" is now separate from \\\"{removed_primary}\\\". Data reloaded.\",\n            style={\"color\": \"green\", \"fontWeight\": \"bold\"}\n        )\n    raise PreventUpdate\n\n\n# â”€â”€ Location CSV upload callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    [Output(\"locations-registry-table\", \"data\"),\n     Output(\"locations-upload-status\", \"children\")],\n    Input(\"upload-locations-csv\", \"contents\"),\n    State(\"upload-locations-csv\", \"filename\"),\n    prevent_initial_call=True,\n)\ndef upload_locations_csv(contents, filename):\n    global SNOW_CONFIG\n    if contents is None:\n        raise PreventUpdate\n    try:\n        import io\n        content_type, content_string = contents.split(\",\")\n        decoded = base64.b64decode(content_string).decode(\"utf-8\")\n        csv_df = pd.read_csv(io.StringIO(decoded))\n        csv_df.columns = [c.strip() for c in csv_df.columns]\n\n        is_routes_format = \"Building Name\" in csv_df.columns\n        if is_routes_format:\n            name_col = \"Building Name\"\n            addr_col = \"Billing Street\"\n        else:\n            name_col = \"location_name\" if \"location_name\" in csv_df.columns else None\n            addr_col = \"address\" if \"address\" in csv_df.columns else None\n\n        if name_col is None or name_col not in csv_df.columns:\n            return no_update, html.Span(\n                \"CSV must have 'Building Name' or 'location_name' column.\", style={\"color\": \"red\"})\n\n        known_locations = sorted(\n            DF_ALL[DF_ALL[\"location\"].str.len() > 0][\"location\"].unique()\n        ) if not DF_ALL.empty else []\n        registry = []\n        for _, row in csv_df.iterrows():\n            name = str(row.get(name_col, \"\")).strip()\n            if not name:\n                continue\n            address = str(row.get(addr_col, \"\")).strip() if addr_col and addr_col in csv_df.columns else \"\"\n            if address.lower() == \"nan\":\n                address = \"\"\n\n            crew_sw = \"\"\n            crew_pl = \"\"\n            if is_routes_format:\n                crew_sw = str(row.get(\"Crew Sidewalk\", \"\")).strip()\n                crew_pl = str(row.get(\"Crew Parking Lot\", \"\")).strip()\n            else:\n                crew_sw = str(row.get(\"crew_sidewalk\", \"\")).strip() if \"crew_sidewalk\" in csv_df.columns else \"\"\n                crew_pl = str(row.get(\"crew_parking_lot\", \"\")).strip() if \"crew_parking_lot\" in csv_df.columns else \"\"\n            if crew_sw.lower() == \"nan\":\n                crew_sw = \"\"\n            if crew_pl.lower() == \"nan\":\n                crew_pl = \"\"\n\n            lat = row.get(\"lat\", None) if \"lat\" in csv_df.columns else None\n            lon = row.get(\"lon\", None) if \"lon\" in csv_df.columns else None\n            if pd.isna(lat) if lat is not None else True:\n                lat = None\n            if pd.isna(lon) if lon is not None else True:\n                lon = None\n\n            matched, score = _fuzzy_match_location(name, known_locations)\n            if not matched and address:\n                matched, score = _fuzzy_match_location(address, known_locations)\n            match_display = f\"{matched} ({score}%)\" if matched else \"\"\n            registry.append({\n                \"location_name\": name,\n                \"address\": address,\n                \"lat\": float(lat) if lat is not None else \"\",\n                \"lon\": float(lon) if lon is not None else \"\",\n                \"matched_chat_location\": match_display,\n                \"crew_sidewalk\": crew_sw,\n                \"crew_parking_lot\": crew_pl,\n                \"_matched_raw\": matched,\n                \"_match_score\": score,\n            })\n        registry, auto_count = _auto_assign_crews(registry, DF_ALL)\n        SNOW_CONFIG[\"location_registry\"] = registry\n        table_data = [\n            {k: r.get(k, \"\") for k in [\"location_name\", \"address\", \"matched_chat_location\", \"crew_sidewalk\", \"crew_parking_lot\"]}\n            for r in registry\n        ]\n        matched_count = sum(1 for r in registry if r.get(\"_matched_raw\"))\n        parts = [f\"Loaded {len(registry)} locations from {filename}.\",\n                 f\"{matched_count} matched to chat data.\"]\n        if auto_count:\n            parts.append(f\"Auto-assigned crews to {auto_count} locations.\")\n        status = html.Span(\n            \" \".join(parts),\n            style={\"color\": \"green\", \"fontWeight\": \"bold\"})\n        return table_data, status\n    except Exception as e:\n        return no_update, html.Span(f\"Error parsing CSV: {e}\", style={\"color\": \"red\"})\n\n\n@app.callback(\n    [Output(\"locations-registry-table\", \"data\", allow_duplicate=True),\n     Output(\"locations-auto-assign-status\", \"children\")],\n    Input(\"locations-auto-assign-btn\", \"n_clicks\"),\n    State(\"locations-registry-table\", \"data\"),\n    prevent_initial_call=True,\n)\ndef auto_assign_crews(n_clicks, table_data):\n    global SNOW_CONFIG\n    if not n_clicks:\n        raise PreventUpdate\n    try:\n        registry = SNOW_CONFIG.get(\"location_registry\", [])\n        if not registry and table_data:\n            registry = []\n            for row in table_data:\n                entry = dict(row)\n                mcl = entry.get(\"matched_chat_location\", \"\")\n                if mcl and \"(\" in mcl:\n                    entry[\"_matched_raw\"] = mcl.split(\"(\")[0].strip()\n                registry.append(entry)\n\n        if not registry:\n            return no_update, html.Span(\"No locations loaded. Upload a CSV first.\",\n                                         style={\"color\": \"orange\"})\n\n        registry, assigned = _auto_assign_crews(registry, DF_ALL)\n        SNOW_CONFIG[\"location_registry\"] = registry\n        updated_table = [\n            {k: r.get(k, \"\") for k in [\"location_name\", \"address\", \"matched_chat_location\", \"crew_sidewalk\", \"crew_parking_lot\"]}\n            for r in registry\n        ]\n        status = html.Span(\n            f\"Auto-assigned crews to {assigned} locations. Review and edit as needed, then Save.\",\n            style={\"color\": \"green\", \"fontWeight\": \"bold\"})\n        return updated_table, status\n    except Exception as e:\n        return no_update, html.Span(f\"Error: {e}\", style={\"color\": \"red\"})\n\n\n@app.callback(\n    Output(\"locations-save-status\", \"children\"),\n    Input(\"locations-save-btn\", \"n_clicks\"),\n    State(\"locations-registry-table\", \"data\"),\n    prevent_initial_call=True,\n)\ndef save_locations_registry(n_clicks, table_data):\n    global SNOW_CONFIG\n    if not n_clicks:\n        raise PreventUpdate\n    try:\n        if table_data:\n            existing = {r.get(\"location_name\", \"\"): r for r in SNOW_CONFIG.get(\"location_registry\", [])}\n            clean_registry = []\n            for row in table_data:\n                name = row.get(\"location_name\", \"\")\n                prev = existing.get(name, {})\n                entry = {\n                    \"location_name\": name,\n                    \"address\": row.get(\"address\", prev.get(\"address\", \"\")),\n                    \"lat\": prev.get(\"lat\", \"\"),\n                    \"lon\": prev.get(\"lon\", \"\"),\n                    \"matched_chat_location\": row.get(\"matched_chat_location\", prev.get(\"matched_chat_location\", \"\")),\n                    \"crew_sidewalk\": row.get(\"crew_sidewalk\", \"\"),\n                    \"crew_parking_lot\": row.get(\"crew_parking_lot\", \"\"),\n                    \"_matched_raw\": prev.get(\"_matched_raw\", \"\"),\n                    \"_match_score\": prev.get(\"_match_score\", 0),\n                }\n                if not entry[\"_matched_raw\"]:\n                    mcl = entry[\"matched_chat_location\"]\n                    if mcl and \"(\" in mcl:\n                        entry[\"_matched_raw\"] = mcl.split(\"(\")[0].strip()\n                clean_registry.append(entry)\n            SNOW_CONFIG[\"location_registry\"] = clean_registry\n        config_path = os.path.join(DATA_DIR, \"config\", \"snow_removal.json\")\n        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n        with open(config_path, \"w\") as f:\n            json.dump(SNOW_CONFIG, f, indent=2)\n        return html.Span(\"Locations saved!\", style={\"color\": \"green\", \"fontWeight\": \"bold\"})\n    except Exception as e:\n        return html.Span(f\"Error saving: {e}\", style={\"color\": \"red\"})\n\n\n# â”€â”€ DC Boundary helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _load_dc_boundary():\n    cache_path = os.path.join(DATA_DIR, \"config\", \"dc_boundary.json\")\n    if os.path.exists(cache_path):\n        try:\n            with open(cache_path, encoding=\"utf-8\") as f:\n                return json.load(f)\n        except (json.JSONDecodeError, OSError):\n            pass\n    try:\n        import urllib.request\n        url = (\"https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/\"\n               \"Administrative_Other_Boundaries_WebMercator/MapServer/10/\"\n               \"query?where=1%3D1&outFields=*&outSR=4326&f=json\")\n        req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        with urllib.request.urlopen(req, timeout=15) as resp:\n            data = json.loads(resp.read().decode(\"utf-8\"))\n        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n        with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f)\n        return data\n    except Exception as e:\n        print(f\"[map] Failed to load DC boundary: {e}\")\n        return None\n\n\n# â”€â”€ Deployment Breakdown callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"dep-breakdown-table-container\", \"children\"),\n    Input(\"dep-breakdown-select\", \"value\"),\n)\ndef update_deployment_breakdown(selected_dep):\n    if not selected_dep:\n        raise PreventUpdate\n    try:\n        dep_df = DF_ALL[(DF_ALL[\"deployment\"] == selected_dep) & (DF_ALL[\"noise_type\"] == \"clean\")]\n        non_trackable = set(SNOW_CONFIG.get(\"non_trackable_senders\", []))\n        if non_trackable:\n            dep_df = dep_df[~dep_df[\"sender_resolved\"].isin(non_trackable)]\n\n        if dep_df.empty:\n            return html.P(\"No data for this deployment.\", className=\"text-muted\")\n\n        locations = dep_df[dep_df[\"location\"].str.len() > 0][\"location\"].unique()\n        \n        all_crews = sorted(set(\n            crew for chat in dep_df[\"chat\"].unique()\n            for crew, _, _ in [_extract_crew_from_chat(chat)]\n            if crew\n        ))\n\n        registry = SNOW_CONFIG.get(\"location_registry\", [])\n        registry_map = {}\n        for r in registry:\n            raw = r.get(\"_matched_raw\", \"\")\n            if raw:\n                registry_map[_normalize_location(raw)] = r\n\n        rows = []\n        for loc in sorted(locations):\n            loc_msgs = dep_df[dep_df[\"location\"] == loc]\n            crews_in_loc = loc_msgs[\"chat\"].unique()\n            detected_crews = []\n            detected_type = \"Unknown\"\n            for ch in crews_in_loc:\n                crew, is_sw, is_pl = _extract_crew_from_chat(ch)\n                if crew:\n                    detected_crews.append(crew)\n                    if is_pl:\n                        detected_type = \"Parking Lot\"\n                    elif is_sw:\n                        detected_type = \"Sidewalk\"\n\n            reg = registry_map.get(_normalize_location(loc), {})\n            assigned_sw = reg.get(\"crew_sidewalk\", \"\")\n            assigned_pl = reg.get(\"crew_parking_lot\", \"\")\n\n            visits = len(loc_msgs)\n            senders = loc_msgs[\"sender_resolved\"].nunique()\n\n            rows.append({\n                \"location\": loc,\n                \"type\": detected_type,\n                \"visits\": visits,\n                \"senders\": senders,\n                \"detected_crew\": \", \".join(detected_crews) if detected_crews else \"Unknown\",\n                \"crew_sidewalk\": assigned_sw,\n                \"crew_parking_lot\": assigned_pl,\n            })\n\n        if not rows:\n            return html.P(\"No locations found for this deployment.\", className=\"text-muted\")\n\n        all_crew_options = sorted(set(\n            crew for chat in ALL_CHATS\n            for crew, _, _ in [_extract_crew_from_chat(chat)]\n            if crew\n        ))\n\n        table = dash_table.DataTable(\n            id=\"dep-breakdown-table\",\n            columns=[\n                {\"name\": \"Location\", \"id\": \"location\", \"editable\": False},\n                {\"name\": \"Type\", \"id\": \"type\", \"editable\": False},\n                {\"name\": \"Visits\", \"id\": \"visits\", \"type\": \"numeric\", \"editable\": False},\n                {\"name\": \"Reporters\", \"id\": \"senders\", \"type\": \"numeric\", \"editable\": False},\n                {\"name\": \"Detected Crew\", \"id\": \"detected_crew\", \"editable\": False},\n                {\"name\": \"Assigned SW Crew\", \"id\": \"crew_sidewalk\",\n                 \"editable\": True, \"presentation\": \"dropdown\"},\n                {\"name\": \"Assigned PL Crew\", \"id\": \"crew_parking_lot\",\n                 \"editable\": True, \"presentation\": \"dropdown\"},\n            ],\n            data=rows,\n            dropdown={\n                \"crew_sidewalk\": {\n                    \"options\": [{\"label\": \"\", \"value\": \"\"}] + \n                               [{\"label\": c, \"value\": c} for c in all_crew_options],\n                },\n                \"crew_parking_lot\": {\n                    \"options\": [{\"label\": \"\", \"value\": \"\"}] + \n                               [{\"label\": c, \"value\": c} for c in all_crew_options],\n                },\n            },\n            style_table={\"overflowX\": \"auto\"},\n            style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"12px\",\n                        \"minWidth\": \"80px\", \"maxWidth\": \"250px\", \"overflow\": \"hidden\",\n                        \"textOverflow\": \"ellipsis\"},\n            style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f1f3f5\", \"fontSize\": \"12px\"},\n            style_data_conditional=[\n                {\"if\": {\"column_id\": \"crew_sidewalk\"}, \"backgroundColor\": \"#f0fff0\"},\n                {\"if\": {\"column_id\": \"crew_parking_lot\"}, \"backgroundColor\": \"#fff0f0\"},\n            ],\n            page_size=30,\n            sort_action=\"native\",\n            filter_action=\"native\",\n        )\n\n        summary = html.Div([\n            html.P([\n                html.Strong(f\"{len(rows)} locations\"),\n                f\" serviced by \",\n                html.Strong(f\"{len(all_crews)} crews\"),\n                f\" | {sum(r['visits'] for r in rows)} total visits\",\n            ], className=\"mb-2\", style={\"fontSize\": \"13px\"}),\n        ])\n\n        return html.Div([summary, table])\n\n    except Exception as e:\n        print(f\"[Deployment Breakdown] Error: {e}\")\n        import traceback; traceback.print_exc()\n        return html.P(\"Error loading deployment breakdown.\", className=\"text-muted\")\n\n\n@app.callback(\n    Output(\"dep-save-crews-status\", \"children\"),\n    Input(\"dep-save-crews-btn\", \"n_clicks\"),\n    State(\"dep-breakdown-table\", \"data\"),\n    prevent_initial_call=True,\n)\ndef save_deployment_crew_assignments(n_clicks, table_data):\n    global SNOW_CONFIG\n    if not n_clicks or not table_data:\n        raise PreventUpdate\n    try:\n        registry = SNOW_CONFIG.get(\"location_registry\", [])\n        registry_by_norm = {}\n        for r in registry:\n            raw = r.get(\"_matched_raw\", \"\")\n            if raw:\n                registry_by_norm[_normalize_location(raw)] = r\n            mcl = r.get(\"matched_chat_location\", \"\")\n            if mcl and \"(\" in mcl:\n                mcl_raw = mcl.split(\"(\")[0].strip()\n                if mcl_raw:\n                    registry_by_norm.setdefault(_normalize_location(mcl_raw), r)\n            loc_name = r.get(\"location_name\", \"\")\n            if loc_name:\n                registry_by_norm.setdefault(_normalize_location(loc_name), r)\n\n        updated = 0\n        for row in table_data:\n            loc = row.get(\"location\", \"\")\n            norm = _normalize_location(loc)\n            if norm in registry_by_norm:\n                entry = registry_by_norm[norm]\n                new_sw = row.get(\"crew_sidewalk\", \"\")\n                new_pl = row.get(\"crew_parking_lot\", \"\")\n                if entry.get(\"crew_sidewalk\", \"\") != new_sw or entry.get(\"crew_parking_lot\", \"\") != new_pl:\n                    entry[\"crew_sidewalk\"] = new_sw\n                    entry[\"crew_parking_lot\"] = new_pl\n                    updated += 1\n\n        config_path = os.path.join(DATA_DIR, \"config\", \"snow_removal.json\")\n        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(SNOW_CONFIG, f, indent=2, ensure_ascii=False)\n        return html.Span(f\"Saved! {updated} assignments updated.\", style={\"color\": \"green\"})\n    except Exception as e:\n        return html.Span(f\"Error: {e}\", style={\"color\": \"red\"})\n\n\n# â”€â”€ Map callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"chart-service-map\", \"figure\"),\n    Input(\"main-tabs\", \"active_tab\"),\n    Input(\"map-deployment-filter\", \"value\"),\n    prevent_initial_call=True,\n)\ndef render_service_map(active_tab, dep_filter):\n    if active_tab != \"tab-map\":\n        raise PreventUpdate\n    try:\n        fig = go.Figure()\n\n        boundary_data = _load_dc_boundary()\n        if boundary_data and \"features\" in boundary_data:\n            for feature in boundary_data[\"features\"]:\n                geom = feature.get(\"geometry\", {})\n                rings = geom.get(\"rings\", [])\n                for ring in rings:\n                    lats = [pt[1] for pt in ring]\n                    lons = [pt[0] for pt in ring]\n                    lats.append(lats[0])\n                    lons.append(lons[0])\n                    fig.add_trace(go.Scattermap(\n                        lat=lats,\n                        lon=lons,\n                        mode=\"lines\",\n                        line=dict(width=2, color=\"#1f77b4\"),\n                        name=\"DC Boundary\",\n                        showlegend=True,\n                        hoverinfo=\"skip\",\n                    ))\n\n        registry = SNOW_CONFIG.get(\"location_registry\", [])\n\n        if dep_filter and dep_filter != \"ALL\":\n            dep_df = DF_ALL[(DF_ALL[\"deployment\"] == dep_filter) & (DF_ALL[\"noise_type\"] == \"clean\")]\n            non_trackable = set(SNOW_CONFIG.get(\"non_trackable_senders\", []))\n            if non_trackable:\n                dep_df = dep_df[~dep_df[\"sender_resolved\"].isin(non_trackable)]\n            active_locations = set(dep_df[dep_df[\"location\"].str.len() > 0][\"location\"].unique())\n            loc_counts = dep_df[dep_df[\"location\"].str.len() > 0][\"location\"].value_counts().to_dict()\n        else:\n            active_locations = None\n            loc_counts = {}\n            if not DF_ALL.empty:\n                counts = DF_ALL[DF_ALL[\"location\"].str.len() > 0][\"location\"].value_counts()\n                loc_counts = counts.to_dict()\n\n        locs_with_coords = [\n            r for r in registry\n            if r.get(\"lat\") and r.get(\"lon\")\n            and r[\"lat\"] != \"\" and r[\"lon\"] != \"\"\n        ]\n\n        if active_locations is not None:\n            filtered_locs = []\n            for r in locs_with_coords:\n                m = r.get(\"_matched_raw\", \"\")\n                if not m:\n                    mc = r.get(\"matched_chat_location\", \"\")\n                    if mc and \"(\" in mc:\n                        m = mc.split(\"(\")[0].strip()\n                norm_m = _normalize_location(m) if m else \"\"\n                if norm_m and any(_normalize_location(al) == norm_m for al in active_locations):\n                    filtered_locs.append(r)\n                elif not norm_m:\n                    loc_name_norm = _normalize_location(r.get(\"location_name\", \"\"))\n                    for al in active_locations:\n                        if _normalize_location(al) == loc_name_norm:\n                            filtered_locs.append(r)\n                            break\n            locs_with_coords = filtered_locs\n\n        if locs_with_coords:\n            BOLD_COLORS = [\n                \"#e6194b\", \"#3cb44b\", \"#4363d8\", \"#f58231\", \"#911eb4\",\n                \"#42d4f4\", \"#f032e6\", \"#bfef45\", \"#fabed4\", \"#469990\",\n                \"#dcbeff\", \"#9A6324\", \"#800000\", \"#aaffc3\", \"#808000\",\n                \"#000075\", \"#ff4500\", \"#00ced1\",\n            ]\n\n            all_individual_crews = set()\n            for r in locs_with_coords:\n                for field in [\"crew_sidewalk\", \"crew_parking_lot\"]:\n                    val = r.get(field, \"\")\n                    if val:\n                        for c in val.split(\",\"):\n                            c = c.strip()\n                            if c:\n                                all_individual_crews.add(c)\n            all_individual_crews = sorted(all_individual_crews)\n            if not all_individual_crews:\n                all_individual_crews = [\"Unassigned\"]\n            indiv_color = {c: BOLD_COLORS[i % len(BOLD_COLORS)] for i, c in enumerate(all_individual_crews)}\n            indiv_color[\"Unassigned\"] = \"#888888\"\n\n            def _primary_crew(r):\n                sw = r.get(\"crew_sidewalk\", \"\")\n                pl = r.get(\"crew_parking_lot\", \"\")\n                val = sw if sw else (pl if pl else \"\")\n                if not val:\n                    return \"Unassigned\"\n                return val.split(\",\")[0].strip()\n\n            crew_groups = {}\n            for r in locs_with_coords:\n                pc = _primary_crew(r)\n                crew_groups.setdefault(pc, []).append(r)\n\n            for crew in sorted(crew_groups.keys()):\n                crew_locs = crew_groups[crew]\n                lats = [float(r[\"lat\"]) for r in crew_locs]\n                lons = [float(r[\"lon\"]) for r in crew_locs]\n                sizes = []\n                hovers = []\n                for r in crew_locs:\n                    m = r.get(\"_matched_raw\", \"\")\n                    if not m:\n                        mc = r.get(\"matched_chat_location\", \"\")\n                        if mc and \"(\" in mc:\n                            m = mc.split(\"(\")[0].strip()\n                    cnt = loc_counts.get(m, 1)\n                    sizes.append(max(10, min(30, 8 + cnt * 2)))\n                    sw = r.get(\"crew_sidewalk\", \"\")\n                    pl = r.get(\"crew_parking_lot\", \"\")\n                    hover_parts = [f\"<b>{r.get('location_name', '')}</b>\"]\n                    hover_parts.append(f\"Visits: {loc_counts.get(m, 0)}\")\n                    if sw:\n                        hover_parts.append(f\"Sidewalk: {sw}\")\n                    if pl:\n                        hover_parts.append(f\"Parking: {pl}\")\n                    if not sw and not pl:\n                        hover_parts.append(\"Crew: Unassigned\")\n                    hovers.append(\"<br>\".join(hover_parts))\n                fig.add_trace(go.Scattermap(\n                    lat=lats,\n                    lon=lons,\n                    mode=\"markers\",\n                    marker=dict(\n                        size=sizes,\n                        color=indiv_color.get(crew, \"#888888\"),\n                        opacity=0.95,\n                    ),\n                    text=hovers,\n                    hoverinfo=\"text\",\n                    name=crew,\n                    showlegend=True,\n                ))\n\n        title = \"DC Service Map\"\n        if dep_filter and dep_filter != \"ALL\":\n            title += f\" â€” {dep_filter}\"\n            n_locs = len(locs_with_coords) if locs_with_coords else 0\n            title += f\" ({n_locs} sites)\"\n\n        fig.update_layout(\n            map=dict(\n                style=\"open-street-map\",\n                center=dict(lat=38.9072, lon=-77.0369),\n                zoom=11,\n            ),\n            margin=dict(l=0, r=0, t=30, b=0),\n            height=700,\n            title=title,\n            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n        )\n        return fig\n    except Exception as e:\n        print(f\"[map] Error rendering map: {e}\")\n        return _empty_fig(\"DC Service Map\")\n\n\n# â”€â”€ Helper: empty figure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _empty_fig(title):\n    fig = go.Figure()\n    fig.add_annotation(\n        text=\"No data for current filters\",\n        xref=\"paper\", yref=\"paper\", x=0.5, y=0.5,\n        showarrow=False, font=dict(size=16, color=\"gray\"),\n    )\n    fig.update_layout(\n        title=title,\n        margin=dict(l=10, r=10, t=40, b=10),\n        height=400,\n    )\n    return fig\n\n\n# â”€â”€ Finances callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"fin-kpi-revenue\", \"children\"),\n    Output(\"fin-kpi-costs\", \"children\"),\n    Output(\"fin-kpi-profit\", \"children\"),\n    Output(\"fin-kpi-profit\", \"style\"),\n    Output(\"fin-kpi-margin\", \"children\"),\n    Output(\"fin-kpi-margin\", \"style\"),\n    Output(\"fin-kpi-sites\", \"children\"),\n    Output(\"fin-kpi-salt\", \"children\"),\n    Output(\"fin-kpi-cost-hr\", \"children\"),\n    Output(\"fin-kpi-rev-dep\", \"children\"),\n    Output(\"fin-deployment-table-container\", \"children\"),\n    Output(\"fin-crew-table-container\", \"children\"),\n    Output(\"fin-chart-revenue\", \"figure\"),\n    Input(\"main-tabs\", \"active_tab\"),\n    Input(\"fin-recalc-btn\", \"n_clicks\"),\n    State(\"fin-labor-sw\", \"value\"),\n    State(\"fin-labor-pl\", \"value\"),\n    State(\"fin-machine-rate\", \"value\"),\n    State(\"fin-salt-cost\", \"value\"),\n    State(\"fin-salt-sw\", \"value\"),\n    State(\"fin-salt-pl\", \"value\"),\n    State(\"fin-workers-sw\", \"value\"),\n    State(\"fin-workers-pl\", \"value\"),\n    State(\"fin-overhead\", \"value\"),\n    State(\"fin-default-tier\", \"value\"),\n)\ndef update_finances(active_tab, n_clicks, labor_sw, labor_pl, machine_rate,\n                    salt_cost, salt_sw, salt_pl, workers_sw, workers_pl,\n                    overhead, default_tier):\n    if active_tab != \"tab-finances\":\n        raise PreventUpdate\n    try:\n        temp_config = dict(SNOW_CONFIG)\n        temp_config[\"finance_config\"] = {\n            \"labor_rate_sidewalk\": labor_sw or 25.0,\n            \"labor_rate_parking\": labor_pl or 30.0,\n            \"machine_hourly_rate\": machine_rate or 75.0,\n            \"salt_cost_per_lb\": salt_cost or 0.15,\n            \"salt_lbs_per_site_sidewalk\": salt_sw or 50.0,\n            \"salt_lbs_per_site_parking\": salt_pl or 200.0,\n            \"workers_sidewalk\": workers_sw or 3,\n            \"workers_parking\": min(workers_pl or 2, 2),\n            \"overhead_pct\": overhead or 10.0,\n            \"default_snow_tier\": default_tier or \"price_under_6in\",\n            \"deployment_snow_tiers\": SNOW_CONFIG.get(\"finance_config\", {}).get(\"deployment_snow_tiers\", {}),\n        }\n\n        fin = _compute_financials(DF_ALL, ALL_DEPLOYMENTS_LIST, temp_config, PRICING_INDEX)\n\n        rev_str = f\"${fin['total_revenue']:,.0f}\"\n        cost_str = f\"${fin['total_costs']:,.0f}\"\n        profit_str = f\"${fin['total_profit']:,.0f}\"\n        profit_color = \"#28a745\" if fin['total_profit'] >= 0 else \"#dc3545\"\n        margin_str = f\"{fin['profit_margin']:.1f}%\"\n        margin_color = \"#28a745\" if fin['profit_margin'] >= 0 else \"#dc3545\"\n        sites_str = f\"{fin['sites_serviced']} / {fin['sites_contracted']}\"\n        salt_str = f\"{fin['total_salt_lbs']:,.0f}\"\n        cost_hr_str = f\"${fin['cost_per_hour']:,.0f}\"\n        rev_dep_str = f\"${fin['revenue_per_deployment']:,.0f}\"\n\n        dep_table = dash_table.DataTable(\n            columns=[\n                {\"name\": \"Deployment\", \"id\": \"deployment\"},\n                {\"name\": \"Type\", \"id\": \"type\"},\n                {\"name\": \"Tier\", \"id\": \"snow_tier\"},\n                {\"name\": \"Source\", \"id\": \"source\"},\n                {\"name\": \"Routes\", \"id\": \"total_sites\", \"type\": \"numeric\"},\n                {\"name\": \"SW\", \"id\": \"sw_routes\", \"type\": \"numeric\"},\n                {\"name\": \"PL\", \"id\": \"pl_routes\", \"type\": \"numeric\"},\n                {\"name\": \"Crews\", \"id\": \"crews\", \"type\": \"numeric\"},\n                {\"name\": \"Hours\", \"id\": \"hours\", \"type\": \"numeric\"},\n                {\"name\": \"Revenue\", \"id\": \"revenue\", \"type\": \"numeric\"},\n                {\"name\": \"Labor\", \"id\": \"labor_cost\", \"type\": \"numeric\"},\n                {\"name\": \"Machine\", \"id\": \"machine_cost\", \"type\": \"numeric\"},\n                {\"name\": \"Salt\", \"id\": \"material_cost\", \"type\": \"numeric\"},\n                {\"name\": \"Overhead\", \"id\": \"overhead\", \"type\": \"numeric\"},\n                {\"name\": \"Total Cost\", \"id\": \"total_cost\", \"type\": \"numeric\"},\n                {\"name\": \"Profit\", \"id\": \"profit\", \"type\": \"numeric\"},\n                {\"name\": \"Margin %\", \"id\": \"margin\", \"type\": \"numeric\"},\n                {\"name\": \"Salt (lbs)\", \"id\": \"salt_lbs\", \"type\": \"numeric\"},\n            ],\n            data=fin[\"deployment_financials\"],\n            style_table={\"overflowX\": \"auto\"},\n            style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"12px\"},\n            style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f1f3f5\", \"fontSize\": \"12px\"},\n            style_data_conditional=[\n                {\"if\": {\"filter_query\": \"{profit} < 0\"}, \"color\": \"#dc3545\", \"fontWeight\": \"bold\"},\n                {\"if\": {\"filter_query\": \"{margin} >= 20\"}, \"color\": \"#28a745\"},\n            ],\n            page_size=20,\n        ) if fin[\"deployment_financials\"] else html.P(\"No deployment financial data available.\", className=\"text-muted\")\n\n        crew_table = dash_table.DataTable(\n            columns=[\n                {\"name\": \"Crew\", \"id\": \"crew\"},\n                {\"name\": \"Type\", \"id\": \"type\"},\n                {\"name\": \"Workers\", \"id\": \"workers\", \"type\": \"numeric\"},\n                {\"name\": \"Deps\", \"id\": \"deployments\", \"type\": \"numeric\"},\n                {\"name\": \"Sites\", \"id\": \"sites\", \"type\": \"numeric\"},\n                {\"name\": \"Hours\", \"id\": \"hours\", \"type\": \"numeric\"},\n                {\"name\": \"Revenue\", \"id\": \"revenue\", \"type\": \"numeric\"},\n                {\"name\": \"Labor\", \"id\": \"labor_cost\", \"type\": \"numeric\"},\n                {\"name\": \"Machine\", \"id\": \"machine_cost\", \"type\": \"numeric\"},\n                {\"name\": \"Total Cost\", \"id\": \"total_cost\", \"type\": \"numeric\"},\n                {\"name\": \"Profit\", \"id\": \"profit\", \"type\": \"numeric\"},\n                {\"name\": \"$/Hour\", \"id\": \"rate_per_hour\", \"type\": \"numeric\"},\n            ],\n            data=fin[\"crew_financials\"],\n            style_table={\"overflowX\": \"auto\"},\n            style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"12px\"},\n            style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f1f3f5\", \"fontSize\": \"12px\"},\n            style_data_conditional=[\n                {\"if\": {\"filter_query\": \"{profit} < 0\"}, \"color\": \"#dc3545\"},\n            ],\n            page_size=20,\n        ) if fin[\"crew_financials\"] else html.P(\"No crew financial data available.\", className=\"text-muted\")\n\n        if fin[\"deployment_financials\"]:\n            dep_df_chart = pd.DataFrame(fin[\"deployment_financials\"])\n            fig = go.Figure()\n            fig.add_trace(go.Bar(name=\"Revenue\", x=dep_df_chart[\"deployment\"], y=dep_df_chart[\"revenue\"],\n                                marker_color=\"#28a745\"))\n            fig.add_trace(go.Bar(name=\"Labor\", x=dep_df_chart[\"deployment\"], y=dep_df_chart[\"labor_cost\"],\n                                marker_color=\"#fd7e14\"))\n            fig.add_trace(go.Bar(name=\"Machine\", x=dep_df_chart[\"deployment\"], y=dep_df_chart[\"machine_cost\"],\n                                marker_color=\"#6f42c1\"))\n            fig.add_trace(go.Bar(name=\"Salt\", x=dep_df_chart[\"deployment\"], y=dep_df_chart[\"material_cost\"],\n                                marker_color=\"#20c997\"))\n            fig.add_trace(go.Bar(name=\"Profit\", x=dep_df_chart[\"deployment\"], y=dep_df_chart[\"profit\"],\n                                marker_color=\"#17a2b8\"))\n            fig.update_layout(barmode=\"group\", template=\"plotly_white\",\n                            margin=dict(l=40, r=20, t=30, b=40),\n                            legend=dict(orientation=\"h\", y=1.1),\n                            yaxis_title=\"Amount ($)\")\n        else:\n            fig = go.Figure()\n            fig.update_layout(template=\"plotly_white\",\n                            annotations=[{\"text\": \"No financial data\", \"showarrow\": False,\n                                         \"font\": {\"size\": 16, \"color\": \"gray\"}}])\n\n        return (rev_str, cost_str, profit_str, {\"color\": profit_color},\n                margin_str, {\"color\": margin_color},\n                sites_str, salt_str, cost_hr_str, rev_dep_str,\n                dep_table, crew_table, fig)\n    except Exception as e:\n        print(f\"[Finances] Error: {e}\")\n        import traceback; traceback.print_exc()\n        empty_fig = go.Figure()\n        empty_fig.update_layout(template=\"plotly_white\")\n        return (\"$0\", \"$0\", \"$0\", {}, \"0%\", {}, \"0 / 0\", \"0\", \"$0\", \"$0\",\n                html.P(\"Error computing financials.\", className=\"text-muted\"),\n                html.P(\"Error computing financials.\", className=\"text-muted\"),\n                empty_fig)\n\n\n@app.callback(\n    Output(\"fin-save-status\", \"children\"),\n    Input(\"fin-save-btn\", \"n_clicks\"),\n    State(\"fin-labor-sw\", \"value\"),\n    State(\"fin-labor-pl\", \"value\"),\n    State(\"fin-machine-rate\", \"value\"),\n    State(\"fin-salt-cost\", \"value\"),\n    State(\"fin-salt-sw\", \"value\"),\n    State(\"fin-salt-pl\", \"value\"),\n    State(\"fin-workers-sw\", \"value\"),\n    State(\"fin-workers-pl\", \"value\"),\n    State(\"fin-overhead\", \"value\"),\n    State(\"fin-default-tier\", \"value\"),\n    prevent_initial_call=True,\n)\ndef save_finance_config(n_clicks, labor_sw, labor_pl, machine_rate,\n                        salt_cost, salt_sw, salt_pl, workers_sw, workers_pl,\n                        overhead, default_tier):\n    if not n_clicks:\n        raise PreventUpdate\n    try:\n        config_path = os.path.join(DATA_DIR, \"config\", \"snow_removal.json\")\n        config = load_config(config_path)\n        config[\"finance_config\"] = {\n            \"labor_rate_sidewalk\": labor_sw or 25.0,\n            \"labor_rate_parking\": labor_pl or 30.0,\n            \"machine_hourly_rate\": machine_rate or 75.0,\n            \"salt_cost_per_lb\": salt_cost or 0.15,\n            \"salt_lbs_per_site_sidewalk\": salt_sw or 50.0,\n            \"salt_lbs_per_site_parking\": salt_pl or 200.0,\n            \"workers_sidewalk\": workers_sw or 3,\n            \"workers_parking\": min(workers_pl or 2, 2),\n            \"overhead_pct\": overhead or 10.0,\n            \"default_snow_tier\": default_tier or \"price_under_6in\",\n            \"deployment_snow_tiers\": config.get(\"finance_config\", {}).get(\"deployment_snow_tiers\", {}),\n        }\n        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(config, f, indent=2, ensure_ascii=False)\n        global SNOW_CONFIG\n        SNOW_CONFIG = config\n        return html.Span(\"Saved!\", style={\"color\": \"green\"})\n    except Exception as e:\n        return html.Span(f\"Error: {e}\", style={\"color\": \"red\"})\n\n\n# â”€â”€ Invoice List on Settings Load â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"invoice-list-container\", \"children\"),\n    Input(\"main-tabs\", \"active_tab\"),\n)\ndef show_invoice_list_on_load(active_tab):\n    if active_tab != \"tab-settings\":\n        raise PreventUpdate\n    return _render_invoice_list(SNOW_CONFIG.get(\"invoices\", []))\n\n\n# â”€â”€ Invoice Upload Callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    [Output(\"invoice-upload-status\", \"children\"),\n     Output(\"invoice-preview-container\", \"children\"),\n     Output(\"invoice-list-container\", \"children\", allow_duplicate=True)],\n    Input(\"upload-invoice\", \"contents\"),\n    State(\"upload-invoice\", \"filename\"),\n    prevent_initial_call=True,\n)\ndef handle_invoice_upload(contents_list, filenames_list):\n    global SNOW_CONFIG\n    if not contents_list:\n        raise PreventUpdate\n    if isinstance(contents_list, str):\n        contents_list = [contents_list]\n        filenames_list = [filenames_list]\n\n    config_path = os.path.join(DATA_DIR, \"config\", \"snow_removal.json\")\n    invoices_registry = SNOW_CONFIG.get(\"invoices\", [])\n\n    all_parsed = []\n    errors = []\n    for content, fname in zip(contents_list, filenames_list):\n        try:\n            _, content_string = content.split(\",\")\n            decoded = base64.b64decode(content_string)\n            parsed = parse_invoice_bytes(decoded, fname)\n            if parsed:\n                for inv in parsed:\n                    matched_dep = match_invoice_to_deployment(\n                        inv.get(\"deployment_date\"), ALL_DEPLOYMENTS_LIST, tolerance_days=3)\n                    inv[\"matched_deployment\"] = matched_dep\n                    inv_record = {\n                        \"filename\": fname,\n                        \"deployment_type\": inv[\"deployment_type\"],\n                        \"snow_tier\": inv[\"snow_tier\"],\n                        \"deployment_date\": inv[\"deployment_date\"],\n                        \"matched_deployment\": matched_dep,\n                        \"total_billed\": inv[\"total_billed\"],\n                        \"site_count\": inv[\"site_count\"],\n                        \"format\": inv[\"format\"],\n                        \"line_items\": inv[\"line_items\"],\n                    }\n                    existing = [i for i in invoices_registry\n                                if i.get(\"filename\") == fname and i.get(\"deployment_date\") == inv.get(\"deployment_date\")]\n                    if existing:\n                        invoices_registry.remove(existing[0])\n                    invoices_registry.append(inv_record)\n                all_parsed.extend(parsed)\n            else:\n                errors.append(f\"{fname}: No invoice data found\")\n        except Exception as e:\n            errors.append(f\"{fname}: {e}\")\n\n    SNOW_CONFIG[\"invoices\"] = invoices_registry\n    try:\n        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(SNOW_CONFIG, f, indent=2, ensure_ascii=False)\n    except Exception as e:\n        errors.append(f\"Config save error: {e}\")\n\n    if errors:\n        status = html.Div([\n            html.Span(f\"Imported {len(all_parsed)} invoice(s). \", style={\"color\": \"green\"}),\n            html.Span(f\"Errors: {'; '.join(errors)}\", style={\"color\": \"red\"}),\n        ])\n    else:\n        status = html.Span(f\"Successfully imported {len(all_parsed)} invoice(s).\", style={\"color\": \"green\"})\n\n    preview_rows = []\n    for inv in all_parsed:\n        preview_rows.append(\n            dbc.Card([\n                dbc.CardBody([\n                    html.H6(f\"{inv['filename']}\", className=\"mb-1\"),\n                    html.P(f\"Type: {inv['deployment_type']} | Tier: {inv['snow_tier'].replace('price_', '').replace('_', ' ').title()} | \"\n                           f\"Sites: {inv['site_count']} | Total: ${inv['total_billed']:,.2f}\",\n                           className=\"mb-1\", style={\"fontSize\": \"13px\"}),\n                    html.P(f\"Date: {inv.get('deployment_date', 'Unknown')} | \"\n                           f\"Matched Deployment: {inv.get('matched_deployment', 'No match')} | \"\n                           f\"Format: {inv['format']}\",\n                           className=\"text-muted mb-0\", style={\"fontSize\": \"12px\"}),\n                ])\n            ], className=\"mb-2 shadow-sm\")\n        )\n\n    inv_list = _render_invoice_list(invoices_registry)\n\n    return status, html.Div(preview_rows), inv_list\n\n\ndef _render_invoice_list(invoices_registry):\n    if not invoices_registry:\n        return html.P(\"No invoices imported yet.\", className=\"text-muted\", style={\"fontSize\": \"13px\"})\n\n    rows = []\n    for inv in invoices_registry:\n        tier_label = inv.get(\"snow_tier\", \"\").replace(\"price_\", \"\").replace(\"_\", \" \").title()\n        rows.append({\n            \"filename\": inv.get(\"filename\", \"\"),\n            \"type\": inv.get(\"deployment_type\", \"\"),\n            \"tier\": tier_label,\n            \"date\": inv.get(\"deployment_date\", \"\"),\n            \"deployment\": inv.get(\"matched_deployment\", \"\"),\n            \"sites\": inv.get(\"site_count\", 0),\n            \"total\": f\"${inv.get('total_billed', 0):,.2f}\",\n        })\n\n    return dash_table.DataTable(\n        columns=[\n            {\"name\": \"File\", \"id\": \"filename\"},\n            {\"name\": \"Type\", \"id\": \"type\"},\n            {\"name\": \"Tier\", \"id\": \"tier\"},\n            {\"name\": \"Date\", \"id\": \"date\"},\n            {\"name\": \"Deployment\", \"id\": \"deployment\"},\n            {\"name\": \"Sites\", \"id\": \"sites\", \"type\": \"numeric\"},\n            {\"name\": \"Total Billed\", \"id\": \"total\"},\n        ],\n        data=rows,\n        style_table={\"overflowX\": \"auto\"},\n        style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"12px\"},\n        style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f1f3f5\", \"fontSize\": \"12px\"},\n        page_size=10,\n    )\n\n\n# â”€â”€ Per-Deployment Labor Overrides Callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"fin-labor-overrides-container\", \"children\"),\n    Input(\"main-tabs\", \"active_tab\"),\n    Input(\"fin-labor-overrides-status\", \"children\"),\n)\ndef render_labor_overrides(active_tab, _trigger):\n    if active_tab != \"tab-finances\":\n        raise PreventUpdate\n\n    if not ALL_DEPLOYMENTS_LIST:\n        return html.P(\"No deployments available.\", className=\"text-muted\", style={\"fontSize\": \"13px\"})\n\n    labor_overrides = SNOW_CONFIG.get(\"labor_overrides\", {})\n    all_crews = sorted(DF_ALL[\"chat\"].unique()) if not DF_ALL.empty else []\n\n    rows_data = []\n    for dep in ALL_DEPLOYMENTS_LIST:\n        label = dep[\"label\"]\n        dep_overrides = labor_overrides.get(label, {})\n        dep_df = DF_ALL[(DF_ALL.get(\"deployment\") == label) if \"deployment\" in DF_ALL.columns else DF_ALL[\"msg_date\"].dt.date.between(dep[\"start_date\"], dep[\"end_date\"])]\n        dep_crews = sorted(dep_df[\"chat\"].unique()) if not dep_df.empty else all_crews[:5]\n\n        for crew in dep_crews:\n            crew_override = dep_overrides.get(crew, {})\n            rows_data.append({\n                \"deployment\": label,\n                \"crew\": crew,\n                \"labor_rate\": crew_override.get(\"labor_rate\", \"\"),\n                \"workers\": crew_override.get(\"workers\", \"\"),\n                \"hours\": crew_override.get(\"hours\", \"\"),\n            })\n\n    if not rows_data:\n        return html.P(\"No crew data for labor overrides.\", className=\"text-muted\", style={\"fontSize\": \"13px\"})\n\n    return dash_table.DataTable(\n        id=\"fin-labor-overrides-table\",\n        columns=[\n            {\"name\": \"Deployment\", \"id\": \"deployment\"},\n            {\"name\": \"Crew\", \"id\": \"crew\"},\n            {\"name\": \"Labor $/hr\", \"id\": \"labor_rate\", \"type\": \"numeric\", \"editable\": True},\n            {\"name\": \"Workers\", \"id\": \"workers\", \"type\": \"numeric\", \"editable\": True},\n            {\"name\": \"Hours\", \"id\": \"hours\", \"type\": \"numeric\", \"editable\": True},\n        ],\n        data=rows_data,\n        editable=True,\n        style_table={\"overflowX\": \"auto\", \"maxHeight\": \"400px\", \"overflowY\": \"auto\"},\n        style_cell={\"textAlign\": \"left\", \"padding\": \"6px\", \"fontSize\": \"12px\"},\n        style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f1f3f5\", \"fontSize\": \"12px\",\n                      \"position\": \"sticky\", \"top\": 0},\n        style_data_conditional=[\n            {\"if\": {\"column_editable\": True}, \"backgroundColor\": \"#fffde7\"},\n        ],\n        page_size=50,\n    )\n\n\n@app.callback(\n    Output(\"fin-labor-overrides-status\", \"children\"),\n    Input(\"fin-save-labor-overrides\", \"n_clicks\"),\n    State(\"fin-labor-overrides-table\", \"data\"),\n    prevent_initial_call=True,\n)\ndef save_labor_overrides(n_clicks, table_data):\n    global SNOW_CONFIG\n    if not n_clicks or not table_data:\n        raise PreventUpdate\n\n    labor_overrides = {}\n    for row in table_data:\n        dep = row.get(\"deployment\", \"\")\n        crew = row.get(\"crew\", \"\")\n        if not dep or not crew:\n            continue\n        lr = row.get(\"labor_rate\")\n        wk = row.get(\"workers\")\n        hrs = row.get(\"hours\")\n        if lr or wk or hrs:\n            if dep not in labor_overrides:\n                labor_overrides[dep] = {}\n            override = {}\n            if lr and lr != \"\":\n                try:\n                    override[\"labor_rate\"] = float(lr)\n                except (ValueError, TypeError):\n                    pass\n            if wk and wk != \"\":\n                try:\n                    override[\"workers\"] = int(wk)\n                except (ValueError, TypeError):\n                    pass\n            if hrs and hrs != \"\":\n                try:\n                    override[\"hours\"] = float(hrs)\n                except (ValueError, TypeError):\n                    pass\n            if override:\n                labor_overrides[dep][crew] = override\n\n    SNOW_CONFIG[\"labor_overrides\"] = labor_overrides\n\n    config_path = os.path.join(DATA_DIR, \"config\", \"snow_removal.json\")\n    try:\n        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(SNOW_CONFIG, f, indent=2, ensure_ascii=False)\n        return html.Span(\"Labor overrides saved!\", style={\"color\": \"green\"})\n    except Exception as e:\n        return html.Span(f\"Error: {e}\", style={\"color\": \"red\"})\n\n\n# â”€â”€ Invoice Reconciliation Callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"fin-recon-deployment\", \"options\"),\n    Input(\"main-tabs\", \"active_tab\"),\n)\ndef populate_recon_deployments(active_tab):\n    if active_tab != \"tab-finances\":\n        raise PreventUpdate\n    invoices = SNOW_CONFIG.get(\"invoices\", [])\n    options = []\n    seen = set()\n    for inv in invoices:\n        dep = inv.get(\"matched_deployment\")\n        if dep and dep not in seen:\n            seen.add(dep)\n            options.append({\"label\": f\"{dep} ({inv.get('deployment_type', '')})\", \"value\": dep})\n    if not options:\n        for dep in ALL_DEPLOYMENTS_LIST:\n            options.append({\"label\": dep[\"label\"], \"value\": dep[\"label\"]})\n    return options\n\n\n@app.callback(\n    Output(\"fin-recon-container\", \"children\"),\n    Input(\"fin-recon-btn\", \"n_clicks\"),\n    State(\"fin-recon-deployment\", \"value\"),\n    prevent_initial_call=True,\n)\ndef reconcile_deployment(n_clicks, dep_label):\n    if not n_clicks or not dep_label:\n        raise PreventUpdate\n\n    invoices = SNOW_CONFIG.get(\"invoices\", [])\n    matched_invoices = [inv for inv in invoices if inv.get(\"matched_deployment\") == dep_label]\n    if not matched_invoices:\n        return html.P(f\"No invoices found for deployment '{dep_label}'. Upload invoices in Settings first.\",\n                      className=\"text-muted\")\n\n    dep_info = next((d for d in ALL_DEPLOYMENTS_LIST if d[\"label\"] == dep_label), None)\n    if not dep_info:\n        return html.P(\"Deployment not found.\", className=\"text-muted\")\n\n    start = pd.Timestamp(dep_info[\"start_date\"])\n    end = pd.Timestamp(dep_info[\"end_date\"])\n    df_clean = DF_ALL[(DF_ALL[\"noise_type\"] == \"clean\") &\n                      (DF_ALL[\"msg_date\"] >= start) &\n                      (DF_ALL[\"msg_date\"] <= end + pd.Timedelta(days=1))]\n    chat_locations = list(df_clean[df_clean[\"location\"].str.len() > 0][\"location\"].unique()) if not df_clean.empty else []\n\n    job_logs = build_job_logs(df_clean, SNOW_CONFIG)\n    dep_routes = get_billable_routes_df(job_logs, deployment=dep_label) if not job_logs.empty else None\n\n    results_children = []\n    for inv in matched_invoices:\n        recon = reconcile_invoice_with_chat(inv, chat_locations, PRICING_INDEX, _normalize_location,\n                                            billable_routes=dep_routes)\n\n        route_info = \"\"\n        if recon.get(\"billable_routes_total\", 0) > 0:\n            route_info = (f\" | Billable Routes: {recon['billable_routes_total']} \"\n                         f\"(SW: {recon['sw_routes']}, PL: {recon['pl_routes']})\")\n\n        summary = dbc.Alert([\n            html.Strong(f\"{inv.get('filename', 'Invoice')} â€” {inv.get('deployment_type', '')}\"),\n            html.Br(),\n            html.Span(f\"Invoiced: {recon['sites_invoiced']} sites | \"\n                      f\"Found in chat: {recon['sites_in_chat']} | \"\n                      f\"Not in chat: {recon['sites_not_in_chat']} | \"\n                      f\"Price mismatches: {recon['price_mismatches']} | \"\n                      f\"Chat but not invoiced: {recon['chat_not_invoiced_count']}\"\n                      f\"{route_info}\"),\n        ], color=\"info\" if recon[\"sites_not_in_chat\"] == 0 else \"warning\", className=\"mb-2\")\n\n        issue_rows = [r for r in recon[\"line_items\"] if r[\"status\"] != \"ok\"]\n        if issue_rows:\n            issue_table = dash_table.DataTable(\n                columns=[\n                    {\"name\": \"Building\", \"id\": \"building_name\"},\n                    {\"name\": \"Address\", \"id\": \"address\"},\n                    {\"name\": \"Billed\", \"id\": \"billed_amount\", \"type\": \"numeric\"},\n                    {\"name\": \"Contract\", \"id\": \"contract_price\", \"type\": \"numeric\"},\n                    {\"name\": \"In Chat?\", \"id\": \"in_chat_data\"},\n                    {\"name\": \"Service Areas\", \"id\": \"service_areas\"},\n                    {\"name\": \"Status\", \"id\": \"status\"},\n                    {\"name\": \"Notes\", \"id\": \"notes\"},\n                ],\n                data=[{k: (f\"${v:,.2f}\" if k in (\"billed_amount\", \"contract_price\") and v else v)\n                       for k, v in r.items() if k != \"matched_chat_location\"} for r in issue_rows],\n                style_table={\"overflowX\": \"auto\"},\n                style_cell={\"textAlign\": \"left\", \"padding\": \"4px\", \"fontSize\": \"11px\"},\n                style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f1f3f5\", \"fontSize\": \"11px\"},\n                style_data_conditional=[\n                    {\"if\": {\"filter_query\": '{status} = \"not_in_chat\"'}, \"backgroundColor\": \"#fff3cd\"},\n                    {\"if\": {\"filter_query\": '{status} = \"price_mismatch\"'}, \"backgroundColor\": \"#f8d7da\"},\n                ],\n                page_size=20,\n            )\n        else:\n            issue_table = html.P(\"All invoiced sites match chat data.\", className=\"text-success\",\n                                style={\"fontSize\": \"13px\"})\n\n        chat_missing = recon.get(\"chat_not_invoiced\", [])\n        if chat_missing:\n            missing_list = html.Details([\n                html.Summary(f\"{len(chat_missing)} site(s) in chat but not invoiced\",\n                            style={\"fontSize\": \"13px\", \"color\": \"#856404\", \"cursor\": \"pointer\"}),\n                html.Ul([html.Li(loc, style={\"fontSize\": \"12px\"}) for loc in chat_missing[:30]]),\n            ], className=\"mt-1\")\n        else:\n            missing_list = html.Div()\n\n        results_children.extend([summary, issue_table, missing_list])\n\n    return html.Div(results_children)\n\n\n# â”€â”€ Completion Verification Callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n@app.callback(\n    Output(\"fin-verify-deployment\", \"options\"),\n    Input(\"main-tabs\", \"active_tab\"),\n)\ndef populate_verify_deployments(active_tab):\n    if active_tab != \"tab-finances\":\n        raise PreventUpdate\n    invoices = SNOW_CONFIG.get(\"invoices\", [])\n    options = []\n    seen = set()\n    for inv in invoices:\n        if inv.get(\"format\") in (\"completion_report\", \"pretreatment_report\"):\n            dep = inv.get(\"matched_deployment\")\n            if dep and dep not in seen:\n                seen.add(dep)\n                options.append({\"label\": f\"{dep} ({inv.get('deployment_type', '')})\", \"value\": dep})\n    if not options:\n        for dep in ALL_DEPLOYMENTS_LIST:\n            options.append({\"label\": dep[\"label\"], \"value\": dep[\"label\"]})\n    return options\n\n\n@app.callback(\n    Output(\"fin-verify-container\", \"children\"),\n    Input(\"fin-verify-btn\", \"n_clicks\"),\n    State(\"fin-verify-deployment\", \"value\"),\n    prevent_initial_call=True,\n)\ndef verify_completion(n_clicks, dep_label):\n    if not n_clicks or not dep_label:\n        raise PreventUpdate\n\n    invoices = SNOW_CONFIG.get(\"invoices\", [])\n    completion_invoices = [inv for inv in invoices\n                          if inv.get(\"matched_deployment\") == dep_label\n                          and inv.get(\"format\") in (\"completion_report\", \"pretreatment_report\")]\n\n    if not completion_invoices:\n        return html.P(f\"No completion/pre-treatment reports found for '{dep_label}'. \"\n                      \"Upload portal completion reports (Excel) in Settings.\",\n                      className=\"text-muted\")\n\n    dep_info = next((d for d in ALL_DEPLOYMENTS_LIST if d[\"label\"] == dep_label), None)\n    if not dep_info:\n        return html.P(\"Deployment not found.\", className=\"text-muted\")\n\n    start = pd.Timestamp(dep_info[\"start_date\"])\n    end = pd.Timestamp(dep_info[\"end_date\"])\n    df_clean = DF_ALL[(DF_ALL[\"noise_type\"] == \"clean\") &\n                      (DF_ALL[\"msg_date\"] >= start) &\n                      (DF_ALL[\"msg_date\"] <= end + pd.Timedelta(days=1))]\n\n    chat_locs_by_crew = {}\n    if not df_clean.empty:\n        for _, row in df_clean[df_clean[\"location\"].str.len() > 0].iterrows():\n            crew = row[\"chat\"]\n            loc = row[\"location\"]\n            if crew not in chat_locs_by_crew:\n                chat_locs_by_crew[crew] = set()\n            chat_locs_by_crew[crew].add(_normalize_location(loc))\n\n    results_children = []\n    for inv in completion_invoices:\n        verify_rows = []\n        for item in inv.get(\"line_items\", []):\n            bname = item.get(\"building_name\", \"\")\n            bname_norm = _normalize_location(bname)\n\n            portal_crew = item.get(\"removal_crew\") or item.get(\"pretreatment_crew\") or item.get(\"created_by\", \"\")\n            portal_time = item.get(\"created_time\", \"\")\n            pct_done = item.get(\"pct_completed\", \"\")\n            sw_done = item.get(\"sidewalks_cleared\") or item.get(\"sidewalks_pretreated\", \"\")\n            pl_done = item.get(\"parking_cleared\") or item.get(\"parking_pretreated\", \"\")\n\n            chat_crew_found = None\n            chat_confirmed = False\n            for crew, locs in chat_locs_by_crew.items():\n                if bname_norm in locs:\n                    chat_crew_found = crew\n                    chat_confirmed = True\n                    break\n                for loc in locs:\n                    if bname_norm and (bname_norm in loc or loc in bname_norm):\n                        chat_crew_found = crew\n                        chat_confirmed = True\n                        break\n                if chat_confirmed:\n                    break\n\n            status = \"verified\" if chat_confirmed else \"unverified\"\n            crew_match = \"\"\n            if portal_crew and chat_crew_found:\n                crew_match = \"match\" if _normalize_location(portal_crew) in _normalize_location(chat_crew_found) else \"different\"\n            elif not portal_crew and chat_crew_found:\n                crew_match = \"chat only\"\n            elif portal_crew and not chat_crew_found:\n                crew_match = \"portal only\"\n\n            verify_rows.append({\n                \"building\": bname,\n                \"portal_crew\": portal_crew,\n                \"portal_time\": str(portal_time),\n                \"completion\": str(pct_done) if pct_done else \"\",\n                \"sw\": str(sw_done),\n                \"pl\": str(pl_done),\n                \"chat_crew\": chat_crew_found or \"\",\n                \"status\": status,\n                \"crew_match\": crew_match,\n            })\n\n        verified = sum(1 for r in verify_rows if r[\"status\"] == \"verified\")\n        unverified = sum(1 for r in verify_rows if r[\"status\"] == \"unverified\")\n\n        summary = dbc.Alert([\n            html.Strong(f\"{inv.get('filename', 'Report')} â€” {inv.get('deployment_type', '')}\"),\n            html.Br(),\n            html.Span(f\"Total sites: {len(verify_rows)} | \"\n                      f\"Verified in chat: {verified} | \"\n                      f\"Unverified: {unverified}\"),\n        ], color=\"success\" if unverified == 0 else \"warning\", className=\"mb-2\")\n\n        vtable = dash_table.DataTable(\n            columns=[\n                {\"name\": \"Building\", \"id\": \"building\"},\n                {\"name\": \"Portal Crew\", \"id\": \"portal_crew\"},\n                {\"name\": \"Portal Time\", \"id\": \"portal_time\"},\n                {\"name\": \"Completion\", \"id\": \"completion\"},\n                {\"name\": \"SW\", \"id\": \"sw\"},\n                {\"name\": \"PL\", \"id\": \"pl\"},\n                {\"name\": \"Chat Crew\", \"id\": \"chat_crew\"},\n                {\"name\": \"Status\", \"id\": \"status\"},\n                {\"name\": \"Crew Match\", \"id\": \"crew_match\"},\n            ],\n            data=verify_rows,\n            style_table={\"overflowX\": \"auto\"},\n            style_cell={\"textAlign\": \"left\", \"padding\": \"4px\", \"fontSize\": \"11px\"},\n            style_header={\"fontWeight\": \"bold\", \"backgroundColor\": \"#f1f3f5\", \"fontSize\": \"11px\"},\n            style_data_conditional=[\n                {\"if\": {\"filter_query\": '{status} = \"unverified\"'}, \"backgroundColor\": \"#fff3cd\"},\n                {\"if\": {\"filter_query\": '{status} = \"verified\"'}, \"backgroundColor\": \"#d4edda\"},\n                {\"if\": {\"filter_query\": '{crew_match} = \"different\"'}, \"color\": \"#dc3545\", \"fontWeight\": \"bold\"},\n            ],\n            page_size=30,\n        )\n\n        results_children.extend([summary, vtable, html.Hr(className=\"my-2\")])\n\n    return html.Div(results_children)\n\n\n# â”€â”€ Apply Labor Overrides in Financial Calculations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _get_crew_labor_override(dep_label, crew_chat, snow_config):\n    overrides = snow_config.get(\"labor_overrides\", {})\n    dep_overrides = overrides.get(dep_label, {})\n    return dep_overrides.get(crew_chat, {})\n\n\n# â”€â”€ Entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef print_report():\n    \"\"\"Print analytic metrics report to terminal.\"\"\"\n    sep = \"â”€\" * 72\n    print(f\"\\n{'â•' * 72}\")\n    print(\"  WhatsApp Chat Dashboard â€” Data Report\")\n    print(f\"{'â•' * 72}\\n\")\n\n    # â”€â”€ Overall stats\n    pct_clean = CLEAN_MSGS / TOTAL_MSGS * 100 if TOTAL_MSGS else 0\n    pct_noise = NOISE_MSGS / TOTAL_MSGS * 100 if TOTAL_MSGS else 0\n    print(f\"  Total messages:  {TOTAL_MSGS}\")\n    print(f\"  Clean:           {CLEAN_MSGS}  ({pct_clean:.1f}%)\")\n    print(f\"  Noise:           {NOISE_MSGS}  ({pct_noise:.1f}%)\")\n    print(f\"  Unique chats:    {NUM_CHATS}\")\n    print(f\"  Unique senders:  {NUM_SENDERS}\")\n    print(f\"  Message types:   {', '.join(ALL_TYPES)}\")\n\n    # â”€â”€ Deployment summary\n    print(f\"\\n{sep}\")\n    print(\"  Deployment Summary\")\n    print(sep)\n    if ALL_DEPLOYMENTS_LIST:\n        print(f\"  {'#':>2s}  {'Date Range':<20s} {'Days':>4s} {'Crews':>5s} \"\n              f\"{'Sites':>5s} {'Msgs':>5s}\")\n        print(f\"  {'â”€' * 2}  {'â”€' * 20} {'â”€' * 4} {'â”€' * 5} {'â”€' * 5} {'â”€' * 5}\")\n        for dep in ALL_DEPLOYMENTS_LIST:\n            grp = DF_ALL[DF_ALL[\"deployment\"] == dep[\"label\"]]\n            grp_clean = grp[grp[\"noise_type\"] == \"clean\"]\n            n_crews = grp_clean[\"chat\"].nunique()\n            n_msgs = len(grp_clean)\n            visits_df = _build_site_visits(grp_clean, \"chat\")\n            n_sites = len(visits_df)\n            print(f\"  {dep['id']:>2d}  {dep['label']:<20s} {dep['days']:>4d} \"\n                  f\"{n_crews:>5d} {n_sites:>5d} {n_msgs:>5d}\")\n    else:\n        print(\"  No deployments detected.\")\n\n    # â”€â”€ Noise breakdown\n    print(f\"\\n{sep}\")\n    print(\"  Noise Breakdown\")\n    print(sep)\n    noise_counts = DF_ALL[\"noise_type\"].value_counts()\n    for ntype in sorted(noise_counts.index):\n        cnt = noise_counts[ntype]\n        pct = cnt / TOTAL_MSGS * 100\n        bar = \"â–ˆ\" * int(pct / 2) + \"â–‘\" * (50 - int(pct / 2))\n        print(f\"  {ntype:<25s} {cnt:>4d}  ({pct:5.1f}%)  {bar}\")\n\n    # â”€â”€ Per-chat breakdown\n    print(f\"\\n{sep}\")\n    print(\"  Per-Chat Breakdown\")\n    print(sep)\n    print(f\"  {'Chat':<35s} {'Total':>5s} {'Clean':>5s} {'Noise':>5s} \"\n          f\"{'%Clean':>6s}  {'Senders'}\")\n    print(f\"  {'â”€' * 35} {'â”€' * 5} {'â”€' * 5} {'â”€' * 5} {'â”€' * 6}  {'â”€' * 20}\")\n    for chat in ALL_CHATS:\n        sub = DF_ALL[DF_ALL[\"chat\"] == chat]\n        n_total = len(sub)\n        n_clean = len(sub[sub[\"noise_type\"] == \"clean\"])\n        n_noise = n_total - n_clean\n        pct = n_clean / n_total * 100 if n_total else 0\n        senders_in = sorted(\n            sub[sub[\"sender_resolved\"] != \"\"][\"sender_resolved\"].unique())\n        senders_str = \", \".join(senders_in)\n        if len(senders_str) > 20:\n            senders_str = senders_str[:17] + \"...\"\n        chat_short = chat[:35]\n        print(f\"  {chat_short:<35s} {n_total:>5d} {n_clean:>5d} {n_noise:>5d} \"\n              f\"{pct:>5.1f}%  {senders_str}\")\n\n    # â”€â”€ Per-sender stats (resolved)\n    print(f\"\\n{sep}\")\n    print(\"  Per-Sender Stats (resolved)\")\n    print(sep)\n    df_with_sender = DF_ALL[DF_ALL[\"sender_resolved\"] != \"\"]\n    sender_stats = (df_with_sender.groupby(\"sender_resolved\")\n                    .agg(total=(\"noise_type\", \"size\"),\n                         clean=(\"noise_type\", lambda x: (x == \"clean\").sum()),\n                         chats=(\"chat\", \"nunique\"),\n                         avg_len=(\"content_len\", \"mean\"))\n                    .sort_values(\"total\", ascending=False))\n    print(f\"  {'Sender':<25s} {'Total':>5s} {'Clean':>5s} {'Chats':>5s} \"\n          f\"{'Avg Len':>7s}\")\n    print(f\"  {'â”€' * 25} {'â”€' * 5} {'â”€' * 5} {'â”€' * 5} {'â”€' * 7}\")\n    for sender, row in sender_stats.iterrows():\n        sender_short = str(sender)[:25]\n        print(f\"  {sender_short:<25s} {int(row['total']):>5d} \"\n              f\"{int(row['clean']):>5d} {int(row['chats']):>5d} \"\n              f\"{row['avg_len']:>7.1f}\")\n\n    # â”€â”€ Time distribution\n    print(f\"\\n{sep}\")\n    print(\"  Hourly Distribution (clean messages)\")\n    print(sep)\n    df_clean_timed = DF_ALL[\n        (DF_ALL[\"noise_type\"] == \"clean\") & DF_ALL[\"hour_int\"].notna()].copy()\n    if not df_clean_timed.empty:\n        df_clean_timed[\"hour_int\"] = df_clean_timed[\"hour_int\"].astype(int)\n        hour_counts = df_clean_timed[\"hour_int\"].value_counts().sort_index()\n        max_count = hour_counts.max() if len(hour_counts) else 1\n        for h in range(24):\n            cnt = hour_counts.get(h, 0)\n            bar_len = int(cnt / max_count * 40) if max_count else 0\n            label = f\"{h % 12 or 12:>2d} {'AM' if h < 12 else 'PM'}\"\n            bar = \"â–ˆ\" * bar_len\n            print(f\"  {label}  {cnt:>3d}  {bar}\")\n\n    # â”€â”€ De-duplication report\n    print(f\"\\n{sep}\")\n    print(\"  File De-duplication\")\n    print(sep)\n    all_files, _ = _discover_json_files(DATA_DIR)\n    chat_file_count = {}\n    for path in all_files:\n        try:\n            with open(path) as f:\n                data = json.load(f)\n            if \"exportInfo\" not in data:\n                continue\n            name = data[\"exportInfo\"][\"chatName\"]\n            key = _normalize_chat_name(name)\n            chat_file_count.setdefault(key, []).append(\n                os.path.relpath(path, DATA_DIR))\n        except (json.JSONDecodeError, KeyError):\n            continue\n    dupes = {k: v for k, v in chat_file_count.items() if len(v) > 1}\n    if dupes:\n        for name, files in sorted(dupes.items()):\n            print(f\"  {name}: {len(files)} files (kept largest)\")\n            for fn in files:\n                print(f\"    - {fn}\")\n    else:\n        print(\"  No duplicate chat names found.\")\n\n    # â”€â”€ Reporting efficiency\n    print(f\"\\n{sep}\")\n    print(\"  Reporting Efficiency (per sender per day)\")\n    print(sep)\n    ds = _build_daily_summary(DF_ALL[DF_ALL[\"noise_type\"] == \"clean\"],\n                              \"sender_resolved\")\n    if not ds.empty:\n        print(f\"  {'Sender':<25s} {'Days':>4s} {'Avg 1st':>8s} \"\n              f\"{'Avg Window':>10s} {'Avg Msgs':>8s}\")\n        print(f\"  {'â”€' * 25} {'â”€' * 4} {'â”€' * 8} {'â”€' * 10} {'â”€' * 8}\")\n        for sender, grp in ds.groupby(\"sender\"):\n            n_days = len(grp)\n            avg_first = grp[\"first_hour\"].mean()\n            fh = int(avg_first)\n            fm = int((avg_first - fh) * 60)\n            period = \"AM\" if fh < 12 else \"PM\"\n            dh = fh % 12 or 12\n            avg_win = grp[\"window_hrs\"].mean()\n            avg_msgs = grp[\"msg_count\"].mean()\n            sender_short = str(sender)[:25]\n            print(f\"  {sender_short:<25s} {n_days:>4d} \"\n                  f\"{dh:>2d}:{fm:02d} {period} \"\n                  f\"{avg_win:>9.1f}h {avg_msgs:>8.1f}\")\n    else:\n        print(\"  No daily efficiency data available.\")\n\n    # â”€â”€ Crew Metrics\n    print(f\"\\n{sep}\")\n    print(\"  Crew Metrics (site visits per crew/chat)\")\n    print(sep)\n    df_clean = DF_ALL[DF_ALL[\"noise_type\"] == \"clean\"]\n    visits_df = _build_site_visits(df_clean, \"chat\")\n    if not visits_df.empty:\n        ds_crew = _build_daily_summary(df_clean, \"chat\")\n        sc = _build_crew_scorecard(visits_df, ds_crew)\n        if not sc.empty:\n            print(f\"  {'Crew':<35s} {'Sites':>5s} {'Sites/Day':>9s} \"\n                  f\"{'Sites/Hr':>8s} {'Avg Trans':>9s} {'Hrs':>5s}\")\n            print(f\"  {'â”€' * 35} {'â”€' * 5} {'â”€' * 9} {'â”€' * 8} \"\n                  f\"{'â”€' * 9} {'â”€' * 5}\")\n            for _, row in sc.iterrows():\n                s = str(row[\"sender\"])[:35]\n                print(f\"  {s:<35s} {int(row['total_sites']):>5d} \"\n                      f\"{row['avg_sites_per_day']:>9.1f} \"\n                      f\"{row['avg_sites_per_hour']:>8.1f} \"\n                      f\"{row['avg_transition_min']:>8.1f}m \"\n                      f\"{row['total_active_hrs']:>5.1f}\")\n\n        # Top locations\n        loc_counts = (visits_df[\"location\"].value_counts().head(10)\n                      .reset_index())\n        loc_counts.columns = [\"location\", \"visits\"]\n        if not loc_counts.empty:\n            print(f\"\\n  Top 10 Locations:\")\n            for _, row in loc_counts.iterrows():\n                loc = str(row[\"location\"])[:50]\n                print(f\"    {row['visits']:>3d}  {loc}\")\n    else:\n        print(\"  No site visit data available.\")\n\n    print(f\"\\n{'â•' * 72}\")\n    print(f\"  Dashboard ready at http://0.0.0.0:5000\")\n    print(f\"{'â•' * 72}\\n\")\n\n\nif __name__ == \"__main__\":\n    print_report()\n    app.run(debug=True, host=\"0.0.0.0\", port=5000, threaded=True)\n","path":null,"size_bytes":274505,"size_tokens":null},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"dash>=4.0.0\",\n    \"dash-bootstrap-components>=2.0.4\",\n    \"numpy>=2.4.2\",\n    \"openpyxl>=3.1.5\",\n    \"pandas>=3.0.1\",\n    \"plotly>=6.5.2\",\n]\n","path":null,"size_bytes":288,"size_tokens":null},"main.py":{"content":"def main():\n    print(\"Hello from repl-nix-workspace!\")\n\n\nif __name__ == \"__main__\":\n    main()\n","path":null,"size_bytes":96,"size_tokens":null},"data/domain_model.py":{"content":"\"\"\"Domain model module for the Snow Removal Deployment Tracking system.\n\nMaps WhatsApp chat export data (pandas DataFrame) into formalized snow removal\ndomain entities such as job logs, crew summaries, route segments, and deployment\nburndown metrics.\n\nThis module is standalone and does NOT depend on dashboard_web.py or Dash.\n\"\"\"\n\nimport json\nimport math\nimport os\nimport re\n\nimport numpy as np\nimport pandas as pd\n\n\n# ---------------------------------------------------------------------------\n# Haversine distance calculation\n# ---------------------------------------------------------------------------\n\ndef haversine_km(lat1, lon1, lat2, lon2):\n    R = 6371.0\n    dlat = math.radians(lat2 - lat1)\n    dlon = math.radians(lon2 - lon1)\n    a = (math.sin(dlat / 2) ** 2 +\n         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *\n         math.sin(dlon / 2) ** 2)\n    return R * 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n\ndef build_location_coords(location_registry):\n    norm = lambda s: re.sub(r\"[^a-z0-9 ]\", \"\", s.lower().strip())\n    coords = {}\n    for entry in location_registry:\n        lat = entry.get(\"lat\")\n        lon = entry.get(\"lon\")\n        if lat is None or lon is None or lat == \"\" or lon == \"\":\n            continue\n        try:\n            lat_f, lon_f = float(lat), float(lon)\n        except (ValueError, TypeError):\n            continue\n        for field in [\"_matched_raw\", \"location_name\", \"address\"]:\n            val = entry.get(field, \"\")\n            if val:\n                key = norm(val)\n                if key and key not in coords:\n                    coords[key] = (lat_f, lon_f)\n        mcl = entry.get(\"matched_chat_location\", \"\")\n        if mcl and \"(\" in mcl:\n            raw = mcl.split(\"(\")[0].strip()\n            key = norm(raw)\n            if key and key not in coords:\n                coords[key] = (float(lat), float(lon))\n    return coords\n\n\nCITY_AVG_SPEED_KMH = 25.0\n\n\n# ---------------------------------------------------------------------------\n# Billable Route utilities\n# ---------------------------------------------------------------------------\n\ndef billable_route_key(deployment, location, service_area):\n    loc_norm = re.sub(r\"[^a-z0-9 ]\", \"\", str(location).lower().strip())\n    sa = str(service_area).strip() if service_area else \"Unknown\"\n    return f\"{deployment}|{loc_norm}|{sa}\"\n\n\ndef count_billable_routes(job_logs_df, deployment=None):\n    if job_logs_df is None or job_logs_df.empty:\n        return 0\n    df = job_logs_df\n    if deployment:\n        df = df[df[\"deployment\"] == deployment]\n    if df.empty:\n        return 0\n    routes = df.apply(\n        lambda r: billable_route_key(r.get(\"deployment\", \"\"), r.get(\"location\", \"\"), r.get(\"location_type\", \"\")),\n        axis=1,\n    )\n    return routes.nunique()\n\n\ndef get_billable_routes_df(job_logs_df, deployment=None):\n    if job_logs_df is None or job_logs_df.empty:\n        return pd.DataFrame(columns=[\"deployment\", \"location\", \"service_area\", \"route_key\"])\n    df = job_logs_df.copy()\n    if deployment:\n        df = df[df[\"deployment\"] == deployment]\n    if df.empty:\n        return pd.DataFrame(columns=[\"deployment\", \"location\", \"service_area\", \"route_key\"])\n    df[\"route_key\"] = df.apply(\n        lambda r: billable_route_key(r.get(\"deployment\", \"\"), r.get(\"location\", \"\"), r.get(\"location_type\", \"\")),\n        axis=1,\n    )\n    routes = df.groupby(\"route_key\").agg(\n        deployment=(\"deployment\", \"first\"),\n        location=(\"location\", \"first\"),\n        service_area=(\"location_type\", \"first\"),\n        visits=(\"route_key\", \"size\"),\n    ).reset_index()\n    return routes\n\n\ndef estimate_travel_mins(distance_km, speed_kmh=None):\n    if speed_kmh is None:\n        speed_kmh = CITY_AVG_SPEED_KMH\n    if distance_km <= 0 or speed_kmh <= 0:\n        return 0.0\n    return (distance_km / speed_kmh) * 60.0\n\n\n# ---------------------------------------------------------------------------\n# Helper: auto-detect location type from chat name\n# ---------------------------------------------------------------------------\n\ndef infer_location_type(chat_name):\n    \"\"\"Infer the location type from a chat/group name.\n\n    Returns:\n        str: \"Sidewalk\" if the name contains 'sidewalk' (case-insensitive),\n             \"Parking Lot\" if it contains 'parking', otherwise \"Unknown\".\n    \"\"\"\n    if not chat_name:\n        return \"Unknown\"\n    name_lower = chat_name.lower()\n    if \"sidewalk\" in name_lower:\n        return \"Sidewalk\"\n    if \"parking\" in name_lower:\n        return \"Parking Lot\"\n    return \"Unknown\"\n\n\n# ---------------------------------------------------------------------------\n# 1. Configuration Loading\n# ---------------------------------------------------------------------------\n\ndef load_config(config_path):\n    \"\"\"Load a JSON configuration file for the snow removal domain model.\n\n    The config file may contain:\n        - deployment_types: dict mapping deployment label -> type enum\n        - location_types: dict mapping normalized location name -> type\n        - crew_config: dict mapping crew/chat name -> {crew_size, machines}\n        - non_trackable_senders: list of sender names to exclude from KPIs\n        - standard_travel_times: dict mapping \"from|to\" -> minutes\n        - expected_service_times: dict mapping location_type -> minutes\n\n    If the file does not exist, sensible defaults (empty dicts/lists) are\n    returned.\n\n    Args:\n        config_path: Path to the JSON config file.\n\n    Returns:\n        dict with all configuration keys populated.\n    \"\"\"\n    defaults = {\n        \"deployment_types\": {},\n        \"location_types\": {},\n        \"crew_config\": {},\n        \"non_trackable_senders\": [],\n        \"standard_travel_times\": {},\n        \"expected_service_times\": {},\n    }\n\n    if not os.path.exists(config_path):\n        return defaults\n\n    try:\n        with open(config_path, encoding=\"utf-8\") as f:\n            data = json.load(f)\n    except (json.JSONDecodeError, OSError):\n        return defaults\n\n    for key in defaults:\n        if key not in data:\n            data[key] = defaults[key]\n\n    return data\n\n\n# ---------------------------------------------------------------------------\n# Internal helpers\n# ---------------------------------------------------------------------------\n\ndef _resolve_location_type(location, chat_name, config):\n    \"\"\"Resolve location type from config or auto-detect from chat name.\"\"\"\n    loc_types = config.get(\"location_types\", {})\n    if location and location in loc_types:\n        return loc_types[location]\n    return infer_location_type(chat_name)\n\n\ndef _empty_df(columns):\n    \"\"\"Return an empty DataFrame with the given columns.\"\"\"\n    return pd.DataFrame(columns=columns)\n\n\n# ---------------------------------------------------------------------------\n# 2. Domain Builder Functions\n# ---------------------------------------------------------------------------\n\ndef build_job_logs(df, config):\n    \"\"\"Build job log records from raw chat messages.\n\n    Each image-type message (type == \"image\") or message with a non-empty\n    location field is treated as a job completion report.\n\n    Args:\n        df: Raw messages DataFrame with columns from dashboard_web.py.\n        config: Configuration dict (output of load_config).\n\n    Returns:\n        DataFrame with columns: crew, deployment, date, location,\n        location_type, picture_submitted_at, chat, is_recall,\n        recall_added_time_mins.\n    \"\"\"\n    cols = [\n        \"crew\", \"deployment\", \"date\", \"location\", \"location_type\",\n        \"picture_submitted_at\", \"chat\", \"is_recall\", \"recall_added_time_mins\",\n    ]\n\n    if df is None or df.empty:\n        return _empty_df(cols)\n\n    required_cols = [\"type\", \"location\", \"sender_resolved\", \"chat\", \"time\", \"msg_date\"]\n    for rc in required_cols:\n        if rc not in df.columns:\n            return _empty_df(cols)\n\n    mask = (df[\"type\"] == \"image\") | (df[\"location\"].astype(str).str.len() > 0)\n    subset = df.loc[mask].copy()\n\n    if subset.empty:\n        return _empty_df(cols)\n\n    job_logs = pd.DataFrame()\n    job_logs[\"crew\"] = subset[\"sender_resolved\"]\n    job_logs[\"deployment\"] = subset.get(\"deployment\", pd.Series(dtype=\"object\"))\n    job_logs[\"date\"] = pd.to_datetime(subset[\"msg_date\"])\n    job_logs[\"location\"] = subset[\"location\"]\n    job_logs[\"chat\"] = subset[\"chat\"]\n    job_logs[\"picture_submitted_at\"] = pd.to_datetime(subset[\"time\"])\n\n    job_logs[\"location_type\"] = subset.apply(\n        lambda row: _resolve_location_type(\n            row.get(\"location\", \"\"), row.get(\"chat\", \"\"), config\n        ),\n        axis=1,\n    )\n\n    job_logs = job_logs.reset_index(drop=True)\n\n    job_logs[\"is_recall\"] = False\n    job_logs[\"recall_added_time_mins\"] = np.nan\n\n    job_logs_sorted = job_logs.sort_values(\n        [\"crew\", \"deployment\", \"location\", \"picture_submitted_at\"]\n    ).reset_index(drop=True)\n\n    for idx in range(len(job_logs_sorted)):\n        row = job_logs_sorted.iloc[idx]\n        if not row[\"crew\"] or not row[\"location\"] or pd.isna(row[\"deployment\"]):\n            continue\n        earlier = job_logs_sorted.iloc[:idx]\n        same = earlier[\n            (earlier[\"crew\"] == row[\"crew\"])\n            & (earlier[\"deployment\"] == row[\"deployment\"])\n            & (earlier[\"location\"] == row[\"location\"])\n        ]\n        if not same.empty:\n            job_logs_sorted.at[idx, \"is_recall\"] = True\n            last_visit = same[\"picture_submitted_at\"].max()\n            if pd.notna(last_visit) and pd.notna(row[\"picture_submitted_at\"]):\n                delta = (\n                    row[\"picture_submitted_at\"] - last_visit\n                ).total_seconds() / 60.0\n                job_logs_sorted.at[idx, \"recall_added_time_mins\"] = delta\n\n    return job_logs_sorted[cols].reset_index(drop=True)\n\n\ndef build_crew_summary(job_logs_df, config, location_coords=None):\n    \"\"\"Build per-crew summary statistics from job logs.\n\n    Args:\n        job_logs_df: DataFrame output of build_job_logs.\n        config: Configuration dict (output of load_config).\n        location_coords: dict mapping normalized location name to (lat, lon).\n\n    Returns:\n        DataFrame with columns: crew, days_active, total_sites, total_recalls,\n        avg_sites_per_hour, avg_transition_min, crew_size, machines.\n    \"\"\"\n    cols = [\n        \"crew\", \"days_active\", \"total_sites\", \"total_recalls\",\n        \"avg_sites_per_hour\", \"avg_transition_min\", \"crew_size\", \"machines\",\n    ]\n\n    if job_logs_df is None or job_logs_df.empty:\n        return _empty_df(cols)\n\n    trackable = filter_trackable(job_logs_df, config, sender_col=\"crew\")\n    if trackable.empty:\n        return _empty_df(cols)\n\n    crew_config = config.get(\"crew_config\", {})\n    rows = []\n\n    for crew, grp in trackable.groupby(\"crew\"):\n        days_active = grp[\"date\"].dt.date.nunique()\n        total_sites = count_billable_routes(grp)\n        total_recalls = int(grp[\"is_recall\"].sum())\n\n        times = grp[\"picture_submitted_at\"].dropna().sort_values()\n        if len(times) >= 2:\n            total_hours = (times.max() - times.min()).total_seconds() / 3600.0\n            avg_sites_per_hour = (\n                total_sites / total_hours if total_hours > 0 else 0.0\n            )\n        else:\n            avg_sites_per_hour = 0.0\n\n        segments = build_route_segments(grp, config, location_coords)\n        avg_transition_min = (\n            segments[\"actual_duration_mins\"].mean()\n            if not segments.empty\n            else 0.0\n        )\n\n        cc = crew_config.get(crew, {})\n        crew_size = cc.get(\"crew_size\", 1)\n        machines = cc.get(\"machines\", [])\n\n        rows.append({\n            \"crew\": crew,\n            \"days_active\": days_active,\n            \"total_sites\": total_sites,\n            \"total_recalls\": total_recalls,\n            \"avg_sites_per_hour\": round(avg_sites_per_hour, 2),\n            \"avg_transition_min\": round(avg_transition_min, 1),\n            \"crew_size\": crew_size,\n            \"machines\": machines,\n        })\n\n    return pd.DataFrame(rows, columns=cols)\n\n\ndef build_route_segments(job_logs_df, config, location_coords=None):\n    \"\"\"Build route segments from consecutive job logs for the same crew/day.\n\n    Each pair of consecutive job log entries for the same crew on the same\n    day forms a route segment.\n\n    Args:\n        job_logs_df: DataFrame output of build_job_logs.\n        config: Configuration dict (output of load_config).\n        location_coords: dict mapping normalized location name to (lat, lon).\n\n    Returns:\n        DataFrame with columns: crew, date, origin_location,\n        destination_location, standard_travel_time_mins,\n        actual_duration_mins, is_delayed, distance_km,\n        estimated_travel_mins, travel_efficiency.\n    \"\"\"\n    cols = [\n        \"crew\", \"date\", \"origin_location\", \"destination_location\",\n        \"start_time\", \"end_time\",\n        \"standard_travel_time_mins\", \"actual_duration_mins\", \"is_delayed\",\n        \"distance_km\", \"estimated_travel_mins\", \"travel_efficiency\",\n    ]\n\n    if job_logs_df is None or job_logs_df.empty:\n        return _empty_df(cols)\n\n    if location_coords is None:\n        location_coords = {}\n\n    _norm = lambda s: re.sub(r\"[^a-z0-9 ]\", \"\", s.lower().strip())\n\n    def _lookup_coords(loc_name):\n        key = _norm(loc_name)\n        if key in location_coords:\n            return location_coords[key]\n        for k, v in location_coords.items():\n            if key in k or k in key:\n                return v\n        return None\n\n    sorted_df = job_logs_df.sort_values(\n        [\"crew\", \"date\", \"picture_submitted_at\"]\n    ).reset_index(drop=True)\n\n    standard_times = config.get(\"standard_travel_times\", {})\n    expected_service = config.get(\"expected_service_times\", {})\n    rows = []\n\n    for (crew, dt), grp in sorted_df.groupby(\n        [\"crew\", sorted_df[\"date\"].dt.date]\n    ):\n        grp = grp.reset_index(drop=True)\n        for i in range(1, len(grp)):\n            origin = grp.iloc[i - 1]\n            dest = grp.iloc[i]\n\n            origin_loc = origin[\"location\"]\n            dest_loc = dest[\"location\"]\n\n            key = f\"{origin_loc}|{dest_loc}\"\n            std_travel = standard_times.get(key)\n\n            t0 = origin[\"picture_submitted_at\"]\n            t1 = dest[\"picture_submitted_at\"]\n            if pd.notna(t0) and pd.notna(t1):\n                actual_mins = (t1 - t0).total_seconds() / 60.0\n            else:\n                actual_mins = np.nan\n\n            dist_km = np.nan\n            est_travel = np.nan\n            efficiency = np.nan\n            o_coords = _lookup_coords(origin_loc)\n            d_coords = _lookup_coords(dest_loc)\n            if o_coords and d_coords:\n                dist_km = haversine_km(o_coords[0], o_coords[1], d_coords[0], d_coords[1])\n                est_travel = estimate_travel_mins(dist_km)\n                if std_travel is None and pd.notna(est_travel):\n                    std_travel = round(est_travel, 1)\n\n            is_delayed = False\n            if pd.notna(actual_mins) and std_travel is not None:\n                origin_type = origin.get(\"location_type\", \"Unknown\")\n                svc_time = expected_service.get(origin_type, 0)\n                if actual_mins > svc_time + std_travel:\n                    is_delayed = True\n\n            if pd.notna(actual_mins) and pd.notna(est_travel) and est_travel > 0:\n                efficiency = round(est_travel / actual_mins * 100, 1) if actual_mins > 0 else 100.0\n\n            rows.append({\n                \"crew\": crew,\n                \"date\": pd.Timestamp(dt),\n                \"origin_location\": origin_loc,\n                \"destination_location\": dest_loc,\n                \"start_time\": t0,\n                \"end_time\": t1,\n                \"standard_travel_time_mins\": std_travel,\n                \"actual_duration_mins\": round(actual_mins, 1) if pd.notna(actual_mins) else np.nan,\n                \"is_delayed\": is_delayed,\n                \"distance_km\": round(dist_km, 2) if pd.notna(dist_km) else np.nan,\n                \"estimated_travel_mins\": round(est_travel, 1) if pd.notna(est_travel) else np.nan,\n                \"travel_efficiency\": efficiency if pd.notna(efficiency) else np.nan,\n            })\n\n    if not rows:\n        return _empty_df(cols)\n\n    return pd.DataFrame(rows, columns=cols)\n\n\ndef build_deployment_burndown(job_logs_df, deployments_list, config):\n    \"\"\"Build deployment burndown data comparing actual vs expected pace.\n\n    For each deployment, computes cumulative sites completed over time\n    against an expected linear pace scaled by crew count.  Larger crews\n    are expected to finish the same number of sites faster.\n\n    The expected pace is scaled by crew count:\n        adjusted_hours = expected_hours / crew_count\n        expected_at_t = min(total_sites, total_sites * elapsed / adjusted_hours)\n    More crews = shorter adjusted window = steeper expected curve.\n\n    Args:\n        job_logs_df: DataFrame output of build_job_logs.\n        deployments_list: List of deployment dicts with keys: label,\n            start_date, end_date.\n        config: Configuration dict (output of load_config).\n\n    Returns:\n        DataFrame with columns: deployment, timestamp, cumulative_completed,\n        expected_completed, pct_complete, crew_count.\n    \"\"\"\n    cols = [\n        \"deployment\", \"timestamp\", \"cumulative_completed\",\n        \"expected_completed\", \"pct_complete\", \"crew_count\",\n    ]\n\n    if job_logs_df is None or job_logs_df.empty or not deployments_list:\n        return _empty_df(cols)\n\n    base_expected_hours = config.get(\"expected_deployment_hours\", 12.0)\n    rows = []\n\n    for dep in deployments_list:\n        label = dep.get(\"label\", \"\")\n        dep_logs = job_logs_df[job_logs_df[\"deployment\"] == label].copy()\n        if dep_logs.empty:\n            continue\n\n        dep_logs = dep_logs.sort_values(\"picture_submitted_at\").reset_index(drop=True)\n        total_sites = count_billable_routes(dep_logs)\n        if total_sites == 0:\n            continue\n        start_time = dep_logs[\"picture_submitted_at\"].min()\n\n        if pd.isna(start_time):\n            continue\n\n        crew_count = dep_logs[\"crew\"].nunique() if \"crew\" in dep_logs.columns else 1\n        crew_count = max(crew_count, 1)\n\n        adjusted_hours = base_expected_hours / crew_count\n\n        for i, (_, row) in enumerate(dep_logs.iterrows(), 1):\n            ts = row[\"picture_submitted_at\"]\n            if pd.isna(ts):\n                continue\n            elapsed_hours = (ts - start_time).total_seconds() / 3600.0\n            expected = min(\n                total_sites,\n                total_sites * elapsed_hours / adjusted_hours,\n            )\n            pct = (i / total_sites * 100.0) if total_sites > 0 else 0.0\n            rows.append({\n                \"deployment\": label,\n                \"timestamp\": ts,\n                \"cumulative_completed\": i,\n                \"expected_completed\": round(expected, 2),\n                \"pct_complete\": round(pct, 2),\n                \"crew_count\": crew_count,\n            })\n\n    if not rows:\n        return _empty_df(cols)\n\n    return pd.DataFrame(rows, columns=cols)\n\n\ndef build_location_type_stats(job_logs_df):\n    \"\"\"Compute average service time per location type.\n\n    Uses time gaps between consecutive job logs at the same location type\n    as a proxy for service duration.\n\n    Args:\n        job_logs_df: DataFrame output of build_job_logs.\n\n    Returns:\n        DataFrame with columns: location_type, avg_duration_min, count,\n        std_duration_min.\n    \"\"\"\n    cols = [\"location_type\", \"avg_duration_min\", \"count\", \"std_duration_min\"]\n\n    if job_logs_df is None or job_logs_df.empty:\n        return _empty_df(cols)\n\n    sorted_df = job_logs_df.sort_values(\n        [\"crew\", \"date\", \"picture_submitted_at\"]\n    ).reset_index(drop=True)\n\n    durations = []\n    for (crew, dt), grp in sorted_df.groupby(\n        [\"crew\", sorted_df[\"date\"].dt.date]\n    ):\n        grp = grp.reset_index(drop=True)\n        for i in range(1, len(grp)):\n            t0 = grp.iloc[i - 1][\"picture_submitted_at\"]\n            t1 = grp.iloc[i][\"picture_submitted_at\"]\n            loc_type = grp.iloc[i - 1][\"location_type\"]\n            if pd.notna(t0) and pd.notna(t1):\n                dur = (t1 - t0).total_seconds() / 60.0\n                if dur > 0:\n                    durations.append({\n                        \"location_type\": loc_type,\n                        \"duration_min\": dur,\n                    })\n\n    if not durations:\n        return _empty_df(cols)\n\n    dur_df = pd.DataFrame(durations)\n    stats = dur_df.groupby(\"location_type\")[\"duration_min\"].agg(\n        avg_duration_min=\"mean\",\n        count=\"count\",\n        std_duration_min=\"std\",\n    ).reset_index()\n\n    stats[\"avg_duration_min\"] = stats[\"avg_duration_min\"].round(1)\n    stats[\"std_duration_min\"] = stats[\"std_duration_min\"].round(1).fillna(0.0)\n    stats[\"count\"] = stats[\"count\"].astype(int)\n\n    return stats[cols]\n\n\ndef build_traffic_analysis(route_segments_df):\n    \"\"\"Compute average actual travel time between location pairs.\n\n    Args:\n        route_segments_df: DataFrame output of build_route_segments.\n\n    Returns:\n        DataFrame with columns: origin, destination, avg_travel_min, count,\n        std_travel_min, distance_km, est_travel_min, avg_efficiency.\n    \"\"\"\n    cols = [\"origin\", \"destination\", \"avg_travel_min\", \"count\",\n            \"std_travel_min\", \"distance_km\", \"est_travel_min\", \"avg_efficiency\"]\n\n    if route_segments_df is None or route_segments_df.empty:\n        return _empty_df(cols)\n\n    valid = route_segments_df.dropna(subset=[\"actual_duration_mins\"]).copy()\n    if valid.empty:\n        return _empty_df(cols)\n\n    agg_dict = {\n        \"actual_duration_mins\": [\"mean\", \"count\", \"std\"],\n    }\n    if \"distance_km\" in valid.columns:\n        agg_dict[\"distance_km\"] = \"first\"\n    if \"estimated_travel_mins\" in valid.columns:\n        agg_dict[\"estimated_travel_mins\"] = \"first\"\n    if \"travel_efficiency\" in valid.columns:\n        agg_dict[\"travel_efficiency\"] = \"mean\"\n\n    grouped = valid.groupby(\n        [\"origin_location\", \"destination_location\"]\n    ).agg(agg_dict)\n    grouped.columns = [\"_\".join(c) if c[1] else c[0] for c in grouped.columns]\n    grouped = grouped.reset_index()\n\n    stats = pd.DataFrame()\n    stats[\"origin\"] = grouped[\"origin_location\"]\n    stats[\"destination\"] = grouped[\"destination_location\"]\n    stats[\"avg_travel_min\"] = grouped[\"actual_duration_mins_mean\"].round(1)\n    stats[\"count\"] = grouped[\"actual_duration_mins_count\"].astype(int)\n    stats[\"std_travel_min\"] = grouped[\"actual_duration_mins_std\"].round(1).fillna(0.0)\n    stats[\"distance_km\"] = grouped.get(\"distance_km_first\", pd.Series(dtype=float)).round(2) if \"distance_km_first\" in grouped.columns else np.nan\n    stats[\"est_travel_min\"] = grouped.get(\"estimated_travel_mins_first\", pd.Series(dtype=float)).round(1) if \"estimated_travel_mins_first\" in grouped.columns else np.nan\n    stats[\"avg_efficiency\"] = grouped.get(\"travel_efficiency_mean\", pd.Series(dtype=float)).round(1) if \"travel_efficiency_mean\" in grouped.columns else np.nan\n\n    return stats[cols]\n\n\ndef build_delay_report(route_segments_df, config):\n    \"\"\"Build a report of delayed route segments.\n\n    A segment is delayed when actual_duration exceeds expected service time\n    plus standard travel time.\n\n    Args:\n        route_segments_df: DataFrame output of build_route_segments.\n        config: Configuration dict (output of load_config).\n\n    Returns:\n        DataFrame with columns: crew, date, origin, destination,\n        expected_min, actual_min, delay_min.\n    \"\"\"\n    cols = [\n        \"crew\", \"date\", \"origin\", \"destination\",\n        \"expected_min\", \"actual_min\", \"delay_min\",\n    ]\n\n    if route_segments_df is None or route_segments_df.empty:\n        return _empty_df(cols)\n\n    delayed = route_segments_df[route_segments_df[\"is_delayed\"] == True].copy()\n    if delayed.empty:\n        return _empty_df(cols)\n\n    standard_times = config.get(\"standard_travel_times\", {})\n    expected_service = config.get(\"expected_service_times\", {})\n\n    rows = []\n    for _, seg in delayed.iterrows():\n        key = f\"{seg['origin_location']}|{seg['destination_location']}\"\n        std_travel = standard_times.get(key, 0)\n        svc_time = expected_service.get(\"Unknown\", 0)\n        expected_min = std_travel + svc_time\n        actual_min = seg[\"actual_duration_mins\"]\n        delay = actual_min - expected_min if pd.notna(actual_min) else np.nan\n\n        rows.append({\n            \"crew\": seg[\"crew\"],\n            \"date\": seg[\"date\"],\n            \"origin\": seg[\"origin_location\"],\n            \"destination\": seg[\"destination_location\"],\n            \"expected_min\": round(expected_min, 1),\n            \"actual_min\": round(actual_min, 1) if pd.notna(actual_min) else np.nan,\n            \"delay_min\": round(delay, 1) if pd.notna(delay) else np.nan,\n        })\n\n    if not rows:\n        return _empty_df(cols)\n\n    return pd.DataFrame(rows, columns=cols)\n\n\ndef filter_trackable(df, config, sender_col=\"sender_resolved\"):\n    \"\"\"Remove rows where sender is in the non-trackable senders list.\n\n    Args:\n        df: Any DataFrame containing a sender column.\n        config: Configuration dict (output of load_config).\n        sender_col: Name of the column to check against the exclusion list.\n\n    Returns:\n        Filtered DataFrame with non-trackable senders removed.\n    \"\"\"\n    if df is None or df.empty:\n        return df\n\n    non_trackable = config.get(\"non_trackable_senders\", [])\n    if not non_trackable:\n        return df\n\n    return df[~df[sender_col].isin(non_trackable)].reset_index(drop=True)\n","path":null,"size_bytes":25550,"size_tokens":null},"data/invoice_parser.py":{"content":"\"\"\"Invoice parser for Snow Removal Deployment Tracking.\n\nSupports Excel (.xlsx) and CSV (.csv) invoice files with three formats:\n1. Simple billing: BUILDING NAME, Ward, Service Area, Billing Street, Price column\n2. Pre-treatment report: Snow Priority, Building Name, Billing Street, Ward, crew/completion fields\n3. Completion report: Building Name, Billing Street, Ward, crew assignments, timestamps, verification\n\nAuto-detects format from column headers.\n\"\"\"\n\nimport csv\nimport io\nimport os\nimport re\nfrom datetime import datetime\n\nimport openpyxl\n\n\ndef _normalize(s):\n    if not s:\n        return \"\"\n    return re.sub(r\"[^a-z0-9 ]\", \"\", str(s).lower().strip())\n\n\ndef _detect_format(headers):\n    h_lower = [_normalize(h) for h in headers if h]\n    h_str = \" \".join(h_lower)\n\n    if \"snow  ice removal\" in h_str or \"snow ice removal\" in h_str or \"snow melt\" in h_str or \"pretreatment\" in h_str.replace(\"-\", \"\"):\n        if \"deployed removal crew\" in h_str or \"created datetime\" in h_str or \"percentage completed\" in h_str:\n            return \"completion_report\"\n        if \"deployed pretreatment crew\" in h_str or \"sidewalks pretreated\" in h_str:\n            return \"pretreatment_report\"\n        return \"simple_billing\"\n\n    if \"deployed removal crew\" in h_str or \"contractor snow removal\" in h_str:\n        return \"completion_report\"\n    if \"deployed pretreatment crew\" in h_str or \"sidewalks pretreated\" in h_str:\n        return \"pretreatment_report\"\n\n    for h in h_lower:\n        if any(k in h for k in [\"snow\", \"ice\", \"melt\", \"removal\", \"pre treatment\", \"pretreatment\"]):\n            return \"simple_billing\"\n\n    return \"simple_billing\"\n\n\ndef _detect_deployment_type(headers, title_row=None, filename=None):\n    primary = \"\"\n    if title_row:\n        primary = str(title_row).lower()\n    if filename:\n        primary += \" \" + str(filename).lower()\n\n    if primary:\n        if \"pre-treatment\" in primary or \"pretreatment\" in primary or \"pre treatment\" in primary:\n            return \"Pre-Treatment\"\n        if \"snow removal\" in primary:\n            return \"Snow Removal\"\n        if \"ice removal\" in primary:\n            return \"Ice Removal\"\n        if \"melt only\" in primary or \"melt application\" in primary:\n            return \"Snow Melt\"\n\n    all_text = \" \".join([str(h).lower() for h in headers if h])\n    if \"pre-treatment\" in all_text or \"pretreatment\" in all_text:\n        return \"Pre-Treatment\"\n    if \"ice removal\" in all_text or \"ice melt\" in all_text:\n        return \"Ice Removal\"\n    if \"melt only\" in all_text or \"melt application\" in all_text:\n        return \"Snow Melt\"\n    if \"snow removal\" in all_text or \"snow & ice\" in all_text:\n        return \"Snow Removal\"\n    return \"Snow Removal\"\n\n\ndef _detect_snow_tier(headers, title_row=None):\n    all_text = \" \".join([str(h).lower() for h in headers if h])\n    if title_row:\n        all_text += \" \" + str(title_row).lower()\n\n    if '12\"-24\"' in all_text or \"12 to 24\" in all_text or \">12\" in all_text:\n        return \"price_12_to_24in\"\n    if '6\"-12\"' in all_text or \"6 to 12\" in all_text or \">6\" in all_text:\n        return \"price_6_to_12in\"\n    if '<6\"' in all_text or \"under 6\" in all_text:\n        return \"price_under_6in\"\n    if \"melt only\" in all_text or \"melt application\" in all_text:\n        return \"price_melt_only\"\n    if \"pre-treatment\" in all_text or \"pretreatment\" in all_text:\n        return \"price_melt_only\"\n    return \"price_under_6in\"\n\n\ndef _detect_date_from_filename(filename):\n    patterns = [\n        r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\",\n        r\"(\\d{4})(\\d{2})(\\d{2})\",\n    ]\n    for pat in patterns:\n        m = re.search(pat, filename)\n        if m:\n            try:\n                return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3))).strftime(\"%Y-%m-%d\")\n            except ValueError:\n                pass\n    return None\n\n\ndef _find_price_col(headers):\n    skip_words = {\"priority\", \"id\", \"crew\", \"service area\", \"ward\", \"building\", \"street\",\n                  \"billing\", \"name\", \"status\", \"percentage\", \"created\", \"verification\",\n                  \"attachment\", \"cleared\", \"pretreated\", \"accessible\", \"fire\", \"loading\",\n                  \"dumpster\", \"exits\", \"contractor\", \"only ice\", \"parking lots\", \"side walks\",\n                  \"crosswalk\", \"hydrant\", \"dock\", \"trash\"}\n    exact_skip = {\"snow\", \"ice\", \"removal\"}\n    for i, h in enumerate(headers):\n        if not h:\n            continue\n        h_lower = str(h).lower().strip()\n        if h_lower in exact_skip:\n            continue\n        if any(k in h_lower for k in [\"snow\", \"ice\", \"melt\", \"removal\", \"pre-treatment\", \"pretreatment\", \"pre treatment\"]):\n            if not any(s in h_lower for s in skip_words):\n                return i\n    return None\n\n\ndef _read_rows_from_csv(filepath_or_text, is_text=False):\n    if is_text:\n        lines = filepath_or_text\n    else:\n        with open(filepath_or_text, \"r\", encoding=\"utf-8-sig\") as f:\n            lines = f.read()\n\n    reader = csv.reader(io.StringIO(lines))\n    all_rows = []\n    for row in reader:\n        converted = []\n        for cell in row:\n            cell = cell.strip() if cell else None\n            if cell is None or cell == \"\":\n                converted.append(None)\n            else:\n                try:\n                    converted.append(float(cell))\n                except ValueError:\n                    converted.append(cell)\n        all_rows.append(converted)\n    return all_rows\n\n\ndef _read_rows_from_excel(filepath):\n    wb = openpyxl.load_workbook(filepath, data_only=True)\n    sheets = {}\n    for sheet_name in wb.sheetnames:\n        ws = wb[sheet_name]\n        if ws.max_row < 2:\n            continue\n        rows = []\n        for row in ws.iter_rows(min_row=1, max_row=ws.max_row, values_only=True):\n            rows.append(list(row))\n        if rows:\n            sheets[sheet_name] = rows\n    return sheets\n\n\ndef parse_invoice(filepath, filename=None):\n    if filename is None:\n        filename = os.path.basename(filepath)\n\n    ext = os.path.splitext(filename or filepath)[1].lower()\n    if ext == \".csv\":\n        all_rows = _read_rows_from_csv(filepath)\n        sheets_data = {\"Sheet1\": all_rows} if all_rows else {}\n    else:\n        sheets_data = _read_rows_from_excel(filepath)\n\n    results = []\n    for sheet_name, all_rows in sheets_data.items():\n        if len(all_rows) < 2:\n            continue\n        results.extend(_parse_sheet_rows(all_rows, sheet_name, filename))\n    return results\n\n\ndef _parse_sheet_rows(all_rows, sheet_name, filename):\n    results = []\n\n    title_row = None\n    header_row_idx = None\n    for i, row in enumerate(all_rows):\n        vals = [v for v in row if v is not None]\n        if not vals:\n            continue\n        non_none_count = len(vals)\n        has_building = any(\"building\" in _normalize(str(v)) for v in vals)\n        has_billing = any(\"billing\" in _normalize(str(v)) for v in vals)\n        has_ward = any(\"ward\" in _normalize(str(v)) for v in vals)\n\n        if (has_building or has_billing) and (has_ward or non_none_count >= 4):\n            header_row_idx = i\n            break\n        elif non_none_count <= 2 and i == 0:\n            title_row = str(vals[0]) if vals else None\n\n    if header_row_idx is None:\n        return results\n\n    headers = all_rows[header_row_idx]\n    fmt = _detect_format(headers)\n    dep_type = _detect_deployment_type(headers, title_row, filename)\n    snow_tier = _detect_snow_tier(headers, title_row)\n    dep_date = _detect_date_from_filename(filename)\n\n    col_map = {}\n    for i, h in enumerate(headers):\n        if not h:\n            continue\n        h_norm = _normalize(str(h))\n        if \"building\" in h_norm and \"name\" in h_norm:\n            col_map[\"building_name\"] = i\n        elif \"building name\" in h_norm:\n            col_map[\"building_name\"] = i\n        elif \"billing\" in h_norm and \"street\" in h_norm:\n            col_map[\"address\"] = i\n        elif \"ward\" in h_norm:\n            col_map[\"ward\"] = i\n        elif \"service area\" in h_norm:\n            col_map[\"service_area\"] = i\n        elif \"snow priority\" in h_norm:\n            col_map[\"snow_priority\"] = i\n        elif \"percentage completed\" in h_norm:\n            col_map[\"pct_completed\"] = i\n        elif \"deployed removal crew\" in h_norm:\n            col_map[\"removal_crew\"] = i\n        elif \"deployed pretreatment crew\" in h_norm:\n            col_map[\"pretreatment_crew\"] = i\n        elif \"created by\" in h_norm:\n            col_map[\"created_by\"] = i\n        elif \"created date\" in h_norm or \"time created\" in h_norm:\n            col_map[\"created_time\"] = i\n        elif \"sidewalks pretreated\" in h_norm:\n            col_map[\"sidewalks_pretreated\"] = i\n        elif \"parking lots pretreated\" in h_norm:\n            col_map[\"parking_pretreated\"] = i\n        elif \"verification status\" in h_norm:\n            col_map[\"verification_status\"] = i\n        elif \"verification comment\" in h_norm:\n            col_map[\"verification_comments\"] = i\n        elif \"parking lots cleared\" in h_norm or \"parking\" in h_norm and \"cleared\" in h_norm:\n            col_map[\"parking_cleared\"] = i\n        elif \"side walks cleared\" in h_norm or \"sidewalk\" in h_norm and \"cleared\" in h_norm:\n            col_map[\"sidewalks_cleared\"] = i\n        elif \"ice melt\" in h_norm and \"applied\" in h_norm:\n            col_map[\"ice_melt_applied\"] = i\n        elif \"snow  ice removal\" in h_norm or \"snow ice removal\" in h_norm:\n            col_map[\"snow_removal_id\"] = i\n\n    price_col = _find_price_col(headers)\n\n    line_items = []\n    for row in all_rows[header_row_idx + 1:]:\n        if not any(v is not None for v in row):\n            continue\n\n        bname_idx = col_map.get(\"building_name\")\n        if bname_idx is not None and bname_idx < len(row):\n            building_name = str(row[bname_idx]).strip() if row[bname_idx] else \"\"\n        else:\n            building_name = str(row[0]).strip() if row and row[0] else \"\"\n\n        if not building_name:\n            continue\n\n        skip_keywords = [\"total\", \"subtotal\", \"grand total\", \"t&m\", \"additional time\"]\n        if any(k in building_name.lower() for k in skip_keywords):\n            continue\n\n        item = {\"building_name\": building_name}\n\n        addr_idx = col_map.get(\"address\")\n        if addr_idx is not None and addr_idx < len(row) and row[addr_idx]:\n            item[\"address\"] = str(row[addr_idx]).strip()\n\n        ward_idx = col_map.get(\"ward\")\n        if ward_idx is not None and ward_idx < len(row) and row[ward_idx]:\n            item[\"ward\"] = str(row[ward_idx]).strip()\n\n        sa_idx = col_map.get(\"service_area\")\n        if sa_idx is not None and sa_idx < len(row) and row[sa_idx]:\n            item[\"service_area\"] = str(row[sa_idx]).strip()\n\n        if price_col is not None and price_col < len(row) and row[price_col] is not None:\n            try:\n                item[\"billed_amount\"] = float(row[price_col])\n            except (ValueError, TypeError):\n                pass\n\n        prio_idx = col_map.get(\"snow_priority\")\n        if prio_idx is not None and prio_idx < len(row) and row[prio_idx]:\n            try:\n                item[\"snow_priority\"] = int(row[prio_idx])\n            except (ValueError, TypeError):\n                item[\"snow_priority\"] = str(row[prio_idx])\n\n        for field in [\"removal_crew\", \"pretreatment_crew\", \"created_by\", \"created_time\",\n                      \"pct_completed\", \"verification_status\", \"verification_comments\",\n                      \"sidewalks_pretreated\", \"parking_pretreated\",\n                      \"sidewalks_cleared\", \"parking_cleared\", \"ice_melt_applied\"]:\n            idx = col_map.get(field)\n            if idx is not None and idx < len(row) and row[idx] is not None:\n                val = row[idx]\n                if isinstance(val, (int, float)):\n                    item[field] = val\n                else:\n                    item[field] = str(val).strip()\n\n        line_items.append(item)\n\n    if line_items:\n        total_billed = sum(it.get(\"billed_amount\", 0) for it in line_items)\n        results.append({\n            \"filename\": filename,\n            \"sheet\": sheet_name,\n            \"format\": fmt,\n            \"deployment_type\": dep_type,\n            \"snow_tier\": snow_tier,\n            \"deployment_date\": dep_date,\n            \"title\": title_row,\n            \"total_billed\": round(total_billed, 2),\n            \"site_count\": len(line_items),\n            \"line_items\": line_items,\n        })\n\n    return results\n\n\ndef parse_invoice_bytes(content_bytes, filename):\n    import tempfile\n    ext = os.path.splitext(filename)[1].lower() if filename else \".xlsx\"\n    if ext == \".csv\":\n        try:\n            text = content_bytes.decode(\"utf-8-sig\")\n        except UnicodeDecodeError:\n            text = content_bytes.decode(\"latin-1\")\n        all_rows = _read_rows_from_csv(text, is_text=True)\n        if not all_rows:\n            return []\n        sheets_data = {\"Sheet1\": all_rows}\n        results = []\n        for sheet_name, rows in sheets_data.items():\n            if len(rows) < 2:\n                continue\n            results.extend(_parse_sheet_rows(rows, sheet_name, filename))\n        return results\n    else:\n        with tempfile.NamedTemporaryFile(suffix=ext, delete=False) as tmp:\n            tmp.write(content_bytes)\n            tmp_path = tmp.name\n        try:\n            return parse_invoice(tmp_path, filename)\n        finally:\n            os.unlink(tmp_path)\n\n\ndef match_invoice_to_deployment(invoice_date_str, deployments_list, tolerance_days=2):\n    if not invoice_date_str or not deployments_list:\n        return None\n\n    try:\n        inv_date = datetime.strptime(invoice_date_str, \"%Y-%m-%d\").date()\n    except (ValueError, TypeError):\n        return None\n\n    from datetime import timedelta\n    best_match = None\n    best_dist = float(\"inf\")\n\n    for dep in deployments_list:\n        start = dep[\"start_date\"]\n        end = dep[\"end_date\"]\n        if start <= inv_date <= end:\n            return dep[\"label\"]\n        dist = min(abs((inv_date - start).days), abs((inv_date - end).days))\n        if dist <= tolerance_days and dist < best_dist:\n            best_dist = dist\n            best_match = dep[\"label\"]\n\n    return best_match\n\n\ndef reconcile_invoice_with_chat(invoice, chat_locations, pricing_index, normalize_fn,\n                                billable_routes=None):\n    line_items = invoice.get(\"line_items\", [])\n    reconciliation = []\n\n    for item in line_items:\n        bname = item.get(\"building_name\", \"\")\n        addr = item.get(\"address\", \"\")\n        billed = item.get(\"billed_amount\", 0)\n\n        bname_norm = normalize_fn(bname)\n        addr_norm = normalize_fn(addr)\n\n        in_chat = False\n        matched_chat_loc = None\n        matched_service_areas = []\n        for cloc in chat_locations:\n            cloc_norm = normalize_fn(cloc)\n            if cloc_norm == bname_norm or cloc_norm == addr_norm:\n                in_chat = True\n                matched_chat_loc = cloc\n                break\n            if bname_norm and bname_norm in cloc_norm:\n                in_chat = True\n                matched_chat_loc = cloc\n                break\n            if addr_norm and addr_norm in cloc_norm:\n                in_chat = True\n                matched_chat_loc = cloc\n                break\n\n        if in_chat and billable_routes is not None and matched_chat_loc:\n            loc_norm = normalize_fn(matched_chat_loc)\n            for _, route in billable_routes.iterrows():\n                route_loc = normalize_fn(str(route.get(\"location\", \"\")))\n                if route_loc == loc_norm:\n                    matched_service_areas.append(route.get(\"service_area\", \"Unknown\"))\n\n        contract_price = None\n        if pricing_index:\n            pricing = pricing_index.get(bname_norm) or pricing_index.get(addr_norm)\n            if pricing:\n                tier = invoice.get(\"snow_tier\", \"price_under_6in\")\n                contract_price = pricing.get(tier, 0)\n\n        status = \"ok\"\n        notes = \"\"\n        if not in_chat:\n            status = \"not_in_chat\"\n            notes = \"Site invoiced but not found in chat data\"\n        elif billed and contract_price and abs(billed - contract_price) > 1:\n            status = \"price_mismatch\"\n            notes = f\"Billed ${billed:.2f} vs contract ${contract_price:.2f}\"\n\n        reconciliation.append({\n            \"building_name\": bname,\n            \"address\": addr,\n            \"billed_amount\": billed,\n            \"contract_price\": contract_price,\n            \"in_chat_data\": in_chat,\n            \"matched_chat_location\": matched_chat_loc,\n            \"service_areas\": \", \".join(sorted(set(matched_service_areas))) if matched_service_areas else \"\",\n            \"status\": status,\n            \"notes\": notes,\n        })\n\n    chat_not_invoiced = []\n    invoiced_names = set(_normalize(it[\"building_name\"]) for it in line_items)\n    invoiced_addrs = set(_normalize(it.get(\"address\", \"\")) for it in line_items)\n\n    for cloc in chat_locations:\n        cloc_norm = normalize_fn(cloc)\n        found = cloc_norm in invoiced_names or cloc_norm in invoiced_addrs\n        if not found:\n            for iname in invoiced_names:\n                if iname and iname in cloc_norm:\n                    found = True\n                    break\n            if not found:\n                for iaddr in invoiced_addrs:\n                    if iaddr and iaddr in cloc_norm:\n                        found = True\n                        break\n        if not found:\n            chat_not_invoiced.append(cloc)\n\n    sw_routes = 0\n    pl_routes = 0\n    if billable_routes is not None and not billable_routes.empty:\n        for _, route in billable_routes.iterrows():\n            sa = str(route.get(\"service_area\", \"\")).strip()\n            if sa == \"Parking Lot\":\n                pl_routes += 1\n            else:\n                sw_routes += 1\n\n    return {\n        \"line_items\": reconciliation,\n        \"sites_invoiced\": len(line_items),\n        \"sites_in_chat\": sum(1 for r in reconciliation if r[\"in_chat_data\"]),\n        \"sites_not_in_chat\": sum(1 for r in reconciliation if not r[\"in_chat_data\"]),\n        \"price_mismatches\": sum(1 for r in reconciliation if r[\"status\"] == \"price_mismatch\"),\n        \"chat_not_invoiced\": chat_not_invoiced,\n        \"chat_not_invoiced_count\": len(chat_not_invoiced),\n        \"billable_routes_total\": sw_routes + pl_routes,\n        \"sw_routes\": sw_routes,\n        \"pl_routes\": pl_routes,\n    }\n","path":null,"size_bytes":18403,"size_tokens":null}},"version":2}